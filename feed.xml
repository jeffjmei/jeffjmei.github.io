<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jeffjmei.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jeffjmei.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-13T19:48:14+00:00</updated><id>https://jeffjmei.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">The Double Descent Phenomenon</title><link href="https://jeffjmei.github.io/blog/2025/double-descent/" rel="alternate" type="text/html" title="The Double Descent Phenomenon"/><published>2025-04-12T22:25:00+00:00</published><updated>2025-04-12T22:25:00+00:00</updated><id>https://jeffjmei.github.io/blog/2025/double-descent</id><content type="html" xml:base="https://jeffjmei.github.io/blog/2025/double-descent/"><![CDATA[<h1 id="double-descent">Double Descent</h1> <p>The discovery of the descent phenomenon (Belkin, 2019) has upended the classical understanding of bias-variance trade-off. Classical understanding suggests that increasing model complexity inevitably leads to overfitting. A long standing mystery has been why neural networks have such successful generalization performance even when it has been “overfit” on training data. The double descent phenomenon demonstrates that the bias-variance trade-off is incomplete. For low capacity models, the bias-variance trade-off successfully explains generalization performance. However, for high capacity models (e.g. neural networks, random forest, etc.), increasing model complexity often leads to improved generalization error. Consequently, studying over-parameterized models – models that interpolate the training data to achieve perfect training error – have been an active field of research.</p> <p><img src="/assets/img/double-descent-curve.png" alt="Double Descent Curve" style="width:100%; max-width:600px;"/></p> <p>Recent evidence suggests SGD behaves differently in these over-parameterized contexts than in classical scenarios (Ma, et al., 2018; Belkin, 2021). In particular, whereas SGD tends to reach local minima in classical scenarios, SGD tends to reach global minima in over-parameterized regimes.</p> <p><img src="/assets/img/sgd-under-over-parameterized.png" alt="SGD Over-Parameterized" style="width:100%; max-width:600px;"/></p> <p>Moreover, while SGD with fixed learning rate does not converge in classical scenarios, it converges exponentially in over-parameterized scenarios.</p> <p><img src="/assets/img/double-descent-comparisons.png" alt="Double Descent Comparison" style="width:100%; max-width:600px;"/></p> <p>All in all, machine learning theory has long lagged behind our empirical understanding. The double-descent phenomenon and the surprising advantages of over-parameterization are only the first insights into explaining the remarkable success of machine learning models.</p> <p><strong>References:</strong></p> <ol> <li> <p><strong>Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S.</strong> (2019). <em>Reconciling modern machine-learning practice and the classical bias–variance trade-off.</em> Proceedings of the National Academy of Sciences, 116(32), 15849–15854. <a href="https://doi.org/10.1073/pnas.1903070116">https://doi.org/10.1073/pnas.1903070116</a></p> </li> <li> <p><strong>Ma, S., Bassily, R., &amp; Belkin, M.</strong> (2018). <em>The power of interpolation: Understanding the effectiveness of SGD in modern over-parameterized learning.</em> Proceedings of the 35th International Conference on Machine Learning (ICML), PMLR 80:3325–3334. <a href="http://proceedings.mlr.press/v80/ma18a/ma18a.pdf">http://proceedings.mlr.press/v80/ma18a/ma18a.pdf</a></p> </li> <li> <p><strong>Belkin, M.</strong> (2021). <em>Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation.</em> Acta Numerica, 30, 203–248. <a href="https://doi.org/10.1017/S0962492921000039">https://doi.org/10.1017/S0962492921000039</a></p> </li> </ol>]]></content><author><name></name></author><category term="ml"/><category term="theory"/><category term="tutorials"/><summary type="html"><![CDATA[Modern machine learning models defy the bias-variance trade-off. A new theory of machine learning must be developed to explain the double descent phenomenon and the success of over-parameterized models.]]></summary></entry></feed>