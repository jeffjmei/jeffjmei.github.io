<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jeffjmei.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jeffjmei.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-12T04:17:23+00:00</updated><id>https://jeffjmei.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Change Point Analysis Introduction</title><link href="https://jeffjmei.github.io/blog/2025/change-point-intro/" rel="alternate" type="text/html" title="Change Point Analysis Introduction"/><published>2025-08-01T12:25:00+00:00</published><updated>2025-08-01T12:25:00+00:00</updated><id>https://jeffjmei.github.io/blog/2025/change-point-intro</id><content type="html" xml:base="https://jeffjmei.github.io/blog/2025/change-point-intro/"><![CDATA[<div class="row d-flex justify-content-center text-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DUR_NDUR_volatility-480.webp 480w,/assets/img/DUR_NDUR_volatility-800.webp 800w,/assets/img/DUR_NDUR_volatility-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/DUR_NDUR_volatility.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="labor-statistics" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Volatility of Labor Statistics. </div> <p>When you collect data for a long enough time, the data-generating process will inevitably change. The mean might shift, the variance might shrink. <em>Change points</em> are where these changes occur.</p> <p>Failing to incorporate <em>change points</em> into an analysis risks producing spurious inference. In other words, we risk making false conclusions. Research in <em>change point analysis</em> is focused on extending the statistical repertoire to handle distribution shifts to produce valid statistical inference.</p> <p>Change points are present in a wide variety of applications, but their primary use is with biomedical and economic/financial data. Stock prices are collected every second of every day, but market conditions change constantly. Over time, laws get passed and policies get enacted. Each of these can trigger a bull run or a recession. We cannot accurately study these time-series without considering the distribution shifts.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Volatility of Labor Statistics.]]></summary></entry><entry><title type="html">Generalized Additive Models</title><link href="https://jeffjmei.github.io/blog/2025/additive-models/" rel="alternate" type="text/html" title="Generalized Additive Models"/><published>2025-04-15T22:25:00+00:00</published><updated>2025-04-15T22:25:00+00:00</updated><id>https://jeffjmei.github.io/blog/2025/additive-models</id><content type="html" xml:base="https://jeffjmei.github.io/blog/2025/additive-models/"><![CDATA[<h1 id="generalized-additive-models">Generalized Additive Models</h1> <p>Nonparametric additive models are flexible extensions of linear models. They can fit to complex smooth surfaces, but they suffer in high dimensions since they potentially fit to insignificant components. Therefore, a mechanism for filtering out insignificant components is of paramount importance. Since the advent of the LASSO <a class="citation" href="#tibshirani_regression_1996">(Tibshirani, 1996)</a>, many procedures have applied regularization techniques to instill variable selection capabilities. In this literature review, we cover two approaches that apply soft-thresholding to filter out insignificant components in the context of additive models. Lin and Zhang propose the COSSO (COmponent Selection and Smoothing Operator), which generalizes the LASSO to the context of additive models in order to facilitate component selection <a class="citation" href="#lin_component_2006">(Lin &amp; Zhang, 2006)</a>. Building on this work, <a class="citation" href="#ravikumar_sparse_2008">(Ravikumar et al., 2009)</a> proposes a variation that generalizes the COSSO and applies a backfitting procedure to facilitate variable selection.</p> <p>Additive models were introduced as a generalization of linear models <a class="citation" href="#hastie_generalized_1986">(Hastie &amp; Tibshirani, 1986)</a>. They take the form</p> \[\begin{equation} Y_i = \sum_{j=1}^{p} f_j(X_{ij}) + \varepsilon_i \label{eq:additive_model} \end{equation}\] <p>where \(\varepsilon\) is often gaussian noise. It is easy to see that if we let \(f(X) = X \beta\), then we have the linear model again. In practice, it is common to let \(f \in \mathcal{S}^2[0, 1]\), the second order Sobolev space on \([0, 1]\), defined as the set \(\left\{ f: f, f' \text{ abs. continuous}, \int_{0}^{1} (f''(x))^2 d{x} &lt; \infty \right\}\). This is an enormous class of functions that includes polynomials, \(\sin\), \(\cos\), \(\log\), and many more functions, making additive models an incredibly flexible approach to data modeling.</p> <p>It is worth noting that additive models can be extended to the class of Smoothing-Spline Analysis of Variance (SS-ANOVA) models if interactions are considered as well. That is, SS-ANOVA models take the form</p> \[\begin{equation} Y_i = \sum_{j=1}^{p} f_j(X_{ij}) + \sum_{j &lt; k} f_{jk}(X_{ij}) + ... + \varepsilon_i. \label{eq:ss_anova} \end{equation}\] <p>While additive models are incredibly flexible, one problem is that they suffer in high dimensions. Often times when \(p\) is large, there are components in the model that make little to no contributions to the model performance. However, without filtering out these features, the model will end up fitting to the noise. Therefore,</p> <p>In this literature review, we will provide an overview of methods used to overcome this issue. In particular we will consider the COmponent Selection and Smoothing Operator (COSSO) <a class="citation" href="#lin_component_2006">(Lin &amp; Zhang, 2006)</a> and the Sparse Additive Model (SpAM) <a class="citation" href="#ravikumar_sparse_2008">(Ravikumar et al., 2009)</a>.</p> <h1 id="reproducing-kernel-hilbert-spaces">Reproducing Kernel Hilbert Spaces</h1> <p>It is essential to understand the basics of Reproducing Kernel Hilbert Spaces (RKHS), as they play a significant role in both the COSSO and SpAM procedures.</p> <p>RKHSs are useful tools in the smoothing spline literature. They offer a feasible approach to access a potentially infinite dimensional class of functions (e.g. 2nd order Sobolev space). For a more detailed approach, refer to <a class="citation" href="#gu_smoothing_2002">(Gu, 2002)</a> and <a class="citation" href="#nosedal-sanchez_reproducing_2012">(Nosedal-Sanchez et al., 2012)</a>. Here, we will provide an informal overview of RKHS theory.</p> <p>A <strong>functional</strong> \(L\) is defined as a mapping from a linear space \(\mathcal{V}\) to the real numbers \(L: \mathcal{V} \to \mathbb{R}\). A functional of particular interest is the <strong>evaluation functional</strong> \([\cdot]\), such that \([x]f = f(x)\). That is, the evaluation functional \([x]\) is equal to evaluating the function \(f\) at \(x\).</p> <blockquote> <p><strong>Riesz Representation Theorem</strong></p> <p>Let \(\mathcal{H}\) be a Hilbert space with continuous functional \(L\) defined on it. For any \(f \in \mathcal{H}\), there exists a unique \(g \in \mathcal{H}\) such that \(Lf = \langle f, g \rangle.\)</p> </blockquote> <p>If we take \([x]\) to be the evaluation functional, it follows that for any continuous function \(f\), there exists some representer \(R_x\) such that \(\langle R_x, f \rangle = f(x)\). We will call \(R_x\) the <strong>reproducing kernel</strong> of the RKHS \(\mathcal{H}_R\).</p> <div class="row d-flex justify-content-center text-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/additive-models/Riesz-480.webp 480w,/assets/img/additive-models/Riesz-800.webp 800w,/assets/img/additive-models/Riesz-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/additive-models/Riesz.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="riesz-representation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Riesz Representation Theorem. </div> <p>The significance of this result is that it provides two different ways to evaluate a functional. We can evaluate it on the set \(X\) with the kernel, or we can project the data onto a different space \(\mathcal{H}_R\) and evaluate the functional using the inner product. Most often however, we will evaluate the functional using the kernel because it is typically more computationally convenient.</p> <p>To find a RKHS, we note that there is a convenient relationship between non-negative definite functions and reproducing kernels.</p> <blockquote> <p><strong>RKHS - Non-Negative Definite Relationship</strong></p> <p>For every non-negative definite function \(R(x, y)\) on \(X\), there exists a unique RKHS \(\mathcal{H}_R\) with the reproducing kernel \(R(x, y)\). The converse is also true. For every RKHS \(\mathcal{H}_R\), there exists a unique non-negative definite function \(R(x, y)\) on \(X\).</p> </blockquote> <div class="row d-flex justify-content-center text-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/additive-models/RKHS-480.webp 480w,/assets/img/additive-models/RKHS-800.webp 800w,/assets/img/additive-models/RKHS-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/additive-models/RKHS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="rkhs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption text-center"> Reproducing Kernel Hilbert Space. </div> <p>For the remainder of this paper, we define the inner product to be</p> \[\begin{equation} \langle f, g \rangle = \sum_{v = 0}^{m-1} \left( \int_{0}^{1} f^{(v)} d{x}\right) \left( \int_{0}^{1} g^{(v)} d{x}\right) + \int_{0}^{1} f^{(m)}g^{(m)} d{x}. \label{eq:inner_prod} \end{equation}\] <p>One important consequence of the RKHS framework, is that we are able to represent functions in \(\mathcal{H}_R\) by their kernel functions:</p> \[\begin{equation} f(x) = \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c + b \textbf{1}_n. \label{eq:rkhs_function} \end{equation}\] <p>By representing functions in \(\mathcal{H}_R\) by their kernels, we are able to obtain notions of “similarity” between functions in a richer space \(\mathcal{H}_R\) without having to do any computations within that richer space. Instead, our computations remain on the original space \(X\).</p> <h1 id="cosso-component-selection-and-smoothing">COSSO: Component Selection and Smoothing</h1> <p>The COSSO is a flexible approach that performs component selection to filter out small to insignificant components. This is similar to the LASSO <a class="citation" href="#tibshirani_regression_1996">(Tibshirani, 1996)</a>, but instead of penalizing on coefficient size, the COSSO penalizes, in an informal sense, “component size.” Roughly speaking, size of a function is described as the squared integral over \([0, 1]\). To develop this fully, we must dive into the theory of Reproducing Kernel Hilbert Spaces (RKHS).</p> <p>In particular, the problem that we are solving is</p> \[\begin{equation} \hat{f} = \text{argmin}_f \sum_{i=1}^{n} (y_i - f(x_i))^2 + \lambda_n \sum_{j=1}^{p} \| P^j f \| \label{eq:cosso} \end{equation}\] <p>where the norm in the 2nd order Sobolev space is</p> \[\begin{equation} \| f \| = \left( \int_{0}^{1} f(t) d{t} \right)^2 + \left( \int_{0}^{1} f'(t) d{t} \right)^2 + \int_{0}^{1} \left( f''(t) \right)^2 d{t}. \label{eq:norm} \end{equation}\] <p>The first term in (\ref{eq:cosso}) encourages the optimization problem to fit as closely to the data as possible, but it is penalized by the 2nd term, which penalizes by model complexity.</p> <h2 id="relationship-to-lasso">Relationship to LASSO</h2> <p>The authors demonstrate that the COSSO and the LASSO are the same. It is important to note that while the COSSO generalizes the LASSO, the interpretation changes. Instead of penalizing on coefficient size, we are penalizing by component size.</p> <h2 id="algorithm">Algorithm</h2> <p>The authors demonstrate that the COSSO can be decomposed into a non-negative garrote and a smoothing spline problem. Since the algorithm is guaranteed to improve its estimate on every iteration, we can just alternate between the non-negative garrote solution and the smoothing spline solution.</p> <p>The authors note that the original COSSO problem (\ref{eq:cosso}) can be reformulated into the more computationally tractable form</p> \[\begin{equation} \frac{1}{n} \left( y - \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c - b \textbf{1}_n \right)^T \left( y - \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c - b \textbf{1}_n \right) + \lambda_0 \sum_{\alpha=1}^{p} \theta_\alpha c^T R_\alpha c + \lambda \sum_{\alpha=1}^{p} \theta_\alpha. \label{eq:cosso_algo} \end{equation}\] <p>As it turns out, (\ref{eq:cosso_algo}) can be broken down further into two constituent sub-algorithms. In particular, when we fix \(c\) and \(b\), (\ref{eq:cosso_algo}) is reduced into the ridge regression. Similarly, when \(\theta\) is fixed, (\ref{eq:cosso_algo}) reduces into a non-negative garrote.</p> <p>By fixing \(c, b\), the COSSO reduces to the non-negative garrote, where components are selected:</p> \[\begin{equation} \min_\theta (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \label{eq:cosso_garrote} \end{equation}\] <p>where \(\theta_\alpha \ge 0\) and \(z = y - (1/2) n \lambda_0 c - b \textbf{1}_n.\)</p> <p>Similarly, by fixing \(\theta\), we get a problem equivalent to ridge regression, where the functions are smoothed:</p> \[\begin{equation} \min_{c, b} (y - R_\theta c - b \textbf{1}_n)^T (y - R_\theta c - b \textbf{1}_n) + n \lambda_0 c^T R_\theta c. \label{eq:cosso_ridge} \end{equation}\] <p>Now, with the insight that the COSSO can be broken down into problems with known solutions, the proposed algorithm flips between fixing \(\theta\) and fixing \(c \text{ and } b\). In other words, the algorithm flips between the non-negative garrote and ridge regression. We continue until algorithm converges on a solution with a pre-specified error.</p> \[\begin{aligned} &amp;\textbf{Algorithm: COSSO} \\ &amp;\textbf{Initialize:} \text{fix } \theta_\alpha = 1, \alpha = 1, ..., p, \; g(\theta, b, c) = 0 \\ &amp;\textbf{Repeat until convergence:} \\ &amp;\quad 1. \quad \text{Fix } \theta, \text{apply ridge regression (\ref{eq:cosso_ridge})} \\ &amp;\quad 2. \quad (c, b) \gets \text{argmin}_{c, b} \left[ (y - R_\theta c - b \mathbf{1}_n)^T (y - R_\theta c - b \mathbf{1}_n) + n \lambda_0 c^T R_\theta c \right] \\ &amp;\quad 3. \quad \text{Fix } b, c, \text{apply non-negative garrote (\ref{eq:cosso_garrote})} \\ &amp;\quad 4. \quad \theta \gets \text{argmin}_\theta \left[ (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \right] \\ &amp;\quad 5. \quad g(\theta, b, c) \gets \min_\theta \left[ (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \right] \end{aligned}\] <p>However, the authors note that the first iteration makes most of the way to a solution.</p> <h1 id="sparse-additive-models">Sparse Additive Models</h1> <p>Ravikumar et al. (2008) propose the Sparse Additive Model (SpAM), which is similar to the COSSO. Again, the model is proposed is another \(l_1\) penalized approach to add sparsity to the additive model. However, SpAM applies an additional constraint to normalize function size. In doing so, SpAM decouples sparsity and smoothness. Recall that the COSSO penalizes by component sizes, which are functions of both complexity and magnitude. By decoupling sparsity and smoothness, SpAM is more flexible than COSSO.</p> <p>The optimization problem for SpAM is</p> \[\begin{align} &amp;\min_{g_j \in H_j} \mathbb{E}\left[Y - \sum_{j=1}^{p} \beta_j g_j(X_j) \right]^2 \\ &amp;\text{s.t.: } \sum_{j=1}^{p} |\beta_j| \le L \\ &amp; \qquad \mathbb{E}\left[g_j^2\right] = 1. \label{eq:spam} \end{align}\] <p>where \(Y\) is an \(n \times 1\) vector representing the outputs to be predicted, \(X\) is an \(n \times p\) data matrix, \(L \ge 0\) is a penalty constraint, and \(\mathcal{H}_j\) is a RKHS for \(j=1,...,p.\)</p> <p>In the LASSO, we penalize the regression coefficients by taking the norm of the \(\beta\) vector. Here, we take the same idea to encourage sparsity, in addition to adding an additional constraint of \(\mathbb{E}\left[g_j^2\right] = 1\) to limit the set of functions to search.</p> <p>We can rewrite the above constraint to be</p> \[\begin{align*} \min_{f_j \in H_j} &amp;\mathbb{E}\left[Y - \sum_{j=1}^{p} \beta_j g_j(X_j) \right]^2 \\ \text{subject to: } &amp; \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \le L. \end{align*}\] <p>Or equivalently</p> \[\begin{equation} \mathcal{L}(f, \lambda) = \frac{1}{2} \mathbb{E}\left[Y - \sum_{j=1}^{p} f_j(X_j) \right]^2 + \lambda \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]}. \label{eq:spam_lagrange} \end{equation}\] <p>Proof:</p> \[\begin{align*} f_j(X_j) &amp;= \beta_j g_j (X_j) \\ g_j(X_j) &amp;= f_j(X_j) / \beta_j \\ \mathbb{E}\left[g_j(X_j)^2\right] &amp;= \mathbb{E}\left[f_j(X_j)^2 / \beta_j^2\right] = 1 \\ \beta_j^2 &amp;= \mathbb{E}\left[f_j^2(X_j)\right]\\ \beta_j &amp;= \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \\ \sum_{j=1}^{p} |\beta_j| &amp;= \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \le L . \end{align*}\] <p>The authors demonstrate that the minimizers can be expressed as the soft-thresholded projection</p> \[\begin{equation} f_j = \left[ 1 - \frac{\lambda}{\sqrt{\mathbb{E}\left[P_j^2\right]}} \right]_+ \mathbb{E}\left[R_j|X_j\right]. % P_j. \label{eq:spam_minimizers} \end{equation}\] <p>Where residuals excluding the contribution of the \(j\)th component is \(R_j = Y - \sum_{k\neq j}^{} f_k(X_k)\) and the projection from the residuals onto \(\mathcal{H}_j\) is</p> \[\begin{equation} P_j = \mathbb{E}\left[R_j|X_j\right]. \label{eq:projection_pop} \end{equation}\] <p>Equation (\ref{eq:spam_minimizers}) illuminates the inner workings of SpAM. In particular, we can see that the population minimizer is a soft-thresholded projection onto \(\mathcal{H}_j\) where the projection \(P_j\) attempts to reconstruct the signal using information exclusively in the \(j\)th component.</p> <h2 id="algorithm-1">Algorithm:</h2> <p>The problem with our formulation in (\ref{eq:spam_minimizers}) is that it requires information on the population in \(E[P_j^2]\) and \(E[R_j \mid X_j]\). In most practical situations, we will not know the probability distributions and will thus be unable to obtain the expectations. To bridge this gap, we will produce estimates of the expectations.</p> <p>We may represent projection of residuals onto \(\mathcal{H}_j\) defined in (\ref{eq:projection_pop}) with the transformation of the residuals by the smoothing matrix \(\mathcal{S}_j\):</p> \[\begin{equation} \mathbb{E}\left[P_j\right] \approx \hat{P}_j = \mathcal{S}_j R_j. \label{eq:projection_data} \end{equation}\] <p>Consequently,</p> \[\begin{equation} \sqrt{\mathbb{E}\left[P_j^2\right]} \approx \hat{s}_j = \frac{1}{\sqrt{n}} \| \hat{P}_j \| = \sqrt{\text{mean}(\hat{P}_j^2)}. \\ \label{eq:projection_data_error} \end{equation}\] <p>One natural algorithm to solve the problem (\ref{eq:spam_lagrange}) is the coordinate descent algorithm. The coordinate descent algorithm is guaranteed to find the global minimum if the function to be optimized can be decomposed into</p> \[f(\beta_1, ..., \beta_p) = g(\beta_1, ..., \beta_p) + \sum_{j=1}^{p} h_j(\beta_j)\] <p>where \(g\) is both convex and differentiable, and \(h_j\) convex but not necessarily differentiable \cite{hastie_statistical_2016}. Obviously, additive models fit neatly within this framework. The authors call this method \textbf{backfitting}, which can be thought of as a functional version of coordinate descent.</p> <h1 id="discussion">Discussion</h1> <p>It is unfortunate that the authors of SpAM do not make a direct comparison with the COSSO considering their great similarities. While the authors of both papers apply their methods to the Boston data set, the authors of COSSO only report the prediction error, whereas the authors of SpAM only report the selected model. The authors of SpAM appear to have been more interested in how well their method selected variables rather than its overall performance.</p> <p>Without a direct comparison, it is difficult to determine where one method is superior to the other. The authors of the SpAM claim that the decoupling of sparsity and smoothing in their method provides flexibility than the COSSO. What cost is incurred by adding this flexibility? Perhaps the flexibility comes at no cost. Perhaps the flexibility comes at a great cost. The question goes unanswered for now and may be the subject of a future investigation.</p> <p>The authors of both papers present interesting ideas that offer solutions to the problem of selecting components from an additive model when \(p\) is large. Both techniques are similar as they both rely on \(l_1\) penalization methods and RKHS theory.</p>]]></content><author><name></name></author><category term="ml"/><category term="theory"/><category term="tutorials"/><summary type="html"><![CDATA[Generalized additive models are powerful but can easily overfit in high dimensions. Techniques like COSSO and its extensions use soft-thresholding to automatically select important components, making these models more reliable and interpretable.]]></summary></entry><entry><title type="html">The Double Descent Phenomenon</title><link href="https://jeffjmei.github.io/blog/2025/double-descent/" rel="alternate" type="text/html" title="The Double Descent Phenomenon"/><published>2025-04-12T22:25:00+00:00</published><updated>2025-04-12T22:25:00+00:00</updated><id>https://jeffjmei.github.io/blog/2025/double-descent</id><content type="html" xml:base="https://jeffjmei.github.io/blog/2025/double-descent/"><![CDATA[<h1 id="double-descent">Double Descent</h1> <p>The discovery of the descent phenomenon (Belkin, 2019) has upended the classical understanding of bias-variance trade-off. Classical understanding suggests that increasing model complexity inevitably leads to overfitting. A long standing mystery has been why neural networks have such successful generalization performance even when it has been “overfit” on training data. The double descent phenomenon demonstrates that the bias-variance trade-off is incomplete. For low capacity models, the bias-variance trade-off successfully explains generalization performance. However, for high capacity models (e.g. neural networks, random forest, etc.), increasing model complexity often leads to improved generalization error. Consequently, studying over-parameterized models – models that interpolate the training data to achieve perfect training error – have been an active field of research.</p> <p><img src="/assets/img/double-descent-curve.png" alt="Double Descent Curve" style="width:100%; max-width:600px;"/></p> <p>Recent evidence suggests SGD behaves differently in these over-parameterized contexts than in classical scenarios (Ma, et al., 2018; Belkin, 2021). In particular, whereas SGD tends to reach local minima in classical scenarios, SGD tends to reach global minima in over-parameterized regimes.</p> <p><img src="/assets/img/sgd-under-over-parameterized.png" alt="SGD Over-Parameterized" style="width:100%; max-width:600px;"/></p> <p>Moreover, while SGD with fixed learning rate does not converge in classical scenarios, it converges exponentially in over-parameterized scenarios.</p> <p><img src="/assets/img/double-descent-comparisons.png" alt="Double Descent Comparison" style="width:100%; max-width:600px;"/></p> <p>All in all, machine learning theory has long lagged behind our empirical understanding. The double-descent phenomenon and the surprising advantages of over-parameterization are only the first insights into explaining the remarkable success of machine learning models.</p> <p><strong>References:</strong></p> <ol> <li> <p><strong>Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S.</strong> (2019). <em>Reconciling modern machine-learning practice and the classical bias–variance trade-off.</em> Proceedings of the National Academy of Sciences, 116(32), 15849–15854. <a href="https://doi.org/10.1073/pnas.1903070116">https://doi.org/10.1073/pnas.1903070116</a></p> </li> <li> <p><strong>Ma, S., Bassily, R., &amp; Belkin, M.</strong> (2018). <em>The power of interpolation: Understanding the effectiveness of SGD in modern over-parameterized learning.</em> Proceedings of the 35th International Conference on Machine Learning (ICML), PMLR 80:3325–3334. <a href="http://proceedings.mlr.press/v80/ma18a/ma18a.pdf">http://proceedings.mlr.press/v80/ma18a/ma18a.pdf</a></p> </li> <li> <p><strong>Belkin, M.</strong> (2021). <em>Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation.</em> Acta Numerica, 30, 203–248. <a href="https://doi.org/10.1017/S0962492921000039">https://doi.org/10.1017/S0962492921000039</a></p> </li> </ol>]]></content><author><name></name></author><category term="ml"/><category term="theory"/><category term="tutorials"/><summary type="html"><![CDATA[Modern machine learning models defy the bias-variance trade-off. A new theory of machine learning must be developed to explain the double descent phenomenon and the success of over-parameterized models.]]></summary></entry></feed>