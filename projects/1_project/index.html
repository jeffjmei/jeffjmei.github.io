<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Additive Models | Jeff Mei </title> <meta name="author" content="Jeff Mei"> <meta name="description" content="A Literature Review"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jeffjmei.github.io/projects/1_project/"> <script src="/assets/js/theme.js?d6e73d9d499d950c2cdfcb59b98aa108"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jeff</span> Mei </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Additive Models</h1> <p class="post-description">A Literature Review</p> </header> <article> <p>Nonparametric additive models are flexible extensions of linear models. They can fit to complex smooth surfaces, but they suffer in high dimensions since they potentially fit to insignificant components. Therefore, a mechanism for filtering out insignificant components is of paramount importance. Since the advent of the LASSO <a class="citation" href="#tibshirani_regression_1996">(Tibshirani, 1996)</a>, many procedures have applied regularization techniques to instill variable selection capabilities. In this literature review, we cover two approaches that apply soft-thresholding to filter out insignificant components in the context of additive models. Lin and Zhang propose the COSSO (COmponent Selection and Smoothing Operator), which generalizes the LASSO to the context of additive models in order to facilitate component selection <a class="citation" href="#lin_component_2006">(Lin &amp; Zhang, 2006)</a>. Building on this work, <a class="citation" href="#ravikumar_sparse_2008">(Ravikumar et al., 2009)</a> proposes a variation that generalizes the COSSO and applies a backfitting procedure to facilitate variable selection.</p> <h1 id="additive-models">Additive Models</h1> <p>Additive models were introduced as a generalization of linear models <a class="citation" href="#hastie_generalized_1986">(Hastie &amp; Tibshirani, 1986)</a>. They take the form</p> \[\begin{equation} Y_i = \sum_{j=1}^{p} f_j(X_{ij}) + \varepsilon_i \label{eq:additive_model} \end{equation}\] <p>where \(\varepsilon\) is often gaussian noise. It is easy to see that if we let \(f(X) = X \beta\), then we have the linear model again. In practice, it is common to let \(f \in \mathcal{S}^2[0, 1]\), the second order Sobolev space on \([0, 1]\), defined as the set \(\left\{ f: f, f' \text{ abs. continuous}, \int_{0}^{1} (f''(x))^2 d{x} &lt; \infty \right\}\). This is an enormous class of functions that includes polynomials, \(\sin\), \(\cos\), \(\log\), and many more functions, making additive models an incredibly flexible approach to data modeling.</p> <p>It is worth noting that additive models can be extended to the class of Smoothing-Spline Analysis of Variance (SS-ANOVA) models if interactions are considered as well. That is, SS-ANOVA models take the form</p> \[\begin{equation} Y_i = \sum_{j=1}^{p} f_j(X_{ij}) + \sum_{j &lt; k} f_{jk}(X_{ij}) + ... + \varepsilon_i. \label{eq:ss_anova} \end{equation}\] <p>While additive models are incredibly flexible, one problem is that they suffer in high dimensions. Often times when \(p\) is large, there are components in the model that make little to no contributions to the model performance. However, without filtering out these features, the model will end up fitting to the noise. Therefore,</p> <p>In this literature review, we will provide an overview of methods used to overcome this issue. In particular we will consider the COmponent Selection and Smoothing Operator (COSSO) <a class="citation" href="#lin_component_2006">(Lin &amp; Zhang, 2006)</a> and the Sparse Additive Model (SpAM) <a class="citation" href="#ravikumar_sparse_2008">(Ravikumar et al., 2009)</a>.</p> <h1 id="reproducing-kernel-hilbert-spaces">Reproducing Kernel Hilbert Spaces</h1> <p>It is essential to understand the basics of Reproducing Kernel Hilbert Spaces (RKHS), as they play a significant role in both the COSSO and SpAM procedures.</p> <p>RKHSs are useful tools in the smoothing spline literature. They offer a feasible approach to access a potentially infinite dimensional class of functions (e.g. 2nd order Sobolev space). For a more detailed approach, refer to <a class="citation" href="#gu_smoothing_2002">(Gu, 2002)</a> and <a class="citation" href="#nosedal-sanchez_reproducing_2012">(Nosedal-Sanchez et al., 2012)</a>. Here, we will provide an informal overview of RKHS theory.</p> <p>A <strong>functional</strong> \(L\) is defined as a mapping from a linear space \(\mathcal{V}\) to the real numbers \(L: \mathcal{V} \to \mathbb{R}\). A functional of particular interest is the <strong>evaluation functional</strong> \([\cdot]\), such that \([x]f = f(x)\). That is, the evaluation functional \([x]\) is equal to evaluating the function \(f\) at \(x\).</p> <blockquote> <p><strong>Riesz Representation Theorem</strong></p> <p>Let \(\mathcal{H}\) be a Hilbert space with continuous functional \(L\) defined on it. For any \(f \in \mathcal{H}\), there exists a unique \(g \in \mathcal{H}\) such that \(Lf = \langle f, g \rangle.\)</p> </blockquote> <p>If we take \([x]\) to be the evaluation functional, it follows that for any continuous function \(f\), there exists some representer \(R_x\) such that \(\langle R_x, f \rangle = f(x)\). We will call \(R_x\) the <strong>reproducing kernel</strong> of the RKHS \(\mathcal{H}_R\).</p> <div class="row d-flex justify-content-center text-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/additive-models/Riesz-480.webp 480w,/assets/img/additive-models/Riesz-800.webp 800w,/assets/img/additive-models/Riesz-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/additive-models/Riesz.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="riesz-representation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Riesz Representation Theorem. </div> <p>The significance of this result is that it provides two different ways to evaluate a functional. We can evaluate it on the set \(X\) with the kernel, or we can project the data onto a different space \(\mathcal{H}_R\) and evaluate the functional using the inner product. Most often however, we will evaluate the functional using the kernel because it is typically more computationally convenient.</p> <p>To find a RKHS, we note that there is a convenient relationship between non-negative definite functions and reproducing kernels.</p> <blockquote> <p><strong>RKHS - Non-Negative Definite Relationship</strong></p> <p>For every non-negative definite function \(R(x, y)\) on \(X\), there exists a unique RKHS \(\mathcal{H}_R\) with the reproducing kernel \(R(x, y)\). The converse is also true. For every RKHS \(\mathcal{H}_R\), there exists a unique non-negative definite function \(R(x, y)\) on \(X\).</p> </blockquote> <div class="row d-flex justify-content-center text-center"> <div class="col-sm mt-3 mt-md-0" style="max-width: 500px;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/additive-models/RKHS-480.webp 480w,/assets/img/additive-models/RKHS-800.webp 800w,/assets/img/additive-models/RKHS-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/additive-models/RKHS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="rkhs" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption text-center"> Reproducing Kernel Hilbert Space. </div> <p>For the remainder of this paper, we define the inner product to be</p> \[\begin{equation} \langle f, g \rangle = \sum_{v = 0}^{m-1} \left( \int_{0}^{1} f^{(v)} d{x}\right) \left( \int_{0}^{1} g^{(v)} d{x}\right) + \int_{0}^{1} f^{(m)}g^{(m)} d{x}. \label{eq:inner_prod} \end{equation}\] <p>One important consequence of the RKHS framework, is that we are able to represent functions in \(\mathcal{H}_R\) by their kernel functions:</p> \[\begin{equation} f(x) = \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c + b \textbf{1}_n. \label{eq:rkhs_function} \end{equation}\] <p>By representing functions in \(\mathcal{H}_R\) by their kernels, we are able to obtain notions of “similarity” between functions in a richer space \(\mathcal{H}_R\) without having to do any computations within that richer space. Instead, our computations remain on the original space \(X\).</p> <h1 id="cosso-component-selection-and-smoothing">COSSO: Component Selection and Smoothing</h1> <p>The COSSO is a flexible approach that performs component selection to filter out small to insignificant components. This is similar to the LASSO <a class="citation" href="#tibshirani_regression_1996">(Tibshirani, 1996)</a>, but instead of penalizing on coefficient size, the COSSO penalizes, in an informal sense, “component size.” Roughly speaking, size of a function is described as the squared integral over \([0, 1]\). To develop this fully, we must dive into the theory of Reproducing Kernel Hilbert Spaces (RKHS).</p> <p>In particular, the problem that we are solving is</p> \[\begin{equation} \hat{f} = \text{argmin}_f \sum_{i=1}^{n} (y_i - f(x_i))^2 + \lambda_n \sum_{j=1}^{p} \| P^j f \| \label{eq:cosso} \end{equation}\] <p>where the norm in the 2nd order Sobolev space is</p> \[\begin{equation} \| f \| = \left( \int_{0}^{1} f(t) d{t} \right)^2 + \left( \int_{0}^{1} f'(t) d{t} \right)^2 + \int_{0}^{1} \left( f''(t) \right)^2 d{t}. \label{eq:norm} \end{equation}\] <p>The first term in (\ref{eq:cosso}) encourages the optimization problem to fit as closely to the data as possible, but it is penalized by the 2nd term, which penalizes by model complexity.</p> <h2 id="relationship-to-lasso">Relationship to LASSO</h2> <p>The authors demonstrate that the COSSO and the LASSO are the same. It is important to note that while the COSSO generalizes the LASSO, the interpretation changes. Instead of penalizing on coefficient size, we are penalizing by component size.</p> <h2 id="algorithm">Algorithm</h2> <p>The authors demonstrate that the COSSO can be decomposed into a non-negative garrote and a smoothing spline problem. Since the algorithm is guaranteed to improve its estimate on every iteration, we can just alternate between the non-negative garrote solution and the smoothing spline solution.</p> <p>The authors note that the original COSSO problem (\ref{eq:cosso}) can be reformulated into the more computationally tractable form</p> \[\begin{equation} \frac{1}{n} \left( y - \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c - b \textbf{1}_n \right)^T \left( y - \sum_{\alpha=1}^{p} \theta_\alpha R_\alpha c - b \textbf{1}_n \right) + \lambda_0 \sum_{\alpha=1}^{p} \theta_\alpha c^T R_\alpha c + \lambda \sum_{\alpha=1}^{p} \theta_\alpha. \label{eq:cosso_algo} \end{equation}\] <p>As it turns out, (\ref{eq:cosso_algo}) can be broken down further into two constituent sub-algorithms. In particular, when we fix \(c\) and \(b\), (\ref{eq:cosso_algo}) is reduced into the ridge regression. Similarly, when \(\theta\) is fixed, (\ref{eq:cosso_algo}) reduces into a non-negative garrote.</p> <p>By fixing \(c, b\), the COSSO reduces to the non-negative garrote, where components are selected:</p> \[\begin{equation} \min_\theta (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \label{eq:cosso_garrote} \end{equation}\] <p>where \(\theta_\alpha \ge 0\) and \(z = y - (1/2) n \lambda_0 c - b \textbf{1}_n.\)</p> <p>Similarly, by fixing \(\theta\), we get a problem equivalent to ridge regression, where the functions are smoothed:</p> \[\begin{equation} \min_{c, b} (y - R_\theta c - b \textbf{1}_n)^T (y - R_\theta c - b \textbf{1}_n) + n \lambda_0 c^T R_\theta c. \label{eq:cosso_ridge} \end{equation}\] <p>Now, with the insight that the COSSO can be broken down into problems with known solutions, the proposed algorithm flips between fixing \(\theta\) and fixing \(c \text{ and } b\). In other words, the algorithm flips between the non-negative garrote and ridge regression. We continue until algorithm converges on a solution with a pre-specified error.</p> \[\begin{aligned} &amp;\textbf{Algorithm: COSSO} \\ &amp;\textbf{Initialize:} \text{fix } \theta_\alpha = 1, \alpha = 1, ..., p, \; g(\theta, b, c) = 0 \\ &amp;\textbf{Repeat until convergence:} \\ &amp;\quad 1. \quad \text{Fix } \theta, \text{apply ridge regression (\ref{eq:cosso_ridge})} \\ &amp;\quad 2. \quad (c, b) \gets \text{argmin}_{c, b} \left[ (y - R_\theta c - b \mathbf{1}_n)^T (y - R_\theta c - b \mathbf{1}_n) + n \lambda_0 c^T R_\theta c \right] \\ &amp;\quad 3. \quad \text{Fix } b, c, \text{apply non-negative garrote (\ref{eq:cosso_garrote})} \\ &amp;\quad 4. \quad \theta \gets \text{argmin}_\theta \left[ (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \right] \\ &amp;\quad 5. \quad g(\theta, b, c) \gets \min_\theta \left[ (z - G \theta)^T (z - G \theta) + n \lambda \sum_{\alpha=1}^{p} \theta_\alpha \right] \end{aligned}\] <p>However, the authors note that the first iteration makes most of the way to a solution.</p> <h1 id="sparse-additive-models">Sparse Additive Models</h1> <p>Ravikumar et al. (2008) propose the Sparse Additive Model (SpAM), which is similar to the COSSO. Again, the model is proposed is another \(l_1\) penalized approach to add sparsity to the additive model. However, SpAM applies an additional constraint to normalize function size. In doing so, SpAM decouples sparsity and smoothness. Recall that the COSSO penalizes by component sizes, which are functions of both complexity and magnitude. By decoupling sparsity and smoothness, SpAM is more flexible than COSSO.</p> <p>The optimization problem for SpAM is</p> \[\begin{align} &amp;\min_{g_j \in H_j} \mathbb{E}\left[Y - \sum_{j=1}^{p} \beta_j g_j(X_j) \right]^2 \\ &amp;\text{s.t.: } \sum_{j=1}^{p} |\beta_j| \le L \\ &amp; \qquad \mathbb{E}\left[g_j^2\right] = 1. \label{eq:spam} \end{align}\] <p>where \(Y\) is an \(n \times 1\) vector representing the outputs to be predicted, \(X\) is an \(n \times p\) data matrix, \(L \ge 0\) is a penalty constraint, and \(\mathcal{H}_j\) is a RKHS for \(j=1,...,p.\)</p> <p>In the LASSO, we penalize the regression coefficients by taking the norm of the \(\beta\) vector. Here, we take the same idea to encourage sparsity, in addition to adding an additional constraint of \(\mathbb{E}\left[g_j^2\right] = 1\) to limit the set of functions to search.</p> <p>We can rewrite the above constraint to be</p> \[\begin{align*} \min_{f_j \in H_j} &amp;\mathbb{E}\left[Y - \sum_{j=1}^{p} \beta_j g_j(X_j) \right]^2 \\ \text{subject to: } &amp; \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \le L. \end{align*}\] <p>Or equivalently</p> \[\begin{equation} \mathcal{L}(f, \lambda) = \frac{1}{2} \mathbb{E}\left[Y - \sum_{j=1}^{p} f_j(X_j) \right]^2 + \lambda \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]}. \label{eq:spam_lagrange} \end{equation}\] <p>Proof:</p> \[\begin{align*} f_j(X_j) &amp;= \beta_j g_j (X_j) \\ g_j(X_j) &amp;= f_j(X_j) / \beta_j \\ \mathbb{E}\left[g_j(X_j)^2\right] &amp;= \mathbb{E}\left[f_j(X_j)^2 / \beta_j^2\right] = 1 \\ \beta_j^2 &amp;= \mathbb{E}\left[f_j^2(X_j)\right]\\ \beta_j &amp;= \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \\ \sum_{j=1}^{p} |\beta_j| &amp;= \sum_{j=1}^{p} \sqrt{\mathbb{E}\left[f_j^2(X_j)\right]} \le L . \end{align*}\] <p>The authors demonstrate that the minimizers can be expressed as the soft-thresholded projection</p> \[\begin{equation} f_j = \left[ 1 - \frac{\lambda}{\sqrt{\mathbb{E}\left[P_j^2\right]}} \right]_+ \mathbb{E}\left[R_j|X_j\right]. % P_j. \label{eq:spam_minimizers} \end{equation}\] <p>Where residuals excluding the contribution of the \(j\)th component is \(R_j = Y - \sum_{k\neq j}^{} f_k(X_k)\) and the projection from the residuals onto \(\mathcal{H}_j\) is</p> \[\begin{equation} P_j = \mathbb{E}\left[R_j|X_j\right]. \label{eq:projection_pop} \end{equation}\] <p>Equation (\ref{eq:spam_minimizers}) illuminates the inner workings of SpAM. In particular, we can see that the population minimizer is a soft-thresholded projection onto \(\mathcal{H}_j\) where the projection \(P_j\) attempts to reconstruct the signal using information exclusively in the \(j\)th component.</p> <h2 id="algorithm-1">Algorithm:</h2> <p>The problem with our formulation in (\ref{eq:spam_minimizers}) is that it requires information on the population in \(E[P_j^2]\) and \(E[R_j \mid X_j]\). In most practical situations, we will not know the probability distributions and will thus be unable to obtain the expectations. To bridge this gap, we will produce estimates of the expectations.</p> <p>We may represent projection of residuals onto \(\mathcal{H}_j\) defined in (\ref{eq:projection_pop}) with the transformation of the residuals by the smoothing matrix \(\mathcal{S}_j\):</p> \[\begin{equation} \mathbb{E}\left[P_j\right] \approx \hat{P}_j = \mathcal{S}_j R_j. \label{eq:projection_data} \end{equation}\] <p>Consequently,</p> \[\begin{equation} \sqrt{\mathbb{E}\left[P_j^2\right]} \approx \hat{s}_j = \frac{1}{\sqrt{n}} \| \hat{P}_j \| = \sqrt{\text{mean}(\hat{P}_j^2)}. \\ \label{eq:projection_data_error} \end{equation}\] <p>One natural algorithm to solve the problem (\ref{eq:spam_lagrange}) is the coordinate descent algorithm. The coordinate descent algorithm is guaranteed to find the global minimum if the function to be optimized can be decomposed into</p> \[f(\beta_1, ..., \beta_p) = g(\beta_1, ..., \beta_p) + \sum_{j=1}^{p} h_j(\beta_j)\] <p>where \(g\) is both convex and differentiable, and \(h_j\) convex but not necessarily differentiable \cite{hastie_statistical_2016}. Obviously, additive models fit neatly within this framework. The authors call this method \textbf{backfitting}, which can be thought of as a functional version of coordinate descent.</p> <h1 id="discussion">Discussion</h1> <p>It is unfortunate that the authors of SpAM do not make a direct comparison with the COSSO considering their great similarities. While the authors of both papers apply their methods to the Boston data set, the authors of COSSO only report the prediction error, whereas the authors of SpAM only report the selected model. The authors of SpAM appear to have been more interested in how well their method selected variables rather than its overall performance.</p> <p>Without a direct comparison, it is difficult to determine where one method is superior to the other. The authors of the SpAM claim that the decoupling of sparsity and smoothing in their method provides flexibility than the COSSO. What cost is incurred by adding this flexibility? Perhaps the flexibility comes at no cost. Perhaps the flexibility comes at a great cost. The question goes unanswered for now and may be the subject of a future investigation.</p> <p>The authors of both papers present interesting ideas that offer solutions to the problem of selecting components from an additive model when \(p\) is large. Both techniques are similar as they both rely on \(l_1\) penalization methods and RKHS theory.</p> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="nosedal-sanchez_reproducing_2012" class="col-sm-8"> <div class="title">Reproducing Kernel Hilbert Spaces for Penalized Regression: A Tutorial</div> <div class="author"> Alvaro Nosedal-Sanchez, Curtis B. Storlie, Thomas C.M. Lee, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ronald Christensen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>The American Statistician</em>, Feb 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1080/00031305.2012.678196" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">nosedal-sanchez_reproducing_2012</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {Regression}:
             {A} {Tutorial}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{66}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0003-1305, 1537-2731}</span><span class="p">,</span>
  <span class="na">shorttitle</span> <span class="p">=</span> <span class="s">{Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {
                  Regression}}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.678196}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/00031305.2012.678196}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-12-03}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The American Statistician}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nosedal-Sanchez, Alvaro and Storlie, Curtis B. and Lee, Thomas C.M. and Christensen, Ronald}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">feb</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{50--60}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{Nosedal-Sanchez et al. - 2012 - Reproducing Kernel Hilbert Spaces for
            Penalized Re.pdf:/home/jmei/Zotero/storage/DWYKGSDX/Nosedal-Sanchez et
            al. - 2012 - Reproducing Kernel Hilbert Spaces for Penalized
            Re.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2009</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JRSS-B</abbr> </div> <div id="ravikumar_sparse_2008" class="col-sm-8"> <div class="title">Sparse additive models</div> <div class="author"> Pradeep Ravikumar, John Lafferty, Han Liu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Larry Wasserman' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, Nov 2009 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/j.1467-9868.2009.00718.x" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We present a new class of methods for high dimensional non-parametric regression and classiﬁcation called sparse additive models. Our methods combine ideas from sparse linear modelling and additive non-parametric regression. We derive an algorithm for ﬁtting the models that is practical and effective even when the number of covariates is larger than the sample size. Sparse additive models are essentially a functional version of the grouped lasso of Yuan and Lin. They are also closely related to the COSSO model of Lin and Zhang but decouple smoothing and sparsity, enabling the use of arbitrary non-parametric smoothers. We give an analysis of the theoretical properties of sparse additive models and present empirical results on synthetic and real data, showing that they can be effective in ﬁtting sparse non-parametric models in high dimensional data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ravikumar_sparse_2008</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse additive models}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{71}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{13697412, 14679868}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00718.x}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/j.1467-9868.2009.00718.x}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-12-06}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of the Royal Statistical Society: Series B (Statistical
               Methodology)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2009}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1009--1030}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{Ravikumar et al. - 2009 - Sparse additive
            models.pdf:/home/jmei/Zotero/storage/WMPRNGNR/Ravikumar et al. - 2009 -
            Sparse additive models.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2006</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AoS</abbr> </div> <div id="lin_component_2006" class="col-sm-8"> <div class="title">Component Selection and Smoothing in Multivariate Nonparametric Regression</div> <div class="author"> Yi Lin, and Hao Helen Zhang </div> <div class="periodical"> <em>The Annals of Statistics</em>, Nov 2006 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1214/009053606000000722" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We propose a new method for model selection and model fitting in multivariate nonparametric regression models, in the framework of smoothing spline ANOVA. The "COSSO" is a method of regularization with the penalty functional being the sum of component norms, instead of the squared norm employed in the traditional smoothing spline method. The COSSO provides a unified framework for several recent proposals for model selection in linear models and smoothing spline ANOVA models. Theoretical properties, such as the existence and the rate of convergence of the COSSO estimator, are studied. In the special case of a tensor product design with periodic functions , a detailed analysis reveals that the COSSO does model selection by applying a novel soft thresholding type operation to the function components. We give an equivalent formulation of the COSSO estimator which leads naturally to an iterative algorithm. We compare the COSSO with MARS, a popular method that builds functional ANOVA models, in simulations and real examples. The COSSO method can be extended to classification problems and we compare its performance with those of a number of machine learning algorithms on real datasets. The COSSO gives very competitive performance in these studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">lin_component_2006</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Component {Selection} and {Smoothing} in {Multivariate} {
             Nonparametric} {Regression}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1214/009053606000000722}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{5}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2021-11-14}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The Annals of Statistics}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Yi and Zhang, Hao Helen}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2006}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2272--2297}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{PDF:/home/jmei/Zotero/storage/SGM5EUWM/full-text.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2002</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="gu_smoothing_2002" class="col-sm-8"> <div class="title">Smoothing Spline ANOVA Models</div> <div class="author"> Chong Gu </div> <div class="periodical"> Nov 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@book</span><span class="p">{</span><span class="nl">gu_smoothing_2002</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Smoothing {Spline} {ANOVA} {Models}}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-1-4419-2966-2}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Springe}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Chong}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2002}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{PDF:/home/jmei/Zotero/storage/FZWVBAN8/Bickel et al. - 2002 - Springer
            Series in Statistics Springer Science Business Media ,
            LLC.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">1996</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JRSS-B</abbr> </div> <div id="tibshirani_regression_1996" class="col-sm-8"> <div class="title">Regression Shrinkage and Selection Via the Lasso</div> <div class="author"> Robert Tibshirani </div> <div class="periodical"> <em>Journal of the Royal Statistical Society: Series B (Methodological) </em>, Jan 1996 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>We propose a new method for estimation in linear models. The ’lasso’ minim residual sum of squares subject to the sum of the absolute value of the coefficients than a constant. Because of the nature of this constraint it tends to produce coefficients that are exactly 0 and hence gives interpretable models. Our simulatio suggest that the lasso enjoys some of the favourable properties of both subset sele ridge regression. It produces interpretable models like subset selection and exh stability of ridge regression. There is also an interesting relationship with recent adaptive function estimation by Donoho and Johnstone. The lasso idea is quite ge can be applied in a variety of statistical models: extensions to generalized regressio and tree-based models are briefly described.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">tibshirani_regression_1996</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Regression {Shrinkage} and {Selection} {Via} the {Lasso}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{58}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{00359246}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/j.2517-6161.1996.tb02080.x}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">urldate</span> <span class="p">=</span> <span class="s">{2022-12-08}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of the Royal Statistical Society: Series B (Methodological)
               }</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Tibshirani, Robert}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{1996}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{267--288}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{Tibshirani - 1996 - Regression Shrinkage and Selection Via the
            Lasso.pdf:/home/jmei/Zotero/storage/EUCMIGEF/Tibshirani - 1996 -
            Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">1986</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Stat Sci</abbr> </div> <div id="hastie_generalized_1986" class="col-sm-8"> <div class="title">Generalized Additive Models</div> <div class="author"> Trevor Hastie, and Robert Tibshirani </div> <div class="periodical"> <em>Statistical Science</em>, Jan 1986 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Likelihood-based regression models such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariates Xlt X2, ■•-, Xp. We introduce the class of generalized additive models which replaces the linear form 2 (IjXj by a sum of smooth functions _£ $j(Xj). The Sj(-)’s are unspecified functions that are estimated using a scatterplot smoother, in an iterative procedure we call the local scoring algorithm. The technique is applicable to any likelihood-based regression model: the class of generalized linear models contains many of these. In this class the linear predictor tj = £ fyXj is replaced by the additive predictor £ Sj(Xj); hence, the name generalized additive models. We illustrate the technique with binary response and survival data. In both cases, the method proves to be useful in uncovering nonlinear covariate effects. It has the advantage of being completely auto matic, i.e., no " detective work" is needed on the part of the statistician. As a theoretical underpinning, the technique is viewed as an empirical method of maximizing the expected log likelihood, or equivalently, of minimizing the Kullback-Leibler distance to the true model.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hastie_generalized_1986</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Generalized {Additive} {Models}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">language</span> <span class="p">=</span> <span class="s">{en}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Statistical Science}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hastie, Trevor and Tibshirani, Robert}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{1986}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{297--318}</span><span class="p">,</span>
  <span class="na">file</span> <span class="p">=</span> <span class="s">{Hastie and Tibshirani - Generalized Additive
            Models.pdf:/home/jmei/Zotero/storage/GGJBSWYW/Hastie and Tibshirani -
            Generalized Additive Models.pdf:application/pdf}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeff Mei. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>for(var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.querySelectorAll('[id="WeChatBtn"]'),i=0;i<wechatBtn.length;i++)wechatBtn[i].onclick=function(){wechatModal.style.display="block"};window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>