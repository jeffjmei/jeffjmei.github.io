---
layout: post
title: Reproducibility Crisis
date: 2025-04-25 22:25:00
description: It has come to recent attention that a great deal of published research findings cannot be replicated. While the extent of the problem varies by field, the problem has been attributed to a variety of bad statistical practices -- underpowered studies, data dredging, p-hacking -- an entire lexicon developed explicitly to verbalize statistical malpractice.
tags: ml theory tutorials
tikzjax: true
featured: true
---

# P-Hacking
It has come to recent attention that a great deal of published research findings cannot be replicated \cite{ioannidis_why_2018}. While the extent of the problem varies by field, the problem has been attributed to a variety of bad statistical practices: underpowered studies, data dredging, p-hacking -- an entire lexicon developed explicitly to verbalize statistical malpractice. The issue has become so pronounced that some journals have resorted to banning $p$-values altogether. There have, however, been some attempts at rectifying the impacts $p$-hacking has on the literature through the analysis of $p$-curves \cite{simonsohn_p-curve_nodate} \cite{head_extent_2015}. 

In this simulation study, we attempt to illustrate the severity of each of these bad statistical practices. When a researcher commits statistical sin, how big of an impact does that have on their scientific result in terms of the bias introduced? We address this question by roughly following the analyses presented by other authors in the literature \cite{stefan_big_2023} \cite{button_power_2013}. Moreover, we also address the question of how well can $p$-hacking detection procedures can detect $p$-hacking. For this, we introduce original simulations to assess the question -- fusing results analyzed in the $p$-hacking and importing them into the $p$-hacking detection procedure.  

