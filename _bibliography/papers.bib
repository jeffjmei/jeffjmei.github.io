---
---

@string{aps = {American Physical Society,}}

@book{einstein1920relativity,
  title = {Relativity: the Special and General Theory},
  author = {Einstein, Albert},
  year = {1920},
  publisher = {Methuen & Co Ltd},
  html = {relativity.html},
}

@book{einstein1956investigations,
  bibtex_show = {true},
  title = {Investigations on the Theory of the Brownian Movement},
  author = {Einstein, Albert},
  year = {1956},
  publisher = {Courier Corporation},
  preview = {brownian-motion.gif},
}

@article{einstein1950meaning,
  abbr = {AJP},
  bibtex_show = {true},
  title = {The meaning of relativity},
  author = {Einstein, Albert and Taub, AH},
  journal = {American Journal of Physics},
  volume = {18},
  number = {6},
  pages = {403--404},
  year = {1950},
  publisher = {American Association of Physics Teachers},
}

@article{PhysRev.47.777,
  abbr = {PhysRev},
  title = {Can Quantum-Mechanical Description of Physical Reality Be Considered
           Complete?},
  author = {Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract = {In a complete theory there is an element corresponding to each
              element of reality. A sufficient condition for the reality of a
              physical quantity is the possibility of predicting it with
              certainty, without disturbing the system. In quantum mechanics in
              the case of two physical quantities described by non-commuting
              operators, the knowledge of one precludes the knowledge of the
              other. Then either (1) the description of reality given by the wave
              function in quantum mechanics is not complete or (2) these two
              quantities cannot have simultaneous reality. Consideration of the
              problem of making predictions concerning a system on the basis of
              measurements made on another system that had previously interacted
              with it leads to the result that if (1) is false then (2) is also
              false. One is thus led to conclude that the description of reality
              as given by a wave function is not complete.},
  journal = {Phys. Rev.},
  location = {New Jersey},
  volume = {47},
  issue = {10},
  pages = {777--780},
  numpages = {0},
  year = {1935},
  month = {May},
  publisher = aps,
  doi = {10.1103/PhysRev.47.777},
  url = {http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf = {example_pdf.pdf},
  altmetric = {248277},
  dimensions = {true},
  google_scholar_id = {qyhmnyLat1gC},
  video = {https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info = {. *More Information* can be [found
                     here](https://github.com/alshedivat/al-folio/)},
  annotation = {* Example use of superscripts<br>† Albert Einstein},
  selected = {true},
  inspirehep_id = {3255},
}

@article{einstein1905molekularkinetischen,
  title = {{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme
           geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten
           Teilchen},
  author = {Einstein, A.},
  journal = {Annalen der physik},
  volume = {322},
  number = {8},
  pages = {549--560},
  year = {1905},
  publisher = {Wiley Online Library},
}

@article{einstein1905movement,
  abbr = {Ann. Phys.},
  title = {Un the movement of small particles suspended in statiunary liquids
           required by the molecular-kinetic theory 0f heat},
  author = {Einstein, A.},
  journal = {Ann. Phys.},
  volume = {17},
  pages = {549--560},
  year = {1905},
}

@article{einstein1905electrodynamics,
  title = {On the electrodynamics of moving bodies},
  author = {Einstein, A.},
  year = {1905},
}

@article{einstein1905photoelectriceffect,
  bibtex_show = {true},
  abbr = {Ann. Phys.},
  title = "{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes
           betreffenden heuristischen Gesichtspunkt}",
  author = {Albert Einstein},
  abstract = {This is the abstract text.},
  journal = {Ann. Phys.},
  volume = {322},
  number = {6},
  pages = {132--148},
  year = {1905},
  doi = {10.1002/andp.19053220607},
  award = {Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his
           services to Theoretical Physics, and especially for his discovery of
           the law of the photoelectric effect*},
  award_name = {Nobel Prize},
}

@book{przibram1967letters,
  bibtex_show = {true},
  title = {Letters on wave mechanics},
  author = {Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz,
            Hendrik Antoon and Przibram, Karl},
  year = {1967},
  publisher = {Vision},
  preview = {wave-mechanics.gif},
  abbr = {Vision},
}

@article{tibshirani_regression_1996,
  abbr = {JRSS-B},
  bibtex_show = {true},
  title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
  volume = {58},
  issn = {00359246},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
  doi = {10.1111/j.2517-6161.1996.tb02080.x},
  abstract = {We propose a new method for estimation in linear models. The
              'lasso' minim residual sum of squares subject to the sum of the
              absolute value of the coefficients than a constant. Because of the
              nature of this constraint it tends to produce coefficients that are
              exactly 0 and hence gives interpretable models. Our simulatio
              suggest that the lasso enjoys some of the favourable properties of
              both subset sele ridge regression. It produces interpretable models
              like subset selection and exh stability of ridge regression. There
              is also an interesting relationship with recent adaptive function
              estimation by Donoho and Johnstone. The lasso idea is quite ge can
              be applied in a variety of statistical models: extensions to
              generalized regressio and tree-based models are briefly described.},
  language = {en},
  number = {1},
  urldate = {2022-12-08},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)
             },
  author = {Tibshirani, Robert},
  month = jan,
  year = {1996},
  pages = {267--288},
  file = {Tibshirani - 1996 - Regression Shrinkage and Selection Via the
          Lasso.pdf:/home/jmei/Zotero/storage/EUCMIGEF/Tibshirani - 1996 -
          Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@article{lin_component_2006,
  abbr = {AoS},
  bibtex_show = {true},
  title = {Component {Selection} and {Smoothing} in {Multivariate} {
           Nonparametric} {Regression}},
  volume = {34},
  doi = {10.1214/009053606000000722},
  abstract = {We propose a new method for model selection and model fitting in
              multivariate nonparametric regression models, in the framework of
              smoothing spline ANOVA. The "COSSO" is a method of regularization
              with the penalty functional being the sum of component norms,
              instead of the squared norm employed in the traditional smoothing
              spline method. The COSSO provides a unified framework for several
              recent proposals for model selection in linear models and smoothing
              spline ANOVA models. Theoretical properties, such as the existence
              and the rate of convergence of the COSSO estimator, are studied. In
              the special case of a tensor product design with periodic functions
              , a detailed analysis reveals that the COSSO does model selection
              by applying a novel soft thresholding type operation to the
              function components. We give an equivalent formulation of the COSSO
              estimator which leads naturally to an iterative algorithm. We
              compare the COSSO with MARS, a popular method that builds
              functional ANOVA models, in simulations and real examples. The
              COSSO method can be extended to classification problems and we
              compare its performance with those of a number of machine learning
              algorithms on real datasets. The COSSO gives very competitive
              performance in these studies.},
  number = {5},
  urldate = {2021-11-14},
  journal = {The Annals of Statistics},
  author = {Lin, Yi and Zhang, Hao Helen},
  year = {2006},
  pages = {2272--2297},
  file = {PDF:/home/jmei/Zotero/storage/SGM5EUWM/full-text.pdf:application/pdf},
}

@article{ravikumar_sparse_2008,
  abbr = {JRSS-B},
  bibtex_show = {true},
  title = {Sparse additive models},
  volume = {71},
  issn = {13697412, 14679868},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00718.x},
  doi = {10.1111/j.1467-9868.2009.00718.x},
  abstract = {We present a new class of methods for high dimensional
              non-parametric regression and classiﬁcation called sparse additive
              models. Our methods combine ideas from sparse linear modelling and
              additive non-parametric regression. We derive an algorithm for
              ﬁtting the models that is practical and effective even when the
              number of covariates is larger than the sample size. Sparse
              additive models are essentially a functional version of the grouped
              lasso of Yuan and Lin. They are also closely related to the COSSO
              model of Lin and Zhang but decouple smoothing and sparsity,
              enabling the use of arbitrary non-parametric smoothers. We give an
              analysis of the theoretical properties of sparse additive models
              and present empirical results on synthetic and real data, showing
              that they can be effective in ﬁtting sparse non-parametric models
              in high dimensional data.},
  language = {en},
  number = {5},
  urldate = {2022-12-06},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical
             Methodology)},
  author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman,
            Larry},
  month = nov,
  year = {2009},
  pages = {1009--1030},
  file = {Ravikumar et al. - 2009 - Sparse additive
          models.pdf:/home/jmei/Zotero/storage/WMPRNGNR/Ravikumar et al. - 2009 -
          Sparse additive models.pdf:application/pdf},
}

@article{hastie_generalized_1986,
  abbr = {Stat Sci},
  bibtex_show = {true},
  title = {Generalized {Additive} {Models}},
  volume = {1},
  abstract = {Likelihood-based regression models such as the normal linear
              regression model and the linear logistic model, assume a linear (or
              some other parametric) form for the covariates Xlt X2, ■•-, Xp. We
              introduce the class of generalized additive models which replaces
              the linear form 2 (IjXj by a sum of smooth functions \_£ \$j(Xj).
              The Sj(-)'s are unspecified functions that are estimated using a
              scatterplot smoother, in an iterative procedure we call the local
              scoring algorithm. The technique is applicable to any
              likelihood-based regression model: the class of generalized linear
              models contains many of these. In this class the linear predictor
              tj = £ fyXj is replaced by the additive predictor £ Sj(Xj); hence,
              the name generalized additive models. We illustrate the technique
              with binary response and survival data. In both cases, the method
              proves to be useful in uncovering nonlinear covariate effects. It
              has the advantage of being completely auto matic, i.e., no "
              detective work" is needed on the part of the statistician. As a
              theoretical underpinning, the technique is viewed as an empirical
              method of maximizing the expected log likelihood, or equivalently,
              of minimizing the Kullback-Leibler distance to the true model.},
  language = {en},
  number = {3},
  journal = {Statistical Science},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1986},
  pages = {297--318},
  file = {Hastie and Tibshirani - Generalized Additive
          Models.pdf:/home/jmei/Zotero/storage/GGJBSWYW/Hastie and Tibshirani -
          Generalized Additive Models.pdf:application/pdf},
}

@book{gu_smoothing_2002,
  bibtex_show = {true},
  title = {Smoothing {Spline} {ANOVA} {Models}},
  isbn = {978-1-4419-2966-2},
  publisher = {Springe},
  author = {Gu, Chong},
  year = {2002},
  file = {PDF:/home/jmei/Zotero/storage/FZWVBAN8/Bickel et al. - 2002 - Springer
          Series in Statistics Springer Science Business Media ,
          LLC.pdf:application/pdf},
}

@article{nosedal-sanchez_reproducing_2012,
  bibtex_show = {true},
  title = {Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {Regression}:
           {A} {Tutorial}},
  volume = {66},
  issn = {0003-1305, 1537-2731},
  shorttitle = {Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {
                Regression}},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.678196},
  doi = {10.1080/00031305.2012.678196},
  language = {en},
  number = {1},
  urldate = {2022-12-03},
  journal = {The American Statistician},
  author = {Nosedal-Sanchez, Alvaro and Storlie, Curtis B. and Lee, Thomas C.M.
            and Christensen, Ronald},
  month = feb,
  year = {2012},
  pages = {50--60},
  file = {Nosedal-Sanchez et al. - 2012 - Reproducing Kernel Hilbert Spaces for
          Penalized Re.pdf:/home/jmei/Zotero/storage/DWYKGSDX/Nosedal-Sanchez et
          al. - 2012 - Reproducing Kernel Hilbert Spaces for Penalized
          Re.pdf:application/pdf},
}

@article{hastie_statistical_2016,
  bibtex_show = {true},
  title = {Statistical {Learning} with {Sparsity} {Monographs} on {Statistics}
           and {Applied} {Probability}},
  author = {Hastie, Trevor and Tibshirani, Robert and Hastie, Martin Wainwright
            and Tibshirani, @bullet and Wainwright, @bullet},
  year = {2016},
  pages = {362},
  file = {PDF:/home/jmei/Zotero/storage/9P9QP86Q/Hastie et al. - 2016 -
          Statistical Learning with Sparsity Monographs on Statistics and Applied
          Probability 143 143 copy to come from cop.pdf:application/pdf},
}
