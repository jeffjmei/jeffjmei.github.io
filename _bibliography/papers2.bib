
@article{Taylor2015,
  title = {Statistical learning and selective inference},
  volume = {112},
  issn = {10916490},
  doi = {10.1073/pnas.1507583112},
  abstract = {We describe the problem of "selective inference." This addresses
              the following challenge: Having mined a set of data to find
              potential associations, how do we properly assess the strength of
              these associations? The fact that we have "cherry-picked" -
              searched for the strongest associations - means that we must set a
              higher bar for declaring significant the associations that we see.
              This challenge becomes more important in the era of big data and
              complex statistical modeling. The cherry tree (dataset) can be very
              large and the tools for cherry picking (statistical learning
              methods) are now very sophisticated. We describe some recent new
              developments in selective inference and illustrate their use in
              forward stepwise regression, the lasso, and principal components
              analysis.},
  number = {25},
  journal = {Proceedings of the National Academy of Sciences of the United
             States of America},
  author = {Taylor, Jonathan and Tibshirani, Robert J.},
  year = {2015},
  keywords = {Inference, Lasso, P values},
  pages = {7629--7634},
  file = {
          PDF:/home/jmei/Zotero/storage/CWN7HJDC/Statistical_Learning_and_Selective_Inference.pdf:application/pdf
          },
}

@article{efron_prediction_2020,
  title = {Prediction , {Estimation} , and {Attribution}},
  volume = {0},
  issn = {0162-1459},
  url = {https://doi.org/10.1080/01621459.2020.1762613},
  doi = {10.1080/01621459.2020.1762613},
  number = {0},
  journal = {Journal of the American Statistical Association},
  author = {Efron, Bradley},
  year = {2020},
  note = {Publisher: Taylor \& Francis},
  keywords = {black box, ephemeral, predictors, random forests},
  pages = {1--44},
  file = {PDF:/home/jmei/Zotero/storage/IJMMQAEU/Prediction Estimation and
          Attribution.pdf:application/pdf},
}

@article{turner_brief_2009,
  title = {A {Brief} {Introduction} to {Mathematical} {Writing}},
  author = {Turner, William J},
  year = {2009},
  pages = {1--7},
  file = {
          PDF:/home/jmei/Zotero/storage/3S2YFQHP/mathematical_writing.pdf:application/pdf
          },
}

@techreport{efron_large-scale_nodate,
  title = {Large-{Scale} {Inference}: {Empirical} {Bayes} {Methods} for {
           Estimation}, {Testing} and {Prediction}},
  urldate = {2021-03-15},
  author = {Efron, Bradley},
  file = {PDF:/home/jmei/Zotero/storage/I3HTZ445/full-text.pdf:application/pdf},
}

@article{morozov_modelling_2013,
  title = {Modelling biological evolution: recent progress, current challenges
           and future direction},
  url = {http://dx.doi.org/10.1098/rsfs.2013.0054},
  doi = {10.1098/rsfs.2013.0054},
  journal = {Royal Society Publishing},
  author = {Morozov, Andrew},
  year = {2013},
  file = {PDF:/home/jmei/Zotero/storage/44N9M6W7/full-text.pdf:application/pdf},
}

@article{arnaoudova_statistical_2010,
  title = {Statistical phylogenetic tree analysis using differences of means},
  volume = {4},
  url = {www.frontiersin.org},
  doi = {10.3389/fnins.2010.00047},
  abstract = {We propose a statistical method to test whether two phylogenetic
              trees with given alignments are significantly incongruent. Our
              method compares the two distributions of phylogenetic trees given
              by two input alignments, instead of comparing point estimations of
              trees. This statistical approach can be applied to gene tree
              analysis for example, detecting unusual events in genome evolution
              such as horizontal gene transfer and reshuffling. Our method uses
              difference of means to compare two distributions of trees, after
              mapping trees into a vector space. Bootstrapping alignment columns
              can then be applied to obtain p-values. To compute distances
              between means, we employ a "kernel method" which speeds up distance
              calculations when trees are mapped in a high-dimensional feature
              space, e.g., splits or quartets feature space. In this pilot study,
              first we test our statistical method on data sets simulated under a
              coalescence model, to test whether two alignments are generated by
              congruent gene trees. We follow our simulation results with
              applications to data sets of gophers and lice, grasses and their
              endophytes, and different fungal genes from the same genome. A
              companion toolkit, Phylotree, is provided to facilitate
              computational experiments.},
  number = {1},
  urldate = {2021-03-14},
  journal = {Article},
  author = {Arnaoudova, Elissaveta and Haws, David C and Huggins, Peter and
            Jaromczyk, Jerzy W and Moore, Neil and Schardl, Christopher L and
            Yoshida, Ruriko and Robeva, Raina and W Nye, Tom M and Liu, Liang},
  year = {2010},
  keywords = {difference of means, phylogenetic trees, tree congruency},
  file = {PDF:/home/jmei/Zotero/storage/DFLA4MKU/full-text.pdf:application/pdf},
}

@article{mayer_why_2021,
  title = {Why {McConnell} {Dumped} {Trump}},
  author = {Mayer, By Jane},
  year = {2021},
  pages = {1--12},
  file = {PDF:/home/jmei/Zotero/storage/7LZ5XKKX/Why McConnell Dumped Trump The
          New Yorker.pdf:application/pdf},
}

@article{Hoekstra2014,
  title = {Robust misinterpretation of confidence intervals},
  volume = {21},
  issn = {15315320},
  doi = {10.3758/s13423-013-0572-3},
  abstract = {Null hypothesis significance testing (NHST) is undoubtedly the
              most common inferential technique used to justify claims in the
              social sciences. However, even staunch defenders of NHST agree that
              its outcomes are often misinterpreted. Confidence intervals (CIs)
              have frequently been proposed as a more useful alternative to NHST,
              and their use is strongly encouraged in the APA Manual.
              Nevertheless, little is known about how researchers interpret CIs.
              In this study, 120 researchers and 442 students—all in the field of
              psychology—were asked to assess the truth value of six particular
              statements involving different interpretations of a CI. Although
              all six statements were false, both researchers and students
              endorsed, on average, more than three statements, indicating a
              gross misunderstanding of CIs. Self-declared experience with
              statistics was not related to researchers’ performance, and, even
              more surprisingly, researchers hardly outperformed the students,
              even though the students had not received any education on
              statistical inference whatsoever. Our findings suggest that many
              researchers do not know the correct interpretation of a CI. The
              misunderstandings surrounding p-values and CIs are particularly
              unfortunate because they constitute the main tools by which
              psychologists draw conclusions from data.},
  number = {5},
  journal = {Psychonomic Bulletin and Review},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and
            Wagenmakers, Eric Jan},
  year = {2014},
  pmid = {24420726},
  keywords = {Inference, Confidence intervals, Significance testing},
  pages = {1157--1164},
  file = {PDF:/home/jmei/Zotero/storage/YZP7NA3H/robust misinterpretation of
          confidence intervals.pdf:application/pdf},
}

@article{Amei2014,
  title = {Robust estimates of divergence times and selection with a poisson
           random field model: {A} case study of comparative phylogeographic data
           },
  volume = {196},
  issn = {19432631},
  doi = {10.1534/genetics.113.157776},
  abstract = {Mutation frequencies can be modeled as a Poisson random field
              (PRF) to estimate speciation times and the degree of selection on
              newly arisen mutations. This approach provides a quantitative
              theory for comparing intraspecific polymorphism with interspecific
              divergence in the presence of selection and can be used to estimate
              population genetic parameters. Although the original PRF model has
              been extended to more general biological settings to make
              statistical inference about selection and divergence among model
              organisms, it has not been incorporated into phylogeographic
              studies that focus on estimating population genetic parameters for
              nonmodel organisms. Here, we modified a recently developed
              time-dependent PRF model to independently estimate genetic
              parameters from a nuclear and mitochondrial DNA data set of 22
              sister pairs of birds that have diverged across a biogeographic
              barrier. We found that species that inhabit humid habitats had more
              recent divergence times and larger effective population sizes than
              those that inhabit drier habitats, and divergence time estimated
              from the PRF model were similar to estimates from a coalescent
              species-tree approach. Selection coefficients were higher in sister
              pairs that inhabited drier habitats than in those in humid habitats
              , but overall the mitochondrial DNA was under weak selection. Our
              study indicates that PRF models are useful for estimating various
              population genetic parameters and serve as a framework for
              incorporating estimates of selection into comparative
              phylogeographic studies. © 2014 by the Genetics Society of America.
              },
  number = {1},
  journal = {Genetics},
  author = {Amei, Amei and Smith, Brian Tilston},
  year = {2014},
  pages = {225--233},
  file = {PDF:/home/jmei/Zotero/storage/45T3R22D/robust estimates of divergence
          times and selection with a poisson random field
          model.pdf:application/pdf},
}

@article{Huerta-Sanchez2008,
  title = {Population genetics of polymorphism and divergence under fluctuating
           selection},
  volume = {178},
  issn = {00166731},
  doi = {10.1534/genetics.107.073361},
  abstract = {Current methods for detecting fluctuating selection require time
              series data on genotype frequencies. Here, we propose an
              alternative approach that makes use of DNA polymorphism data from a
              sample of individuals collected at a single point in time. Our
              method uses classical diffusion approximations to model temporal
              fluctuations in the selection coefficients to find the expected
              distribution of mutation frequencies in the population. Using the
              Poisson random-field setting we derive the site-frequency spectrum
              (SFS) for three different models of fluctuating selection. We find
              that the general effect of fluctuating selection is to produce a
              more "U"-shaped site-frequency spectrum with an excess of
              high-frequency derived mutations at the expense of middle-frequency
              variants. We present likelihood-ratio tests, comparing the
              fluctuating selection models to the neutral model using SFS data,
              and use Monte Carlo simulations to assess their power. We find that
              we have sufficient power to reject a neutral hypothesis using
              samples on the order of a few hundred SNPs and a sample size of ∼20
              and power to distinguish between selection that varies in time and
              constant selection for a sample of size 20. We also find that
              fluctuating selection increases the probability of fixation of
              selected sites even if, on average, there is no difference in
              selection among a pair of alleles segregating at the locus.
              Fluctuating selection will, therefore, lead to an increase in the
              ratio of divergence to polymorphism similar to that observed under
              positive directional selection. Copyright © 2008 by the Genetics
              Society of America.},
  number = {1},
  journal = {Genetics},
  author = {Huerta-Sanchez, Emilia and Durrett, Rick and Bustamante, Carlos D.},
  year = {2008},
  pages = {325--337},
  file = {PDF:/home/jmei/Zotero/storage/AZXGVEDC/population genetics of
          polymorphism and divergence.pdf:application/pdf},
}

@article{Amei2010,
  title = {A time-dependent {Poisson} random field model for polymorphism within
           and between two related biological species},
  volume = {20},
  issn = {10505164},
  doi = {10.1214/09-AAP668},
  abstract = {We derive a Poisson random field model for population site
              polymorphisms differences within and between two species that share
              a relatively recent common ancestor. The model can be either
              equilibrium or time inhomogeneous. We first consider a random field
              of Markov chains that describes the fate of a set of individual
              mutations. This field is approximated by a Poisson random field
              from which we can make inferences about the amounts of mutation and
              selection that have occurred in the history of observed aligned DNA
              sequences. © Institute of Mathematical Statistics, 2010.},
  number = {5},
  journal = {Annals of Applied Probability},
  author = {Amei, Amei and Sawyer, Stanley},
  year = {2010},
  keywords = {Diffusion approximation, DNA sequences, Poisson random field,
              Population genetics},
  pages = {1663--1696},
  file = {PDF:/home/jmei/Zotero/storage/UBBJTD7W/A Time_Dependent Poisson Random
          Field Model for Polymphism Within and Between Two Related Biological
          Species.pdf:application/pdf},
}

@article{Sethupathy2008,
  title = {A {Tutorial} of the {Poisson} {Random} {Field} {Model} in {Population
           } {Genetics}},
  volume = {2008},
  issn = {1687-8027},
  doi = {10.1155/2008/257864},
  abstract = {Population genetics is the study of allele frequency changes
              driven by various evolutionary forces such as mutation, natural
              selection, and random genetic drift. Although natural selection is
              widely recognized as a bona-fide phenomenon, the extent to which it
              drives evolution continues to remain unclear and controversial.
              Various qualitative techniques, or so-called “tests of neutrality”,
              have been introduced to detect signatures of natural selection. A
              decade and a half ago, Stanley Sawyer and Daniel Hartl provided a
              mathematical framework, referred to as the Poisson random field
              (PRF), with which to determine quantitatively the intensity of
              selection on a particular gene or genomic region. The recent
              availability of large-scale genetic polymorphism data has sparked
              widespread interest in genome-wide investigations of natural
              selection. To that end, the original PRF model is of particular
              interest for geneticists and evolutionary genomicists. In this
              article, we will provide a tutorial of the mathematical derivation
              of the original Sawyer and Hartl PRF model.},
  number = {i},
  journal = {Advances in Bioinformatics},
  author = {Sethupathy, Praveen and Hannenhalli, Sridhar},
  year = {2008},
  pages = {1--9},
  file = {
          PDF:/home/jmei/Zotero/storage/32WBBCSW/poisson_field_model.pdf:application/pdf
          },
}

@article{Amei2012,
  title = {Statistical inference of selection and divergence from a
           time-dependent poisson random field model},
  volume = {7},
  issn = {19326203},
  doi = {10.1371/journal.pone.0034413},
  abstract = {We apply a recently developed time-dependent Poisson random field
              model to aligned DNA sequences from two related biological species
              to estimate selection coefficients and divergence time. We use
              Markov chain Monte Carlo methods to estimate species divergence
              time and selection coefficients for each locus. The model assumes
              that the selective effects of non-synonymous mutations are normally
              distributed across genetic loci but constant within loci, and
              synonymous mutations are selectively neutral. In contrast with
              previous models, we do not assume that the individual species are
              at population equilibrium after divergence. Using a data set of 91
              genes in two Drosophila species, D. melanogaster and D. simulans,
              we estimate the species divergence time t div=2.61N e(or 1.68
              million years, assuming the haploid effective population size N
              e=6.45×10 5 years) and a mean selection coefficient per generation
              μ c=1.98/N e. Although the average selection coefficient is
              positive, the magnitude of the selection is quite small. Results
              from numerical simulations are also presented as an accuracy check
              for the time-dependent model. © 2012 Amei, Sawyer.},
  number = {4},
  journal = {PLoS ONE},
  author = {Amei, Amei and Sawyer, Stanley},
  year = {2012},
  file = {
          StatisticalInferenceOfSelectionAndDivergenceFromATimeDependentPoissonRandomFieldModel.PDF:/home/jmei/Zotero/storage/VJFRL692/StatisticalInferenceOfSelectionAndDivergenceFromATimeDependentPoissonRandomFieldModel.PDF:application/pdf
          },
}

@article{Shan2012,
  title = {An efficient and exact approach for detecting trends with binary
           endpoints},
  volume = {31},
  issn = {02776715},
  doi = {10.1002/sim.4411},
  abstract = {Lloyd (Aust. Nz. J. Stat., 50, 329-345, 2008) developed an exact
              testing approach to control for nuisance parameters, which was
              shown to be advantageous in testing for differences between two
              population proportions. We utilized this approach to obtain
              unconditional tests for trends in 2×K contingency tables. We
              compare the unconditional procedure with other unconditional and
              conditional approaches based on the well-known Cochran-Armitage
              test statistic. We give an example to illustrate the approach, and
              provide a comparison between the methods with regards to type I
              error and power. The proposed procedure is preferable because it is
              less conservative and has superior power properties. © 2011 John
              Wiley \& Sons, Ltd.},
  number = {2},
  journal = {Statistics in Medicine},
  author = {Shan, Guogen and Ma, Changxing and Hutson, Alan D. and Wilding,
            Gregory E.},
  year = {2012},
  keywords = {Cochran-Armitage test, Contingency tables, E+M p-value, Exact
              tests, Nuisance parameters, Unconditional test},
  pages = {155--164},
  file = {PDF:/home/jmei/Zotero/storage/24LMYJRF/An efficient and exact approach
          for detecting trends with binary endpoints.pdf:application/pdf},
}

@article{Ma2015,
  title = {Homogeneity test for correlated binary data},
  volume = {10},
  issn = {19326203},
  doi = {10.1371/journal.pone.0124337},
  abstract = {In ophthalmologic studies, measurements obtained from both eyes of
              an individual are often highly correlated. Ignoring the correlation
              could lead to incorrect inferences. An asymptotic method was
              proposed by Tang and others (2008) for testing equality of
              proportions between two groups under Rosner's model. In this
              article, we investigate three testing procedures for general g ≥ 2
              groups. Our simulation results show the score testing procedure
              usually produces satisfactory type I error control and has
              reasonable power. The three test procedures get closer when sample
              size becomes larger. Examples from ophthalmologic studies are used
              to illustrate our proposed methods.},
  number = {4},
  journal = {PLoS ONE},
  author = {Ma, Changxing and Shan, Guogen and Liu, Song},
  year = {2015},
  pages = {1--12},
  file = {Homogeneity Test for Correlated Binary
          Data.PDF:/home/jmei/Zotero/storage/H27FPMXQ/Homogeneity Test for
          Correlated Binary Data.PDF:application/pdf},
}

@article{Shan2014,
  title = {A new powerful nonparametric rank test for ordered alternative
           problem},
  volume = {9},
  issn = {19326203},
  doi = {10.1371/journal.pone.0112924},
  abstract = {We propose a new nonparametric test for ordered alternative
              problem based on the rank difference between two observations from
              different groups. These groups are assumed to be independent from
              each other. The exact mean and variance of the test statistic under
              the null distribution are derived, and its asymptotic distribution
              is proven to be normal. Furthermore, an extensive power comparison
              between the new test and other commonly used tests shows that the
              new test is generally more powerful than others under various
              conditions, including the same type of distribution, and mixed
              distributions. A real example from an anti-hypertensive drug trial
              is provided to illustrate the application of the tests. The new
              test is therefore recommended for use in practice due to easy
              calculation and substantial power gain. Copyright:},
  number = {11},
  journal = {PLoS ONE},
  author = {Shan, Guogen and Young, Daniel and Kang, Le},
  year = {2014},
  file = {A New Powerful Nonparametric Rank Test for Ordered Alternative
          Problem.PDF:/home/jmei/Zotero/storage/CT2AW8U9/A New Powerful
          Nonparametric Rank Test for Ordered Alternative
          Problem.PDF:application/pdf},
}

@article{Rojo1991,
  title = {On {Nonparametric} {Maximum} {Likelihood} {Estimation} of a {
           Distribution} {Uniformly} {Stochastically} {Smaller} than a {Standard}
           },
  volume = {11},
  issn = {01677152},
  doi = {10.1016/0167-7152(91)90154-J},
  abstract = {The class of distributions F which are uniformly stochastically
              smaller than a known standard G arises naturally when life testing
              experiments are conducted in an environment more severe or
              stressful than the environment in which the system has been
              pretested. The problem of estimating a distribution in this class
              via nonparametric maximum likelihood is considered. Under the
              assumption that the standard lifetime distribution G is continuous
              and strictly increasing, the NPMLE of F is derived in closed form,
              and its inconsistency is demonstrated. ?? 1991.},
  number = {3},
  journal = {Statistics and Probability Letters},
  author = {Rojo, Javier and Samaniego, Francisco J.},
  year = {1991},
  keywords = {dispersive ordering, failure rate, inconsistency, nonparametric
              maximum likelihood, Uniform stochastic ordering},
  pages = {267--271},
}

@incollection{Rojo2004,
  address = {Beachwood,Ohio},
  title = {On the {Estimation} of {Survival} {Functions} {Under} a {Stochastic}
           {Order} {Constraint}},
  booktitle = {The {First} {Erich} {L}. {Lehmann} {Symposium} - {Optimality}},
  author = {Rojo, Javier},
  editor = {Rojo, Javier and Perez-Abreu, Victor},
  year = {2004},
  pages = {37--62},
}

@article{ElBarmi2005,
  title = {Inferences {Under} a {Stochastic} {Ordering} {Constraint} : {The} k-{
           Sample} {Case}},
  volume = {100},
  doi = {10.1198/016214504000000764},
  number = {469},
  journal = {Journal of the American Statistical Association},
  author = {El Barmi, Hammou and Mukerjee, Hari},
  year = {2005},
  keywords = {censoring, confidence bands, hypothesis testing, stochastic
              ordering, weak convergence},
  pages = {252--261},
}

@article{Kaplan1958,
  title = {Nonparametric {Estimation} from {Incomplete} {Observations}},
  volume = {53},
  number = {282},
  journal = {Journal of the American Statistical Association},
  author = {Kaplan, E. L. and Meier, Paul},
  year = {1958},
  pages = {457--481},
}

@article{Johansen1978,
  title = {The {Product} {Limit} {Estimator} as {Maximmum} {Likelihood} {
           Estimator}},
  volume = {5},
  number = {4},
  journal = {Scandinavian Journal of Statistics},
  author = {Johansen, Soren},
  year = {1978},
  keywords = {1, 2, chain model, estimation in the markov, introduction and
              summary, kaplan meier estimator, markov chains, maxi-, mum
              likelihood estimator, product limit estimator},
  pages = {195--199},
}

@article{H.D.BrunkW.E.FranckD.L.Hanson1966,
  title = {Maximum {Likelihood} {Estimation} of the {Distributions} of {Two} {
           Stochastically} {Ordered} {Random} {Variables}},
  volume = {61},
  number = {316},
  journal = {Journal of the American Statistical Association},
  author = {D.L. Hanson, R.V. Hogg H.D. Brunk W.E. Franck},
  year = {1966},
  pages = {1067--1080},
}

@article{Lehmann1955,
  title = {Ordered {Families} of {Distributions}},
  volume = {26},
  number = {3},
  journal = {The Annals of Mathematical Statistics},
  author = {Lehmann, E. L.},
  year = {1955},
  pages = {399--419},
}

@article{Feltz1985,
  title = {Maximum {Likelihood} {Estimation} of the {Survival} {Functions} of {N
           } {Stochastically} {Ordereed} {Random} {Variables}},
  volume = {80},
  number = {392},
  journal = {Journal of the American Statistical Association},
  author = {Feltz, Carol J and Dykstra, Richard L},
  year = {1985},
  keywords = {censored data, kap-, kuhn-, lan-meier product limit estimator,
              linear stochastic ordering, order restrictions},
  pages = {1012--1019},
}

@article{Dykstra1982,
  title = {Maximum {Likelihood} {Estimation} of the {Survival} {Functions} of {
           Stochastically} {Ordered} {Random} {Variables} {Maximum} {Likelihood}
           {Estimation} of the {Survival} {Functions} of {Stochastically} {
           Ordered} {Random} {Variables}},
  volume = {77},
  number = {379},
  journal = {Journal of the American Statistical Association},
  author = {Dykstra, Richard L},
  year = {1982},
  keywords = {2, and the solution, department of cardiology of, ered by dr,
              gath-, martin alpert, maximum likelihood estimation, notation,
              survival, the problem, to motivate the problem, we consider some
              data},
  pages = {621--628},
}

@article{Park2012a,
  title = {Pointwise {Nonparametric} {Maximum} {Likelihood} {Estimator} of {
           Stochastically} {Ordered} {Survivor} {Functions}},
  volume = {99},
  doi = {10.1093/biomet/ass006},
  number = {2},
  journal = {Biometrika},
  author = {Park, Yongseok and Taylor, Jeremy M. G. and Kalbfleisch, John D.},
  year = {2012},
  pages = {327--343},
}

@article{Dykstra1991,
  title = {Statistical {Inference} for {Uniform} {Stochastic} {Ordering} in {
           Several} {Populations}},
  volume = {19},
  number = {2},
  journal = {The Annals of Statistics},
  author = {Dykstra, Richard and Kochar, Subhash and Robertson, Tim},
  year = {1991},
  pages = {870--888},
}

@article{Investigators2007,
  title = {The {Final} 10-{Year} {Follow}-{Up} {Results} {From} the {BARI} {
           Randomized} {Trial}},
  volume = {49},
  doi = {10.1016/j.jacc.2006.11.048},
  number = {15},
  journal = {Journal of the American College of Cardiology},
  author = {Investigators, BARI},
  year = {2007},
  pages = {4--10},
}

@article{Boyles1985,
  title = {Inconsistency of the {Maximum} {Likelihood} {Estimator} of a {
           Distribution} having {Increasing} {Failure} {Rate} {Average}},
  volume = {13},
  number = {1},
  journal = {The Annals of Statistics},
  author = {Boyles, R. A. and Marshall, A. W. and Proschan, F.},
  year = {1985},
  pages = {413--417},
}

@article{Praestgaard1996,
  title = {Asymptotic {Theory} for {Nonparametric} {Estimation} of {Survival} {
           Curves} {Under} {Order} {Restrictions}},
  volume = {24},
  number = {4},
  journal = {The Annals of Statistics},
  author = {Praestgaard, Jens Thomas and Huang, Jiang},
  year = {1996},
  pages = {1679--1716},
}

@article{Mukerjee1996,
  title = {Estimation of {Survival} {Functions} {Under} {Uniform} {Stochastic} {
           Ordering}},
  volume = {91},
  number = {436},
  journal = {Journal of the American Statistical Association},
  author = {Mukerjee, Hari},
  year = {1996},
  keywords = {accelerated life testing, nonparametric estimation},
  pages = {1684--1689},
}

@article{Park2012,
  title = {Constrained {Nonparametric} {Maximum} {Likelihood} {Estimation} of {
           Stochastically} {Ordered} {Survivor} {Functions}},
  volume = {40},
  number = {1},
  journal = {The Canadian Journal of Statistics},
  author = {Park, Yongseok and Kalbfleisch, John D and Taylor, Jeremy M G},
  year = {2012},
  pages = {22--39},
}

@article{Rosenberg2005,
  title = {Maternal obesity and diabetes as risk factors for adverse pregnancy
           outcomes: differences among 4 racial/ethnic groups.},
  volume = {95},
  issn = {0090-0036},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/16118366},
  doi = {10.2105/AJPH.2005.065680},
  abstract = {OBJECTIVES We examined associations between obesity, diabetes, and
              3 adverse pregnancy outcomes--primary cesarean delivery, preterm
              birth, and low birth-weight (LBW)--by racial/ethnic group. Our goal
              was to better understand how these associations differentially
              impact birth outcomes by group in order to develop more focused
              interventions. METHODS Data were collected from the 1999, 2000, and
              2001 New York City birth files for 329,988 singleton births
              containing information on prepregnancy weight and prenatal weight
              gain. Separate logistic regressions for 4 racial/ethnic groups
              predicted the adverse pregnancy outcomes associated with diabetes.
              Other variables in the regressions included obesity, excess weight
              gain, hypertension, preeclampsia, and substance use during
              pregnancy (e.g., smoking). RESULTS Chronic and gestational diabetes
              were significant risks for a primary cesarean and for preterm birth
              in all women. Diabetes as a risk for LBW varied by group. For
              example, whereas chronic diabetes increased the risk for LBW among
              Asians, Hispanics, and Whites (adjusted odds ratios=2.28, 1.69, and
              1.59), respectively, it was not a significant predictor of LBW
              among Blacks. CONCLUSIONS In this large, population-based study,
              obesity and diabetes were independently associated with adverse
              pregnancy outcomes, highlighting the need for women to undergo
              lifestyle changes to help them control their weight during the
              childbearing years and beyond.},
  number = {9},
  urldate = {2017-05-14},
  journal = {American journal of public health},
  author = {Rosenberg, Terry J and Garbers, Samantha and Lipkind, Heather and
            Chiasson, Mary Ann},
  month = sep,
  year = {2005},
  pmid = {16118366},
  note = {Publisher: American Public Health Association},
  pages = {1545--51},
  file = {PDF:/home/jmei/Zotero/storage/2ZVX2653/full-text.pdf:application/pdf},
}

@article{Aldous1993,
  title = {Maternal {Age} at {First} {Childbirth} and {Risk} of {Low} {Birth} {
           Weight} and {Preterm} {Delivery} in {Washington} {State}},
  volume = {270},
  issn = {0098-7484},
  url = {
         http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.1993.03510210060028
         },
  doi = {10.1001/jama.1993.03510210060028},
  abstract = {Objective. —To study the consequences of delayed first
              childbearing in a large, population-based US sample, with separate
              analysis of women aged 40 years or more and adjustment for
              socioeconomic factors, smoking, medical and reproductive conditions
              , and route of delivery. Design and Setting. —Retrospective survey
              of Washington State birth certificates from 1984 through 1988.
              Subjects. —First liveborn singleton infants of women aged at least
              20 years. Of eligible white infants, all those born to women aged
              35 to 39 years (n=4019) and 40 years or more (n=410) and a maternal
              age-stratified random sample of white infants of younger women were
              studied. All eligible black infants were studied. Outcome Measures.
              —Low ({\textless}2500 g) and very low ({\textless}1500 g) birth
              weight and preterm delivery ({\textless}37 weeks of gestation).
              Results. —Adjusted odds ratios for delivering a low-birth-weight
              white infant increased progressively with each 5-year maternal age
              group, reaching 2.3 (95\% confidence interval, 1.6 to 3.4) for
              women aged 40 years or more compared with those aged 20 to 24
              years. The maternal age effects for very low birth weight and
              preterm delivery were similar; for each, the odds ratio was 1.8 for
              the oldest group. No significant maternal age effect was found
              among births of black infants, but only 127 births to women aged 35
              years or more were studied. Conclusion. —Increasing maternal age at
              first childbirth is an independent risk factor for low birth weight
              and preterm delivery of white infants in the United States. (JAMA.
              1993;270:2574-2577)},
  number = {21},
  urldate = {2017-05-14},
  journal = {JAMA: The Journal of the American Medical Association},
  author = {Aldous, Michael B. and Edmonson, M. Bruce},
  month = dec,
  year = {1993},
  note = {Publisher: American Medical Association},
  pages = {2574},
  file = {
          PDF:/home/jmei/Zotero/storage/GJF27KUK/low_birth_weight_age.pdf:application/pdf
          },
}

@article{Stevens2002a,
  title = {Low {Birth} {Weight}},
  volume = {287},
  issn = {14770393},
  doi = {10.1177/074823379401000604},
  number = {2},
  journal = {The Journal of the American Medical Association},
  author = {Stevens, Lise M.},
  year = {2002},
  pages = {7424},
  file = {PDF:/home/jmei/Zotero/storage/RS2U6QIT/JPG0109.pdf:application/pdf},
}

@article{Ickovics2003,
  title = {Group prenatal care and preterm birth weight: results from a matched
           cohort study at public clinics},
  volume = {102},
  issn = {00297844},
  url = {http://www.sciencedirect.com/science/article/pii/S0029784403007658},
  doi = {10.1016/S0029-7844(03)00765-8},
  abstract = {OBJECTIVE To examine the impact of group versus individual
              prenatal care on birth weight and gestational age. METHODS This
              prospective, matched cohort study included pregnant women (N = 458)
              entering prenatal care at 24 or less weeks' gestation; one half
              received group prenatal care with women of the same gestational
              age. Women were matched by clinic, age, race, parity, and infant
              birth date. Women were predominantly black and Hispanic of low
              socioeconomic status, served by one of three public clinics in
              Atlanta, Georgia or New Haven, Connecticut. RESULTS Birth weight
              was greater for infants of women in group versus individual
              prenatal care (P {\textless} .01). Among those born preterm,
              infants of group patients were significantly larger than infants of
              individual-care patients (mean, 2398 versus 1990 g, P {\textless}
              .05). Although not statistically significant, infants of group
              patients were less likely than those of individual-care patients to
              be low birth weight (less than 2500 g; 16 versus 23 infants); very
              low birth weight (less than 1500 g; three versus six infants);
              early preterm (less than 33 weeks; two versus seven infants); or to
              experience neonatal loss (none versus three infants). There were no
              differences in number of prenatal visits or other risk
              characteristics (patient age, race, prior preterm birth).
              CONCLUSIONS Group prenatal care results in higher birth weight,
              especially for infants delivered preterm. Group prenatal care
              provides a structural innovation, permitting more time for
              provider–patient interaction and therefore the opportunity to
              address clinical as well as psychological, social, and behavioral
              factors to promote healthy pregnancy. Results have implications for
              design of sustainable prenatal services that might contribute to
              reduction of racial disparities in adverse perinatal outcomes.},
  number = {5},
  urldate = {2017-05-14},
  journal = {Obstetrics \& Gynecology},
  author = {Ickovics, Jeannette R and Kershaw, Trace S and Westdahl, Claire and
            Schindler Rising, Sharon and Klima, Carrie and Reynolds, Heather and
            Magriples, Urania},
  year = {2003},
  pages = {1051--1057},
  file = {
          PDF:/home/jmei/Zotero/storage/YQNEDBBR/group_prenatal_care.pdf:application/pdf
          },
}

@article{Stevens2002,
  title = {Low {Birth} {Weight}},
  volume = {287},
  issn = {14770393},
  doi = {10.1177/074823379401000604},
  number = {2},
  journal = {The Journal of the American Medical Association},
  author = {Stevens, Lise M.},
  year = {2002},
  pages = {7424},
  file = {PDF:/home/jmei/Zotero/storage/4FQS5A7H/JPG0109.pdf:application/pdf},
}

@article{Fang1999,
  title = {Low {Birth} {Weight}: {Race} and {Maternal} {Nativity}— {Impact} of {
           Community} {Income}},
  volume = {103},
  abstract = {Background. The purpose of this article is to determine the effect
              of community income as a co-factor in the association of low birth
              weight, race, and maternal nativity in New York City. Methods. New
              York City birth records, 1988 through 1994, provided data on
              maternal and infant characteris-tics. There were 274 121 white and
              279 826 black mothers included in this study. Black mothers were
              classified as US-born (South and Northeast) and foreign-born (the
              Caribbean, South America, and Africa). Based on the 1990 US census
              income data, census tracts of the city were aggregated by tertile
              of per capita income as low-, middle-, and high-income communities.
              Incidence of low birth weight was estimated by race, maternal
              nativity in the city as a whole, and each income community.
              Results. Overall, black women had a substantially higher risk of
              low birth weight infants ({\textless}2500 g) than did whites (13.1
              \% vs 4.8\%). Foreign-born black mothers had a birth weight
              advantage over US-born black mothers (10.0\% vs 16.7\%). After
              controlling for socioeconomic and medical characteristics, the
              risks of low birth weight for blacks compared with whites were 0.95
              (95\% confidence interval: 0.87–1.03) and 0.86 (0.69 –1.02) for
              Caribbean-and African-born black mothers, respectively. Moreover,
              in low-income communities, compared with white moth-ers, the risks
              for Caribbean-and African-born black mothers were 0.88 (0.79 –
              0.97) and 0.77 (0.61– 0.96), respec-tively. By contrast, US and
              South American-born black mothers had a consistently higher risk of
              low birth weight infants, regardless of community income level.
              Conclusion. Low birth weight was significantly less frequent among
              whites than among blacks. However, this overall finding masked
              substantial variation among blacks, determined by maternal nativity
              and the income level of the community in which they lived. In fact,
              Caribbean-and African-born black mothers had birth outcomes
              generally similar to and, in poor communities, even more favorable
              than those for whites. Pediatrics 1999;103(1). URL:
              http://www.pediatrics.org/cgi/content/ full/103/1/e5; low birth
              weight, nativity, blacks, New York City, community income.
              ABBREVIATIONS. OR, odds ratio; CI, confidence interval. L ow birth
              weight remains a greater public health problem in the United States
              than in most other industrialized nations. 1 It is more common
              among blacks than among whites and is a major determinant of infant
              mortality. 2,3 The role of inheritance and environment in
              determining birth weight remains unresolved, although recent
              evi-dence suggests that genetic influences may not be the
              overriding determinant. 4},
  number = {1},
  urldate = {2017-05-14},
  journal = {Pediatrics},
  author = {Fang, Jing and Madhavan, Shantha and Alderman, Michael H},
  year = {1999},
  file = {
          PDF:/home/jmei/Zotero/storage/FT2JDX65/m-api-44030a61-6a18-f33b-1297-877e4bdd22c6.pdf:application/pdf
          },
}

@article{Kramer1987,
  title = {Determinants of low birth weight: methodological assessment and
           meta-analysis.},
  volume = {65},
  issn = {0042-9686},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/3322602},
  abstract = {43 determinants of low birth weight were analyzed from 895
              published papers in the English and French literature from
              1970-1984. The assessment was limited to singleton births of women
              living at sea level with no chronic illness; rare factors and
              complications of pregnancy were excluded. The 43 factors were
              categorized as genetic and constitutional, demographic and
              psychosocial, obstetric, nutritional, maternal morbidity during
              pregnancy, toxic exposure and antenatal care. The existence and
              magnitude of a causal effect on birth weight, gestational age,
              prematurity and intrauterine growth retardation were determined by
              a set of methodological standards. In developed countries, the most
              important factor was cigarette smoking, followed by nutrition and
              pre-pregnancy weight. In developing countries the major
              determinants were racial origin, nutrition, low pre-pregnancy
              weight, short maternal stature, and malaria. Pre-pregnancy weight,
              prior premature birth or miscarriage, diethylstilbestrol exposure
              and smoking were major determinants of gestational duration, but
              the majority of prematurity was unexplained in both developed and
              developing countries. There is a need for future research on the
              effect of maternal work, prenatal care, and certain vitamin and
              mineral deficiencies on intrauterine growth, and the effect of
              genital tract infection, prenatal care, maternal employment, stress
              and anxiety on prematurity.},
  number = {5},
  urldate = {2017-05-01},
  journal = {Bulletin of the World Health Organization},
  author = {Kramer, M S},
  year = {1987},
  pmid = {3322602},
  note = {Publisher: World Health Organization},
  keywords = {Abortion History, Abortion, Induced, Age Factors, Alcohol Drinking
              , Behavior, Biology, Birth Weight, Body Weight, Demographic Factors
              , Developed Countries, Developing Countries, Diseases, Economic
              Factors, Family Planning, Fertility, Fertility Control,
              Postconception, Fertility Measurements, Fetus, Gestational Age,
              Health, Infant, Infant, Premature--determinants, Infections, Low
              Birth Weight--determinants, Malaria, Marital Status, Maternal Age,
              Maternal Nutrition, Morbidity, Nuptiality, Nutrition, Parasitic
              Diseases, Parental Age, Parity, Pelvic Infections, Physiology,
              Population, Population Characteristics, Population Dynamics,
              Pregnancy, Pregnancy Outcomes, Psychological Factors, Reproduction,
              Sex Behavior, Smoking, Social Behavior, Socioeconomic Factors,
              Socioeconomic Status, Vitamins, World, Youth},
  pages = {663--737},
  file = {PDF:/home/jmei/Zotero/storage/DE9G5MJQ/full-text.pdf:application/pdf},
}

@techreport{Morgan2005,
  title = {The {U}.{S}. {Electric} {Power} {Sector} and {Climate} {Change} {
           Mitigation}},
  urldate = {2019-09-01},
  institution = {Pew Center on Global Climate Change},
  author = {Morgan, Granger and Apt, Jay and Lave, Lester},
  year = {2005},
}

@misc{Serabian2000,
  title = {Cyber {Threats} and the {US} {Economy} — {Central} {Intelligence} {
           Agency}},
  url = {
         https://www.cia.gov/news-information/speeches-testimony/2000/cyberthreats_022300.html
         },
  urldate = {2019-08-31},
  author = {Serabian, John A. Jr.},
  year = {2000},
}

@article{Kosa2012,
  title = {Are {Older} {Adults} {Prepared} to {Ensure} {Food} {Safety} {During}
           {Extended} {Power} {Outages} and {Other} {Emergencies}?: {Findings}
           from a {National} {Survey}},
  volume = {38},
  issn = {03601277},
  doi = {10.1080/03601277.2011.645436},
  abstract = {Natural disasters and other emergencies can cause an increased
              risk of foodborne illness. We conducted a nationally representative
              survey to understand consumers’ knowledge and use of recommended
              practices during/after extended power outages and other
              emergencies. Because older adults are at an increased risk for
              foodborne illness, this paper presents findings from a sample of
              older adults (n = 290). Only 17\% of respondents reported they are
              fully prepared to keep food safe during an extended power outage.
              Respondents identified lack of cogitation (42\%), storage space (19
              \%), and concern (27\%) as barriers to not being fully prepared. Of
              those who had experienced a recent power outage, less than 40\%
              followed the recommended practices of discarding frozen foods that
              had thawed and discarding refrigerated, perishable foods.
              Additionally, 21\% to 36\% of respondents reported they tasted food
              to determine whether it was safe to eat. Awareness and likelihood
              of following recommended practices were higher among women than
              men. Many older adults are not following recommended practices to
              ensure food safety during/after extended power outages and other
              emergencies. Educational materials need to address barriers and be
              tailored to specific locations and subpopulations of older adults.
              Educators and public health officials can use the survey findings
              to address gaps in older adults’ knowledge and practices and to
              help reduce the risk of foodborne illness among older adults.},
  number = {11},
  journal = {Educational Gerontology},
  author = {Kosa, Katherine M. and Cates, Sheryl C. and Karns, Shawn and Godwin,
            Sandria L. and Coppings, Richard J.},
  year = {2012},
  pages = {763--775},
  file = {PDF:/home/jmei/Zotero/storage/HEDDPUI9/Are Older Adults Prepared to
          Ensure Food Safety During Extended Power Outages and Other Emergencies
          Findings from a National Survey.pdf:application/pdf},
}

@article{Kosari2018,
  title = {Power outages and refrigerated medicines: {The} need for better
           guidelines, awareness and planning},
  volume = {43},
  issn = {0269-4727},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jcpt.12716},
  doi = {10.1111/jcpt.12716},
  number = {5},
  urldate = {2019-08-31},
  journal = {Journal of Clinical Pharmacy and Therapeutics},
  author = {Kosari, S. and Walker, E. J and Anderson, C. and Peterson, G. M. and
            Naunton, M. and Castillo Martinez, E. and Garg, S. and Thomas, J.},
  month = oct,
  year = {2018},
  keywords = {Kosari2018},
  pages = {737--739},
}

@misc{Hofmann2017,
  title = {Defending against outages: {Squirrel} tracker {\textbar} {American} {
           Public} {Power} {Association}},
  url = {
         https://www.publicpower.org/blog/defending-against-outages-squirrel-tracker
         },
  urldate = {2019-08-06},
  journal = {American Public Power Association},
  author = {Hofmann, Alex},
  year = {2017},
}

@techreport{Newman2003,
  title = {The {Structure} and {Function} of {Complex} {Networks}},
  urldate = {2019-07-23},
  author = {Newman, M E J},
  year = {2003},
  note = {Publication Title: Source: SIAM Review Volume: 45 Issue: 2},
  pages = {167--256},
}

@techreport{Usaola2009,
  title = {Probabilistic load flow with wind production uncertainty using
           cumulants and {Cornish}-{Fisher} expansion},
  abstract = {This paper proposes a method for probabilistic load flow in
              networks with wind generation, where the uncertainty of the
              production is non Gaussian. The method is based on the properties
              of the cumulants of the probability density functions (PDF) and the
              Cornish Fisher expansion, which is more suitable for non Gaussian
              PDF than other approaches, such as Gram Charlier series. The paper
              includes examples and comparisons between different methods
              proposed in literature.},
  urldate = {2019-07-23},
  author = {Usaola, Julio},
  year = {2009},
  keywords = {Cornish-Fisher expansion series, cumulants, probabilistic load
              flow, wind generation},
  file = {
          PDF:/home/jmei/Zotero/storage/ZXDNBPFS/m-api-046cf014-f391-6cd1-69a0-1dd0fdd66876.pdf:application/pdf
          },
}

@techreport{Karki2001,
  title = {Reliability/{Cost} {Implications} of {PV} and {Wind} {Energy} {
           Utilization} in {Small} {Isolated} {Power} {Systems}},
  abstract = {The application of renewable energy in electric power systems is
              growing rapidly due to enhanced public concerns for adverse
              environmental impacts and escalation in energy costs associated
              with the use of conventional energy sources. Photovoltaics and wind
              energy sources are being increasingly recognized as cost effective
              generation sources in small isolated power systems primarily
              supplied by costly diesel fuel. The utilization of these energy
              sources can significantly reduce the system fuel costs but can also
              have considerable impact on the system reliability. A realistic
              cost/reliability analysis requires evaluation models that can
              recognize the highly erratic nature of these energy sources while
              maintaining the chronology and interdependence of the random
              variables inherent in them. This paper presents a simulation method
              that provides objective indicators to help system planners decide
              on appropriate installation sites, operating policies, and
              selection of energy types, sizes and mixes in capacity expansion
              when utilizing PV and wind energy in small isolated systems.},
  urldate = {2019-07-23},
  author = {Karki, Rajesh and Billinton, Roy},
  year = {2001},
  note = {Publication Title: IEEE TRANSACTIONS ON ENERGY CONVERSION Volume: 16
          Issue: 4},
  file = {
          PDF:/home/jmei/Zotero/storage/QNU8HP5F/m-api-5c420d53-409c-efbd-298f-42c55a648436.pdf:application/pdf
          },
}

@techreport{Billinton,
  title = {Reliability-network-equivalent approach to distribution-system-re1 i
           a b i {I} it y eva {I} u a t i o n},
  abstract = {The paper presents a reliability-network-equivalent approach to
              distribution-system-reliability assessment. In this technique, a
              general feeder is defined and a simple set of equations is
              utilised. The basic general feeder equations and the reliability
              network equivalent provide a practical technique for evaluating the
              reliability of complex radial distribution systems. The procedure
              is illustrated by application to a relatively simple but practical
              system example.},
  urldate = {2019-07-23},
  author = {Billinton, R and Wang, P},
  file = {
          PDF:/home/jmei/Zotero/storage/L62P2UUM/m-api-2f96ab46-d7be-36e7-44a8-1c49ef8473ce.pdf:application/pdf
          },
}

@article{Volkanovski2009,
  title = {Application of the fault tree analysis for assessment of power system
           reliability},
  url = {www.elsevier.com/locate/ress},
  doi = {10.1016/j.ress.2009.01.004},
  abstract = {A new method for power system reliability analysis using the fault
              tree analysis approach is developed. The method is based on fault
              trees generated for each load point of the power system. The fault
              trees are related to disruption of energy delivery from generators
              to the specific load points. Quantitative evaluation of the fault
              trees, which represents a standpoint for assessment of reliability
              of power delivery, enables identification of the most important
              elements in the power system. The algorithm of the computer code,
              which facilitates the application of the method, has been applied
              to the IEEE test system. The power system reliability was assessed
              and the main contributors to power system reliability have been
              identified, both qualitatively and quantitatively. Crown},
  urldate = {2019-07-20},
  author = {Volkanovski, Andrija and Epin, Marko C ˇ and Mavko, Borut},
  year = {2009},
  keywords = {Fault tree, Power system, Reliability, Safety},
  file = {PDF:/home/jmei/Zotero/storage/VAQVRIZE/full-text.pdf:application/pdf},
}

@techreport{Allan1981,
  title = {Discrete {Convolution} in {Power} {System} {Reliability}},
  abstract = {linearly with the number of discrete points to be convolved UMIST,
              Manchester instead of quadratically as in the case of conventional
              A.M. Leite da Silva, Member IEEE methods. Catholic University, Rio
              de Janeiro The paper first describes the FFT method and how it A.A.
              Abu-Nasser can be applied in practise. This is followed by applying
              the UMIST, Manchester technique to one particular case of
              generation capacity R.C. Burchett, Member IEEE reliability
              evaluation. General Electric Company, Schenectady Notation Key
              Words-Discrete convolution, Sum of discrete s-independent Ai random
              event representing unit i on outage random variables,
              Capacity-outage probability tables, Power systems. A random event
              representing all units on outage Reader Aids-Ci generating capacity
              of unit i Purpose: Report a technique, Tutorial unavailability
              (forced outage rate) of unit Special math needed for explanations:
              Probability theory, Fourier series xi r.v. associated with event Ai
              Special math needed to use results: Same X r.v. associated with
              event A Results useful to: Reliability engineers, Data analysts
              Ximin, X,max minimum and maximum generating power of unit i,
              respectively Abstract-This paper describes a general method for
              convolving. . discrete distributions using Fast Fourier Transforms.
              It can be used in d(-) impulse function (area of one, negligible
              width) evaluating reliability of any system involving discrete or
              discretised con-INT(-) integer value of (.) volution. It has been
              used in power system studies to deduce capacity-* convolution
              symbol outage probability tables and to solve probabilistic load
              flows. These COPT capacity outage probability table studies have
              shown it to be much less time-consuming and more efficient FFT Fast
              Fourier Transform than the conventional direct methods. The method
              is used in the paper to. . evaluate the loss of load probability of
              a generating system in order to LOLP loss of load probablity
              demonstrate the method's application and inherent merits. 2.
              EXAMPLE OF CONVOLUTION Consider a small system consisting of only
              two units 1. INTRODUCTION with capacities C1 and C2. The
              probability of having Cl out is q1 = 1-Pi, and C2 out is q2 = 1-P2.
              The COPT for Many applications of probability theory require two or
              this system can be constructed from these probabilities as more
              distributions to be convolved. These distributions shown in table
              1. can be either continuous or discrete, and both types appear in
              reliability evaluation of systems. In the power system field, two
              particular applications of convolution are 1) the TABLE 1
              evaluation of capacity-outage probability tables [1] COPT for a
              2-unit system representing the generation model of the system, and
              2) the solution to the probabilistic load flow problem [2, 3]. The
              capacity on outage probability first of these applications involves
              only discrete functions 0 Pl P2 and the second involves both. C,
              qtp2 The techniques directly apply to discrete distributions C2 pi
              q2 but can also be applied to continuous distributions if the C, +
              C2 q, q2 latter are first discretised into a sequence of discrete
              impulses. There are a number of ways in which this convolution can
              be performed [3] but most are based on direct The process used to
              deduce the COPT in table 1 is known manipulation of the convolution
              intergral. This paper as convolution and can be described in
              statistical terms us-shows how the convolution process can be
              evaluated using ing the following concepts. From figure 1 the r.v.
              Xis the Fast Fourier Transforms (FFT) and convolving the sum of X,
              and X2, i.e.-distributions after transforming them into the
              frquency domain. This technique has been used in both of the X = Xl
              + X2. (1) previously mentioned power systems applications. These
              studies have shown it to be consistently superior to con-Since Xl
              and X2 are s-independent r.v.'s, the pdff(x) is the ventional
              methods with execution time increasing almost convolution of the
              pdf's f1(x) and f2(x), i.e.-0018-9529/81/1200-0452\$00.75©()1981
              IEEE},
  urldate = {2019-07-20},
  author = {Allan, RN and Leite da Silva, AM and Abu-Nasser, AA and Burchett, RC
            },
  year = {1981},
  note = {Publication Title: IEEE TRANSACTIONS ON RELIABILITY Volume: 30 Issue:
          5},
  file = {
          PDF:/home/jmei/Zotero/storage/ADYI4X4M/m-api-3098f6fd-3ff9-7365-0c96-fc3493fc2882.pdf:application/pdf
          },
}

@techreport{Giorsetto1983,
  title = {Development of a {New} {Procedure} for {Reliability} {Modeling} of {
           Wind} {Turbine} {Generators}},
  abstract = {A method for determining the impact of wind generation on system
              reliability is developed. This method combines the effects of wind
              turbine forced outage rates and varying power output due to wind
              speed variations. Since individual wind turbines on a windfarm all
              have an output related to wind speed, each turbine's output cannot
              be assumed to be independent random variables. Because of this
              situation, special steps must be taken in order to arrive at the
              cumulative distribution function for the windfarm. This
              distribution function may be combined with the system distribution
              function using simple convolution and so the windfarm can be easily
              incorporated into a Loss of Load Expectation computer program. The
              method can then be used to determine the effective load carrying
              capability of a windfarm and also to supply better estimates of the
              benefits of wind generation to the' overall system.},
  urldate = {2019-07-20},
  author = {Giorsetto, Paul and Utsurogi, Kent F.},
  year = {1983},
  note = {Publication Title: IEEE Transactions on Power Apparatus and Systems
          Volume: 102 Issue: 1},
  file = {
          PDF:/home/jmei/Zotero/storage/EX4H7J5J/m-api-5f8f5c9e-f2df-00bd-3508-3fd9cddc262a.pdf:application/pdf
          },
}

@article{Putrus2009,
  title = {Impact of electric vehicles on power distribution networks},
  url = {http://dx.doi.org/10.1109/VPPC.2009.5289760},
  doi = {10.1109/VPPC.2009.5289760},
  abstract = {The market for battery powered and plug-in hybrid electric
              vehicles is currently limited, but this is expected to grow rapidly
              with the increased concern about the environment and advances in
              technology. Due to their high energy capacity, mass deployment of
              electrical vehicles will have significant impact on power networks.
              This impact will dictate the design of the electric vehicle
              interface devices and the way future power networks will be
              designed and controlled. This paper presents the results of an
              analysis of the impact of electric vehicles on existing power
              distribution networks. Evaluation of supply/demand matching and
              potential violations of statutory voltage limits, power quality and
              imbalance are presented.},
  urldate = {2019-07-20},
  author = {Putrus, G. A. and Suwanapingkarl, P. and Johnston, D. and Bentley,
            E. C. and Narayana, M.},
  year = {2009},
  keywords = {component, Distributed Generation, Distribution Networks, Electic
              Vehicles, Smart Grids},
  pages = {7--11},
  file = {PDF:/home/jmei/Zotero/storage/5NZYVMJW/full-text.pdf:application/pdf},
}

@techreport{Billintona,
  title = {Monte {Carlo} simulation model for multiarea generation system
           reliability studies},
  abstract = {In a Monte Carlo simulation model and procedure for reliability
              assessment of multiarea generation systems a chronological
              simulation scheme is described which is capable of recog-nising
              different unit failure and repair distributions , different unit
              types, load forecast uncertainty and tie-line directional transfer
              capabilities and capacity distributions. Distributions of
              reliability indices can be obtained by taking advantage of a wide
              range of output data provided by the chronological Monte Carlo
              method. The proposed method is illustrated with simulation results
              based on two multiarea configurations created by connecting several
              IEEE Reliability Test Systems together. The results show that the
              proposed model can be an effective tool in multi-area generating
              system adequacy evaluation.},
  urldate = {2019-07-20},
  author = {Billinton, R and Gan, L},
  keywords = {Chronological simulation, Indexing t m : Adequacy evolution,
              Matrix labelling algorithm},
  file = {
          PDF:/home/jmei/Zotero/storage/SJU6ZAPB/m-api-2dc740c3-60f2-aed1-4878-f8ebe0f99842.pdf:application/pdf
          },
}

@article{Fan2012a,
  title = {Probabilistic {Power} {Flow} {Studies} for {Transmission} {Systems} {
           With} {Photovoltaic} {Generation} {Using} {Cumulants} {Electromagnetic
           } {Transient} and {Electromechanical} {Transient} {Stability} {Hybrid}
           {Simulation} {View} project {Probabilistic} {Power} {Flow} {Studies}
           for {Transmission} {Sys}},
  url = {https://www.researchgate.net/publication/260509422},
  doi = {10.1109/TPWRS.2012.2190533},
  abstract = {This paper applies a probabilistic power flow (PPF) algorithm to
              evaluate the influence of photovoltaic (PV) generation uncertainty
              on transmission system performance. PV generation has the potential
              to cause a significant impact on power system reliability in the
              near future. A cumulant-based PPF algorithm suitable for large
              systems is used to avoid convolution calculations. Correlation
              among input random variables is considered. Specifically
              correlation between adjacent PV resources are considered. Three
              types of approximation expansions based on cumulants, namely the
              Gram-Charlier expansion, the Edgeworth expansion, and the
              Cornish-Fisher expansion, are compared, and their properties ,
              advantages, and deficiencies are discussed. Additionally, a novel
              probabilistic model of PV generation is developed to obtain the
              probability density function (PDF) of the PV generation production
              based on the environmental conditions. The proposed approaches with
              the three expansions are compared with Monte Carlo simulations
              (MCS) with results for a 2497-bus representation of the Arizona
              area of the Western Electricity Coordinating Council (WECC) system.
              },
  urldate = {2019-07-14},
  author = {Fan, Miao and Vittal, V and Member, Student and Vittal, Vijay and
            Thomas Heydt, Gerald and Fellow, Life and Ayyanar, Raja and Member,
            Senior},
  year = {2012},
  keywords = {correlation, cumulant, Edgeworth expansion, Gram-Charlier
              expansion, Index Terms-Cornish-Fisher expansion, photovoltaic
              generation, probabilistic power flow study, renewable resources,
              stochastic power flow study},
  file = {PDF:/home/jmei/Zotero/storage/444J6NE2/full-text.pdf:application/pdf},
}

@article{Fan2012,
  title = {Probabilistic {Power} {Flow} {Studies} for {Transmission} {Systems} {
           With} {Photovoltaic} {Generation} {Using} {Cumulants} {Electromagnetic
           } {Transient} and {Electromechanical} {Transient} {Stability} {Hybrid}
           {Simulation} {View} project {Probabilistic} {Power} {Flow} {Studies}
           for {Transmission} {Sys}},
  url = {https://www.researchgate.net/publication/260509422},
  doi = {10.1109/TPWRS.2012.2190533},
  abstract = {This paper applies a probabilistic power flow (PPF) algorithm to
              evaluate the influence of photovoltaic (PV) generation uncertainty
              on transmission system performance. PV generation has the potential
              to cause a significant impact on power system reliability in the
              near future. A cumulant-based PPF algorithm suitable for large
              systems is used to avoid convolution calculations. Correlation
              among input random variables is considered. Specifically
              correlation between adjacent PV resources are considered. Three
              types of approximation expansions based on cumulants, namely the
              Gram-Charlier expansion, the Edgeworth expansion, and the
              Cornish-Fisher expansion, are compared, and their properties ,
              advantages, and deficiencies are discussed. Additionally, a novel
              probabilistic model of PV generation is developed to obtain the
              probability density function (PDF) of the PV generation production
              based on the environmental conditions. The proposed approaches with
              the three expansions are compared with Monte Carlo simulations
              (MCS) with results for a 2497-bus representation of the Arizona
              area of the Western Electricity Coordinating Council (WECC) system.
              },
  urldate = {2019-07-14},
  author = {Fan, Miao and Vittal, V and Member, Student and Vittal, Vijay and
            Thomas Heydt, Gerald and Fellow, Life and Ayyanar, Raja and Member,
            Senior},
  year = {2012},
  keywords = {correlation, cumulant, Edgeworth expansion, Gram-Charlier
              expansion, Index Terms-Cornish-Fisher expansion, photovoltaic
              generation, probabilistic power flow study, renewable resources,
              stochastic power flow study},
  file = {PDF:/home/jmei/Zotero/storage/3MENXDEX/full-text.pdf:application/pdf},
}

@article{Morales2010,
  title = {Probabilistic power flow with correlated wind sources},
  volume = {4},
  issn = {17518687},
  doi = {10.1049/iet-gtd.2009.0639},
  abstract = {A probabilistic power flow model that takes into account spatially
              correlated power sources and loads is proposed. It is particularly
              appropriate to assess the impact of intermittent generators such as
              wind power ones on a power network. The proposed model is solved
              using an extended point estimate method that accounts for
              dependencies among the input random variables (i.e. loads and power
              sources). The proposed probabilistic power flow model is
              illustrated through 24-bus and 118-bus case studies. Finally,
              conclusions are duly drawn.},
  number = {5},
  journal = {IET Generation, Transmission \& Distribution},
  author = {Morales, J.M. and Baringo, L. and Conejo, A.J. and Mínguez, R.},
  year = {2010},
  pages = {641},
  file = {
          PDF:/home/jmei/Zotero/storage/FM3HUY9X/Probabilistic_power_flow_with_.pdf:application/pdf
          },
}

@article{Usaola2010,
  title = {Probabilistic load flow with correlated wind power injections},
  volume = {80},
  doi = {10.1016/j.epsr.2009.10.023},
  abstract = {The non-dispatchable nature of wind generation implies that system
              operation depends on wind power prediction programs that forecast
              wind farms production with high levels of uncertainty. This means
              that probabilistic power analysis tools become more and more
              necessary in systems with high wind penetration. Probabilistic load
              flow becomes especially difficult when wind generation is
              considered. The high uncertainty of the production, the
              non-Gaussian probability density function (PDF) and the clear
              dependence among the wind farms poses a challenge for conventional
              tools. The paper proposes an approximation that makes use of the
              properties of statistical moments and Cornish-Fisher expansion to
              tackle these new problems.},
  urldate = {2019-07-14},
  journal = {Electric Power Systems Research},
  author = {Usaola, Julio},
  year = {2010},
  keywords = {Probabilistic load flow, Wind energy},
  pages = {528--536},
  file = {
          PDF:/home/jmei/Zotero/storage/UQC9TIFV/m-api-21475448-7431-bddb-1010-c7b819d51e52.pdf:application/pdf
          },
}

@techreport{Brucoli1985,
  title = {Quadratic probabilistic load flow with linearly modelled dispatch},
  abstract = {A second-order probabilistic load flow technique is presented ,
              which takes into account the effects of nonlinearities in the
              system equations and of different dispatching strategies. This
              approach uses as input a normal probability distribution for the
              loads. The probability distribution for the generated powers is
              computed by linearly modelling the dispatching activity. The
              technique of moments is then applied to second-order approximations
              for the load flow and current equations, allowing the noniterative
              computation of the means and standard deviations for the various
              output quantities. Generation outages are separately treated, with
              the possibility of embedding different postoutage dispatching
              strategies. The cumulative results are finally obtained by
              combining the pertinent probabilistic load flows with the
              occurrence probabilities of the various conditions. Results on two
              sample systems are given.},
  urldate = {2019-07-14},
  author = {Brucoli, M and Torelli, F},
  year = {1985},
  keywords = {load flow, network analysis, security assessment},
  file = {
          PDF:/home/jmei/Zotero/storage/RJ9AX3UX/m-api-36bac9d3-834b-ee27-31d5-d64d015514f2.pdf:application/pdf
          },
}

@techreport{SobierajsklWroclaw1978,
  title = {A {Method} of {Stochastic} {Load} {Flow} {Calculation}},
  urldate = {2019-07-14},
  author = {Sobierajskl Wroclaw, M},
  year = {1978},
  pages = {37--40},
  file = {
          PDF:/home/jmei/Zotero/storage/YTF5G9R6/m-api-1ec892cb-50be-55b5-5a7b-1c93e104fc56.pdf:application/pdf
          },
}

@techreport{Sasson,
  title = {Stochastic {Load} {Flows}},
  abstract = {Abstraci: The load flow study has been at the center of studies
              made for designing and operating power systems for many years. I t
              i s well known that forecasted data used in load flow studies
              contain errors that affect the solution, as can be evidenced by
              running many cases perturbing the input data. This paper presents a
              method for calculating the effect of the propagation of data
              inaccuracies through the load flow calculations, thus obtaining a
              range of values for each output quantity that, to a high degree of
              probability, encloses the operating conditions of the system. The
              method is efficient and can be added to any existing load flow
              program. Results of cases run on the AEP system are included.},
  urldate = {2019-07-14},
  institution = {American Electric Power Service Corp.},
  author = {Sasson, N Y A M},
  file = {
          PDF:/home/jmei/Zotero/storage/BBWNBZI7/m-api-55388b46-3d57-0339-625f-616e16447dc6.pdf:application/pdf
          },
}

@techreport{Rosas-Casalsa,
  title = {Topological vulnerability of the {European} power grid under errors
           and attacks},
  url = {http://www.ucte.org},
  abstract = {We present an analysis of the topological structure and static
              tolerance to errors and attacks of the September 2003 actualization
              of the Union for the Coordination of Transport of Electricity
              (UCTE) power grid, involving thirty-three different networks.
              Though every power grid studied has exponential degree distribution
              and most of them lack typical small-world topology, they display
              patterns of reaction to node loss similar to those observed in
              scale-free networks. We have found that the node removal behaviour
              can be logarithmically related to the power grid size. This
              logarithmic behaviour would suggest that, though size favours
              fragility, growth can reduce it. We conclude that, with the
              ever-growing demand for power and reliability, actual planning
              strategies to increase transmission systems would have to take into
              account this relative increase in vulnerability with size, in order
              facilitate and improve the power grid design and functioning. 2},
  urldate = {2019-07-01},
  author = {Rosas-Casals, Martí and Valverde, Sergi and Solé, Ricard V},
  file = {PDF:/home/jmei/Zotero/storage/YGJN26SY/full-text.pdf:application/pdf},
}

@techreport{LeitedaSilva1984,
  title = {Probabilistic {Load} {Flow} {Considering} {Dependence} {Between} {
           Input} {Nodal} {Powers}},
  abstract = {been considered by most previous PLF-algorithms Henry T.
              Darlington, Stagg Systems Inc. The problem: In the 1970-76 time
              period, many medium-to large-size computer-based electric utility
              dispatch systems were planned and placed in service. Many users of
              these older systems are recognizing the need to upgrade in some
              manner. The question is how and with what? As with any major
              capital purchase, executive management must be convinced that the
              need is real and the cost can be justified. Developing A Solution:
              A seven-step approach is described. 1) Define the existing
              situations and problems. 2) Define required functions and
              attributes for the upgraded system.near and long term. 3) Define at
              least three possible system solutions. 4) Evaluate each potential
              system solution from a technical and operations management
              viewpoint. 5) Perform cost and present worth analysis. 6) Prepare
              an implementation plan. 7) Prepare a budget for the recommended
              system. Goals and Justification David Hayward, REMVEC Mr.
              Darlington proposes a seven-step approach. What fol¬ lows is
              additional detail that should aid in accomplishing the first five
              steps. Goals: Before an upgrade is seriously studied it is wise to
              prepare a set of goals that you expect to achieve. It is essential
              that you be realistic. In preparing goals it is a good idea to
              consider the upgraded system as a stepping stone toward your next
              completely new system. Justification: Normally, several points must
              be combined to provide sufficient justification. A list of
              justification points includes: economic dispatch, unit commitment,
              interchange negotiation, maintenance, reli¬ ability, security
              analysis, man/machine, billing functions, and building
              arrangements. Many of these points can easily be translated into
              dollars and a cost-versus-savings analysis can be done.
              Probabilistic load flow (PLF) is a valuable technique that can
              assess adequacy indices such as the probability of a line flow
              being greater than its thermal rating; probability of a busbar
              voltage being outside its operational constraints, etc. These are
              extremely useful parameters in planning and operation of power
              systems. These indices are achieved because of the technique
              recognizes the probabilistic nature of load, genera¬ tion and
              network configuration within one solution. In order to solve this
              complex problem, the following assumptions have 1) probability of
              network parameters is equal to one 2) linear load flow equations 3)
              total dependence between nodal power injections 4) utility's
              operating policy. Assumption 1) is the most commonly used in
              published PLF-techniques. However, each possible network
              configuration of a power system has an associated probability. This
              is a subject that requires more consideration and future effort
              should concentrate on this aspect of the analysis. The use of
              linearized power flow equations about an ex¬ pected operating point
              can be considered, in terms of accu¬ racy, to be a reasonable
              assumption, for a wide range of input uncertainties. As these
              uncertainties become very large, a more elaborate algorithm or even
              a Monte Carlo technique using the exact load flow equations, should
              be used. This assumption has also been extensively used in
              reliability stud¬ ies. Assumption 2) together with 3) means the
              PLF-solution becomes a sum of independent random variables weighted
              by sensitivity coefficients. Consequently, the solution is ob¬
              tained by a convolution process which can be efficiently carried
              out using fast Fourier transform algorithms. However, there are
              various reasons for correlation to exist between nodal powers.
              These reasons depend on whether load/load, generation/load or
              generation/generation behaviour is being considered. For example, a
              group of loads existing in the same area will tend to increase and
              decrease in a like manner due to environmental or social factors.
              Therefore, there will be a certain degree of dependence between
              them. Correlations involving generation exist because of the need
              to balance the active system power (assumption 4)). In most
              PLF-algorithms, the swing bus must be defined and conse¬ quently
              this unique bus is responsible for balancing the total active
              power. However, this is not true in practise because the balance is
              made using several generation units according to the utility's
              operating policy. Therefore surplus or deficiency of power can not
              be occur, but unfortunately this can happen with conventional
              PLF-algorithms. Even if the swing bus is responsible for all the
              power system balance, this bus can not generate more than its
              available capacity or less than zero. Such violations would give
              fictitious flows in lines near the swing bus. Consequently the
              range of power generation in¬ creases together with the variance of
              the power injection at the swing bus and the neighbouring flows. In
              order to over¬ come this problem, some PLF-algorithms have used a
              linear correlation model. This however is an approximation of the
              actual model. This paper presents a simple and practical
              probabilistic load flow method which considers the dependence
              between the input nodal power uncertainties. This dependence is
              modeled in order to take into account the criteria or policies used
              to balance the active system power and also the linear correla¬
              tion between nodal demands.},
  urldate = {2019-07-14},
  author = {Leite da Silva, A M and Arienti, V L and Allan, R N},
  year = {1984},
  file = {
          PDF:/home/jmei/Zotero/storage/5FE7ECL6/m-api-79bad926-b3f0-d2e9-08cd-748b8e3127c6.pdf:application/pdf
          },
}

@article{Beard2010,
  title = {Key {Technical} {Challenges} for the {Electric} {Power} {Industry}
           and {Climate} {Change}},
  volume = {25},
  doi = {10.1109/TEC.2009.2032578},
  abstract = {This paper, prepared by the Climate Change Technology Subcommittee
              , a subcommittee of the Power and Energy Society Energy Development
              and Power Generation Committee, identifies key technical issues
              facing the electric power industry, related to global climate
              change. The technical challenges arise from: 1) impacts on system
              operating strategies, configuration, and expansion plans of
              emission-reducing technologies; 2) power infrastructure response to
              extreme weather events; 3) effects of government policies including
              an expanded use of renewable and alternative energy technologies;
              and 4) impacts of market rules on power system operation. Possible
              lessons from other industries' responses to climate change are
              explored.},
  number = {2},
  urldate = {2019-07-09},
  journal = {IEEE TRANSACTIONS ON ENERGY CONVERSION},
  author = {Beard, Lisa M and Cardell, Judith B and Dobson, Ian and Galvan,
            Floyd and Hawkins, David and Member, Life and Jewell, Ward and
            Kezunovic, Mladen and Overbye, Thomas J and Sen, Pankaj K and Member,
            Senior and Tylavsky, Daniel J and McConnach, James and Paper no, Ieee
            and Hawkins, D and Jewell, W and Tylavsky, D J},
  year = {2010},
  keywords = {electric power research, extreme weather, global climate change,
              government policy, Index Terms-Alternative technologies},
  pages = {465},
  file = {
          PDF:/home/jmei/Zotero/storage/TSR8NNQR/m-api-2eaeaf0e-730d-e817-0815-aa56b734e4f2.pdf:application/pdf
          },
}

@techreport{Genz1992,
  title = {Interface {Foundation} of {America} {Numerical} {Computation} of {
           Multivariate} {Normal} {Probabilities}},
  urldate = {2019-03-30},
  author = {Genz, Alan},
  year = {1992},
  note = {Publication Title: Source: Journal of Computational and Graphical
          Statistics Volume: 1 Issue: 2},
  pages = {141--149},
  file = {
          PDF:/home/jmei/Zotero/storage/RRD4Q6AV/m-api-293788c5-f5a5-02af-30ea-c636b178176e.pdf:application/pdf
          },
}

@book{IEEEComputationalIntelligenceSociety,
  title = {2017 {SSCI} proceedings},
  isbn = {978-1-5386-2726-6},
  abstract = {Copyright dates differ on cover page and copyright page. "IEEE
              Catalog Number: CFP16COI-USB"--PDF copyright page.},
  author = {{IEEE Computational Intelligence Society} and {Institute of
            Electrical and Electronics Engineers}},
  file = {PDF:/home/jmei/Zotero/storage/5IXFI7KT/full-text.pdf:application/pdf},
}

@techreport{Billinton1998,
  title = {Distribution {System} {Reliability} {Cost}/{Worth} {Analysis} {Using}
           {Analytical} and {Sequential} {Simulation} {Techniques}},
  abstract = {The requirements for extensive justification of new facilities and
              emphasis on the optimization of system cost and reliability are
              steadily increasing. Customer interruption cost analysis provides
              valuable input to electric power supply reliability worth
              assessment. This paper presents both a generalized analytical
              approach and a time sequential Monte Carlo simulation technique for
              evaluating the customer interruption cost in complex radial
              distribution systems. Studies are conducted on two practical
              distribution systems. The results obtained using the analytical
              technique incorporating average restoration times are compared with
              those obtained using the time sequential simulation method and
              random restoration times. The effects on customer interruption cost
              indices associated with alternate supply and protection devices are
              also considered.},
  urldate = {2019-03-25},
  author = {Billinton, Roy and Wang, Peng},
  year = {1998},
  note = {Publication Title: IEEE Transactions on Power Systems Volume: 13
          Issue: 4},
  file = {
          PDF:/home/jmei/Zotero/storage/PW24CJ65/m-api-169f3dfa-dcd2-4303-603b-1b5addc30a9a.pdf:application/pdf
          },
}

@article{Tsao2003,
  title = {Composite {Reliability} {Evaluation} {Model} for {Different} {Types}
           of {Distribution} {Systems}},
  volume = {18},
  doi = {10.1109/TPWRS.2003.811174},
  abstract = {The paper develops a set of composite distribution system
              reliability evaluation models that can be applied to a nonradial
              type system. The developed models reflect the effect of
              distribution substations, primary distribution systems, and the
              interaction between them. First, reliability assessments for five
              configurations of distribution substation are performed using
              minimal cut-set technique. Next, the interaction between
              distribution substation and primary distribution system is modeled
              according to the tripping behavior of feeder circuit breakers. Last
              , the contribution of reliability indices evaluated from the
              primary distribution system is added to the previous two effects ,
              and then the composite load point reliability evaluation models are
              developed. The developed models are demonstrated on five types of
              distribution system modified from a reliability test system.
              Comparisons of reliability indices are made and discussed.},
  number = {2},
  urldate = {2019-03-24},
  journal = {IEEE TRANSACTIONS ON POWER SYSTEMS},
  author = {Tsao, Teng-Fa and Chang, Hong-Chan},
  year = {2003},
  keywords = {distribu-tion substation, distribution system, Index
              Terms-Composite reliability evaluation model, minimal cut-set,
              nonradial, re-liability indices},
  file = {
          PDF:/home/jmei/Zotero/storage/KW69IW7A/m-api-09056067-96dc-c865-2d08-db6e86406132.pdf:application/pdf
          },
}

@techreport{Allan,
  title = {Probabilistic methods applied to electric power systems-are they
           worth it?},
  abstract = {This article reflects on and questions existing approaches to
              maintaining and improving the reliability of power systems and
              reviews and highlights some alternative ways forward. It also
              questions the need and usefulness of present deterministic criteria
              and security standards and instead suggests that integrated cost
              benefit analyses involving customers and their decisions could be a
              more rewarding and applicable approach. The guiding principle
              throughout this article is the basic question: 'Is it worth it?'.
              The primary technical function of a power system is to provide
              electrical energy to its customers as economically as possible with
              an acceptable degree of continuity and qudity, known as reliability
              The most important component is the customer and it is of paramount
              importance t o fulfil and satisfy his/her expectations The t w o
              constraints of economics and reliability, however, are competitive
              since increased reliability of supply generally requires increased
              capital investment The perennial question for system engineers is
              'How can these two constraints be balanced?' This has been achieved
              in many different ways in different countries and by different
              utilities, although generally they are all based on various sets of
              deterministic criteria In the UK for instance, security standards
              have been adopted by the electricity supply industry which attempts
              to ensure that system designs respond t o perceived customer
              demands and needs This article reflects on and questions existing
              approaches, and reviews and highlights some alternative ways
              forward Effects of outages The electricity supply industry operates
              a very complex and highly integrated system Failures in any part of
              it can cause interruptions which range from inconveniencing a small
              number of local residents, to a major and widespread catastrophic
              disruption of supply The economic impact of these outages is not
              POWER ENGINEERING JOURNAL MAY 1992 necessarily restricted to loss
              of revenue by the utility or loss of energy utilisation by the
              customer but in order to estimate true costs, should also include
              indirect costs imposed on society and the environment due to the
              outage For instance, in the case of the 1977 New Year blackout, the
              total costs of the blackouts were attributed as ' 0 Consolidated
              Edison direct costs 3 5\% 0 other direct costs 12 5\% 0 indirect
              costs 84 0\% In order to reduce the frequency and duration of these
              events and to ameliorate their effect, it is necessary to invest
              either in the design phase, the operating phase or both A whole
              series of questions emanate from this concept, including 0 0 0 How
              much should be spent? Is it worth spending any money? Should the
              reliability be increased, maintained a t existing levels or allowed
              to degrade? Who should decide-the utility, a regulator, the
              customer' On what basis should the decision be made? 0 0 The
              underlying trend in all these questions is again 'Is it worth it?'
              Deterministic versus probabilistic criteria Most of the present
              planning, design and operational criteria are based on
              deterministic techniques, such as percentage 121},
  urldate = {2019-03-24},
  author = {Allan, R N and Billinton, R},
  file = {
          PDF:/home/jmei/Zotero/storage/ZNNM4LJ5/m-api-36fdb20f-650d-8cf9-0bab-d21cea603e0a.pdf:application/pdf
          },
}

@article{Hernandez-Fajardo2012,
  title = {Probabilistic study of cascading failures in complex interdependent
           lifeline systems},
  url = {http://dx.doi.org/10.1016/j.ress.2012.10.012},
  doi = {10.1016/j.ress.2012.10.012},
  abstract = {The internal complexity of lifeline systems and their standing
              interdependencies can operate in conjunction to amplify the
              negative effects of external disruptions. This paper introduces a
              simulation-based methodology to evaluate the joint impact of
              interdependence, component fragilities, and cascading failures in
              systemic fragility estimates. The proposed strategy uses a graph
              model of interdependent networks, an enhanced betweenness
              centrality for cascading failures approximation, and an
              interdependence model accounting for coupling uncertainty in the
              simulation of damage propagation for probabilistic performance
              assessment. This methodology is illustrated through its application
              to a realistic set of power and water networks subjected to
              earthquake scenarios and random failures. Test case results reveal
              two key insights: (1) the intensity of a perturbation influences
              interdependent systemic fragility by shaping the magnitudes of
              initial component damage and, sometimes counter-intuitively, the
              subsequent interdependence effects and (2) increasing local
              redundancy mitigates the effects of interdependence on systemic
              performance, but such intervention is incapable of eliminating
              interdependent effects completely. The previous insights provide
              basic guidelines for the design of systemic retrofitting policies.
              Additionally, the limitations of local capacity redundancy as a
              fragility control measure highlight the need for a critical
              assessment of intervention strategies in distributed infrastructure
              networks. Future work will assess the fragility-reduction
              efficiency of strategies involving informed manipulation of
              individual systemic topologies and the interdependence interfaces
              connecting them.},
  urldate = {2019-03-24},
  author = {Hernandez-Fajardo, Isaac and Dueñ As-Osorio, Leonardo},
  year = {2012},
  keywords = {Cascading failures, Fragility propagation, Interdependence,
              Lifeline Systems, Local redundancy, Random failures, Seismic hazard
              },
  file = {PDF:/home/jmei/Zotero/storage/YL8L4XX4/full-text.pdf:application/pdf},
}

@techreport{Taft2015,
  title = {Grid {Architecture}},
  urldate = {2019-03-23},
  author = {Taft, JD and Becker-Dippmann, A},
  year = {2015},
  file = {
          PDF:/home/jmei/Zotero/storage/J3DM6RV7/m-api-4cca5d2d-4fa0-7efb-1e3b-bc82fc834c6a.pdf:application/pdf
          },
}

@techreport{Kenward2014,
  title = {{BLACKOUT}: {EXTREME} {WEATHER}, {CLIMATE} {CHANGE} {AND} {POWER} {
           OUTAGES}},
  url = {www.climatecentral.org},
  urldate = {2019-03-23},
  author = {Kenward, Alyson and Raja, Urooj},
  year = {2014},
  keywords = {bridge, city, cloudy, darkness, east river, energy, hurricane,
              light, new york, night, nightshot, power, river, skyscrapers, storm
              , williamsburg},
  file = {PDF:/home/jmei/Zotero/storage/S5BCVFJ3/full-text.pdf:application/pdf},
}

@techreport{Allan1991,
  title = {A {RELIABILITY} {TEST} {SYSTEM} {FOR} {EDUCATIONAL} {PURPOSES}-{BASIC
           } {DISTRIBUTION} {SYSTEM} {DATA} {AND} {RESULTS}},
  abstract = {This paper describes an electrical distribution system for use in
              teaching power system reliability evaluation. It includes all the
              main elements found in practical systems. However, it is
              sufficiently small that students can analyse it using hand
              calculations and hence fully understand reliability models and
              evaluation techniques. The paper contains all the data needed to
              perform basic reliability analyses. It also contains the basic
              results for a range of case studies and alternative
              design/operating configurations.},
  urldate = {2019-02-13},
  author = {Allan, R N and Billinton, R and Sjarief, I and Goel, L and So, K S},
  year = {1991},
  keywords = {customer indices INTaoDucTION, distribution systems, educational
              studies, Keywoods: reliability test system},
  file = {
          PDF:/home/jmei/Zotero/storage/FVZCIVY8/m-api-247a04ed-8b36-d48d-7901-91fa53c8c8ca.pdf:application/pdf
          },
}

@techreport{Billinton1997,
  title = {Power system health analysis},
  abstract = {This paper presents a technique which combines both probabilistic
              indices and deterministic criteria to reflect the well-being of a
              power system. This technique permits power system planners,
              engineers and operators to maximize the probability of healthy
              operation as well as minimizing the probability of risky operation.
              The concept of system well-being is illustrated in this paper by
              application to the areas of operating reserve assessment and
              composite power system security evaluation.},
  urldate = {2019-02-12},
  author = {Billinton, Roy and Fotuhi-Firuzabad, Mahmud and Aboreshaid, Saleh},
  year = {1997},
  note = {Publication Title: Reliability Engineering and System Safety Volume:
          55},
  pages = {1--8},
  file = {
          PDF:/home/jmei/Zotero/storage/XT6K2XGW/m-api-48df34b7-8760-a391-76d3-2efcbbc17cea.pdf:application/pdf
          },
}

@techreport{Olson1991,
  title = {Approximation of certain multivariate integrals},
  abstract = {A Taylor series approximation to multivariate integrals taken with
              respect to a multivariate probability distribution is proposed and
              applied to the computation of multivariate normal probabilities and
              conditional expectations. The approximation does not require that
              the multivariate distribution have a structured covariance matrix
              and, in its simplest form, can be written as the product of
              univariate integrals. The approximation is compared to that of
              Mendell and Elston (1974) for computing bivariate normal
              probabilities.},
  urldate = {2018-10-16},
  author = {Olson, Jane M and Weissfeld, Lisa A},
  year = {1991},
  note = {Publication Title: Statistics \& Probability Letters Volume: 11 ISBN:
          01677152/91},
  keywords = {Keywordr, Multivariate normal probabilities, Taylor series},
  pages = {309--317},
  file = {
          PDF:/home/jmei/Zotero/storage/QQY6Q7WF/m-api-7cf24545-8334-8753-0d39-febf93e115ae.pdf:application/pdf
          },
}

@article{Aien2016,
  title = {A comprehensive review on uncertainty modeling techniques in power
           system studies},
  url = {http://dx.doi.org/10.1016/j.rser.2015.12.070},
  doi = {10.1016/j.rser.2015.12.070},
  abstract = {As a direct consequence of power systems restructuring on one hand
              and unprecedented renewable energy utilization on the other, the
              uncertainties of power systems are getting more and more attention.
              This fact intensifies the difficulty of decision making in the
              power system context; therefore, the uncertainty analysis of the
              system performance seems necessary. Generally, uncertainties in any
              engineering system study can be represented probabilistically or
              possibilistically. When sufficient historical data of the system
              variables is not available, a probability density function (PDF)
              might not be defined, they must be represented in another manner
              i.e. using possibilistic theory. When some of the system uncertain
              variables are probabilistic and some are possibilistic, neither the
              conventional pure prob-abilistic nor pure possibilistic methods can
              be implemented. Hence, a combined solution is needed. This paper
              gives a complete review on uncertainty modeling approaches for
              power system studies making sense about the strengths and weakness
              of these methods. This work may be used in order to select the most
              appropriate method for each application.},
  urldate = {2018-10-10},
  author = {Aien, Morteza and Hajebrahimi, Ali and Fotuhi-Firuzabad, Mahmud},
  year = {2016},
  keywords = {Decision making, Joint possibilistic–probabilistic uncertainty
              mode, Possibilistic uncertainty modeling, Probabilistic uncertainty
              modeling, Uncertain power system studies},
  file = {PDF:/home/jmei/Zotero/storage/AAB2GN8H/full-text.pdf:application/pdf},
}

@article{Panteli2015,
  title = {Influence of extreme weather and climate change on the resilience of
           power systems: {Impacts} and possible mitigation strategies},
  volume = {127},
  url = {http://dx.doi.org/10.1016/j.epsr.2015.06.012},
  doi = {10.1016/j.epsr.2015.06.012},
  abstract = {A key driver for developing more sustainable energy systems is to
              decrease the effects of climate change, which could include an
              increase in the frequency, intensity and duration of severe weather
              events. Amongst others, extreme weather has a significant impact on
              critical infrastructures, and is considered one of the main causes
              of wide-area electrical disturbances worldwide. In fact,
              weather-related power interruptions often tend to be of high impact
              and sustained duration, ranging from hours to days, because of the
              large damage on transmission and distribution facilities. Hence,
              enhancing the grid resilience to such events is becoming of
              increasing interest. In this outlook, this paper first discusses
              the influence of weather and climate change on the reliability and
              operation of power system components. Since modelling the impact of
              weather is a difficult task because of its stochastic and
              unpredicted nature, a review of existing methodologies is provided
              in order to get an understanding of the key modelling approaches,
              challenges and requirements for assessing the effect of extreme
              weather on the frequency and duration of power system blackouts.
              Then, the emerging concept of resilience is discussed in the
              context of power systems as critical infrastructure, including
              several defense plans for boosting the resilience of power systems
              to extreme weather events. A comprehensive modelling research
              framework is finally outlined, which can help understand and model
              the impact of extreme weather on power systems and how this can be
              prevented or mitigated in the future.},
  urldate = {2018-09-30},
  journal = {Electric Power Systems Research},
  author = {Panteli, Mathaios and Mancarella, Pierluigi},
  year = {2015},
  keywords = {Reliability, Climate change, Power systems blackouts, Resilience,
              Resiliency, Weather},
  pages = {259--270},
  file = {PDF:/home/jmei/Zotero/storage/FT5R8Q64/full-text.pdf:application/pdf},
}

@article{Balijepalli2004,
  title = {Modeling and {Analysis} of {Distribution} {Reliability} {Indices}},
  volume = {19},
  issn = {0885-8977},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1339369},
  doi = {10.1109/TPWRD.2004.829144},
  abstract = {Assessment of customer power supply reliability is an important
              part of distribution system operation and planning. Monte Carlo
              simulations can be used to find the statistical distribution of the
              reliability indices, along with their mean and standard deviation.
              The standard deviation of the reliability indices provides
              distribution engineers with information on the expected range of
              the annual values. However, the Monte Carlo simulation usually is a
              time-consuming computation. In this paper, an efficient Monte Carlo
              simulation method for distribution system reliability assessment is
              presented. Analysis of outage data from a practical distribution
              system is performed to determine the failure and repair models
              appropriate for use in the Monte Carlo simulation. The sensitivity
              of the reliability indices to the choice of model is presented.
              Finally, the impact of protection devices on the statistical
              distribution of System Average Interruption Frequency Index (SAIFI)
              for a practical distribution feeder is presented. \&copy; 2004
              IEEE.},
  number = {4},
  journal = {IEEE Transactions on Power Delivery},
  author = {Balijepalli, Nagaraj and Venkata, S S and Christie, R D},
  year = {2004},
  keywords = {customer power supply reliability, Data analysis, distribution
              reliability indices, distribution system operation, distribution
              system planning, Distribution systems, Failure analysis, Monte
              Carlo methods, Monte Carlo simulation, Monte Carlo simulations,
              Performance analysis, power distribution planning, power
              distribution reliability, Power engineering and energy, Power
              supplies, Power system modeling, Power system planning, Power
              system reliability, protection devices, reliability assessment,
              Reliability engineering, reliability indices, statistical analysis,
              statistical distribution, Statistical distributions, system average
              interruption frequency index},
  pages = {1950--1955},
  file = {
          PDF:/home/jmei/Zotero/storage/U6W55MZD/reliability_indices.pdf:application/pdf
          },
}

@article{Williams2013a,
  title = {Probabilistic load flow modeling comparing maximum entropy and
           gram-charlier probability density function reconstructions},
  volume = {28},
  issn = {08858950},
  doi = {10.1109/TPWRS.2012.2205714},
  abstract = {Probabilistic load flow (PLF) modeling is gaining renewed
              popularity as power grid complexity increases due to growth in
              intermittent renewable energy generation and unpredictable
              probabilistic loads such as plug-in hybrid electric vehicles
              (PEVs). In PLF analysis of grid design, operation and optimization,
              mathematically correct and accurate predictions of probability tail
              regions are required. In this paper, probability theory is used to
              solve electrical grid power load flow. The method applies two
              Maximum Entropy (ME) methods and a Gram-Charlier (GC) expansion to
              generate voltage magnitude, voltage angle and power flow
              probability density functions (PDFs) based on cumulant arithmetic
              treatment of linearized power flow equations. Systematic ME and GC
              parameter tuning effects on solution accuracy and performance is
              reported relative to converged deterministic Monte Carlo (MC)
              results. Comparing ME and GC results versus MC techniques
              demonstrates that ME methods are superior to the GC methods used in
              historical literature, and tens of thousands of MC iterations are
              required to reconstitute statistically accurate PDF tail regions.
              Direct probabilistic solution methods with ME PDF reconstructions
              are therefore proposed as mathematically correct, statistically
              accurate and computationally efficient methods that could be
              applied in the load flow analysis of large-scale networks.},
  number = {1},
  journal = {IEEE Transactions on Power Systems},
  author = {Williams, Trevor and Crawford, Curran},
  year = {2013},
  keywords = {probabilistic load flow, Gram-Charlier, MATPOWER, Maximum Entropy
              (MaxEnt), Monte Carlo, probability density function},
  pages = {272--280},
  file = {
          PDF:/home/jmei/Zotero/storage/9NDAAXYX/PLF_gram_charlier.pdf:application/pdf
          },
}

@article{Zhang2004,
  title = {Probabilistic {Load} {Flow} {Computation} {Using} the {Method} of {
           Combined} {Cumulants} and {Gram}-{Charlier} {Expansion}},
  volume = {19},
  issn = {08858950},
  doi = {10.1109/TPWRS.2003.818743},
  abstract = {Open access transmission has created a deregulated power market
              and brought new challenges to system planning. This paper proposes
              a new method to compute a probabilistic load flow in extensive
              power systems for the purpose of using it as a quick screening tool
              to determine the major investment on improving transmission system
              inadequacy. This innovative method combines the concept of
              Cumulants and Gram-Charlier expansion theory to obtain
              probabilistic distribution functions of transmission line flows. It
              has significantly reduced the computational time while maintaining
              a high degree of accuracy. This enables probabilistic analysis of
              power flow problems to be treated objectively and allows
              quantitative assessment of system reliability. ABSTRACT FROM
              AUTHOR]; Copyright of IEEE Transactions on Power Systems is the
              property of IEEE and its content may not be copied or emailed to
              multiple sites or posted to a listserv without the copyright
              holder's express written permission. However, users may print,
              download, or email articles for individual use. This abstract may
              be abridged. No warranty is given about the accuracy of the copy.
              Users should refer to the original published version of the
              material for the full abstract. (Copyright applies to all
              Abstracts.)},
  number = {1},
  journal = {IEEE Transactions on Power Systems},
  author = {Zhang, Pei and Lee, Stephen T.},
  year = {2004},
  note = {ISBN: 08858950},
  keywords = {Power system, Gram-Charlier expansion, Probabilistic load flow,
              Cumulants, Open access transmission and deregulated power mar,
              Probability distribution function, Transmission planning},
  pages = {676--682},
  file = {
          PDF:/home/jmei/Zotero/storage/RABX24UL/PLF_cumulants.pdf:application/pdf
          },
}

@techreport{Rosas-Casals,
  title = {Topological vulnerability of the {European} power grid under errors
           and attacks},
  url = {http://www.ucte.org},
  abstract = {We present an analysis of the topological structure and static
              tolerance to errors and attacks of the September 2003 actualization
              of the Union for the Coordination of Transport of Electricity
              (UCTE) power grid, involving thirty-three different networks.
              Though every power grid studied has exponential degree distribution
              and most of them lack typical small-world topology, they display
              patterns of reaction to node loss similar to those observed in
              scale-free networks. We have found that the node removal behaviour
              can be logarithmically related to the power grid size. This
              logarithmic behaviour would suggest that, though size favours
              fragility, growth can reduce it. We conclude that, with the
              ever-growing demand for power and reliability, actual planning
              strategies to increase transmission systems would have to take into
              account this relative increase in vulnerability with size, in order
              facilitate and improve the power grid design and functioning. 2},
  urldate = {2018-08-16},
  author = {Rosas-Casals, Martí and Valverde, Sergi and Solé, Ricard V},
  file = {PDF:/home/jmei/Zotero/storage/2735MHKF/full-text.pdf:application/pdf},
}

@article{Fairley2004,
  title = {The unruly power grid},
  volume = {41},
  issn = {0018-9235},
  url = {http://ieeexplore.ieee.org/document/1318179/},
  doi = {10.1109/MSPEC.2004.1318179},
  number = {8},
  urldate = {2018-08-14},
  journal = {IEEE Spectrum},
  author = {Fairley, P.},
  month = aug,
  year = {2004},
  pages = {22--27},
}

@article{Chassin2005,
  title = {Evaluating {North} {American} electric grid reliability using the {
           Baraba}´{siBaraba}´si-{Albert} network model},
  volume = {355},
  url = {www.elsevier.com/locate/physa},
  doi = {10.1016/j.physa.2005.02.051},
  abstract = {The reliability of electric transmission systems is examined using
              a scale-free model of network topology and failure propagation. The
              topologies of the North American eastern and western electric grids
              are analyzed to estimate their reliability based on the
              Baraba´siBaraba´si-Albert network model. A commonly used power
              system reliability index is computed using a simple failure
              propagation model. The results are compared to the values of power
              system reliability indices previously obtained using standard power
              engineering methods, and they suggest that scale-free network
              models are usable to estimate aggregate electric grid reliability.},
  urldate = {2018-08-14},
  journal = {Physica A},
  author = {Chassin, David P and Posse, Christian},
  year = {2005},
  keywords = {Power system reliability, Network robustness, Scale-free networks},
  pages = {667--677},
  file = {PDF:/home/jmei/Zotero/storage/3ZMVNKDQ/full-text.pdf:application/pdf},
}

@article{Haimes2001,
  title = {Leontief-{Based} {Model} of {Risk} in {Complex} {Interconnected} {
           Infrastructures}},
  volume = {7},
  issn = {1076-0342},
  url = {
         http://ascelibrary.org/doi/10.1061/%28ASCE%291076-0342%282001%297%3A1%281%29
         },
  doi = {10.1061/(ASCE)1076-0342(2001)7:1(1)},
  abstract = {Wassily Leontief received the 1973 Nobel Price in Economics for
              developing what came to be known as the Leontief input-output model
              of the economy. Leontief's model enables understanding the
              interconnectedness among the various sectors of an economy and
              forecasting the effect on one segment of a change in another. A
              Leontief-based infrastructure input-output model is developed here
              to enable an accounting of the intraconnectedness within each
              critical infrastructure as well as the interconnectedness among
              them. The linear input/output model is then generalized into a
              generic risk model with the former as the first-order
              approximation. A preliminary study of the dynamics of risk of
              inoperability is discussed, using a Leontief-based dynamic model.
              Several examples are presented to illustrate the theory and its
              applications.},
  number = {1},
  urldate = {2018-07-27},
  journal = {Journal of Infrastructure Systems},
  author = {Haimes, Y. and Jiang, Pu},
  month = mar,
  year = {2001},
  note = {ISBN: 1076-0342},
  pages = {1--12},
  file = {
          PDF:/home/jmei/Zotero/storage/8KA5665B/m-api-33b483a0-d7d6-7b39-3928-f97701dfdfea.pdf:application/pdf
          },
}

@article{Rau1982,
  title = {Reliability of {Interconnected} {Power} {Systems} {With} {Correlated}
           {Demands}},
  volume = {PAS-101},
  issn = {00189510},
  doi = {10.1109/TPAS.1982.317514},
  abstract = {The rationale for system interconnections due to the associated
              reliability improvement and reserve benefits is well understood. In
              calculations to obtain the loss of load probability (LOLP) of
              interconnected systems, the correlation between system loads is
              generally ignored. This paper proposes a method for the calculation
              of the LOLP of two interconnected systems incorporating the
              correlation between demands. This has been done by using the
              bivariate Gram-Charlier expansion. The LOLPs for different tie-line
              capacities are evaluated by using the hourly loads of the two
              interconnected systems. The reported work is an extension of the
              work of Baleriaux and Booth and the notion of equivalent load of
              two areas with correlated loads. The proposed method is
              computationally efficient since convolutions are performed by using
              the appropriate statistical moments and cumulants. The results of a
              study of two interconnected systems using the proposed method has
              been outlined. The accuracy of the bivariate Gram-Charlier's
              expansion for this class of problems has also been discussed.
              Although the method has been applied to two interconnected systems,
              there is no conceptual difficulty in extending it to three or more
              areas by using a multivariate Gram-Charlier expansion.},
  number = {9},
  journal = {IEEE Transactions on Power Apparatus and Systems},
  author = {Rau, N. S. and Necsulescu, C. and Schenk, K. F. and Misra, R. B.},
  year = {1982},
  pages = {3421--3430},
  file = {
          PDF:/home/jmei/Zotero/storage/MXQZVTNB/correlated_demands.pdf:application/pdf
          },
}

@article{Wang2010,
  title = {Generating statistically correct random topologies for testing smart
           grid communication and control networks},
  volume = {1},
  issn = {19493053},
  doi = {10.1109/TSG.2010.2044814},
  abstract = {In order to design an efficient communication scheme and examine
              the efficiency of any networked control architecture in smart grid
              applications, we need to characterize statistically its information
              source, namely the power grid itself. Investigating the statistical
              properties of power grids has the immediate benefit of providing a
              natural simulation platform, producing a large number of power grid
              test cases with realistic topologies, with scalable network size,
              and with realistic electrical parameter settings. The second
              benefit is that one can start analyzing the performance of
              decentralized control algorithms over information networks whose
              topology matches that of the underlying power network and use
              network scientific approaches to determine analytically if these
              architectures would scale well. With these motivations, in this
              paper we study both the topological and elec- trical
              characteristics of power grid networks based on a number of
              synthetic and real-world power systems. The most interesting
              discoveries include: the power grid is sparsely connected with
              obvious small-world properties; its nodal degree distribution can
              be well fitted by a mixture distribution coming from the sum of a
              truncated geometric random variable and an irregular discrete
              random variable; the power grid has very distinctive graph spectral
              density and its algebraic connectivity scales as a power function
              of the network size; the line impedance has a heavy-tailed
              distribution, which can be captured quite accurately by a clipped
              double Pareto lognormal distribution. Based on the discoveries
              mentioned above, we propose an algorithm that generates random
              topology power grids featuring the same topology and electrical
              characteristics found from the real data. Index Terms—Graph models
              for networks, p},
  number = {1},
  journal = {IEEE Transactions on Smart Grid},
  author = {Wang, Zhifang and Scaglione, Anna and Thomas, Robert J.},
  year = {2010},
  note = {ISBN: 1949-3053 VO - 1},
  keywords = {Graph models for networks, Power grid topology},
  pages = {28--39},
  file = {
          PDF:/home/jmei/Zotero/storage/7LRBDQST/RandomTopologies.pdf:application/pdf
          },
}

@article{Wang2009,
  title = {Cascade-based attack vulnerability on the {US} power grid},
  volume = {47},
  issn = {0925-7535},
  url = {https://www.sciencedirect.com/science/article/pii/S0925753509000174},
  doi = {10.1016/J.SSCI.2009.02.002},
  abstract = {The vulnerability of real-life networks subject to intentional
              attacks has been one of the outstanding challenges in the study of
              the network safety. Applying the real data of the US power grid, we
              compare the effects of two different attacks for the network
              robustness against cascading failures, i.e., removal by either the
              descending or ascending orders of the loads. Adopting the initial
              load of a node j to be Lj=[kj(Σm∈Γjkm)]α with kj and Γj being the
              degree of the node j and the set of its neighboring nodes,
              respectively, where α is a tunable parameter and governs the
              strength of the initial load of a node, we investigate the response
              of the US power grid under two attacks during the cascading
              propagation. In the case of α{\textless}0.7, our investigation by
              the numerical simulations leads to a counterintuitive finding on
              the US power grid that the attack on the nodes with the lowest
              loads is more harmful than the attack on the ones with the highest
              loads. In addition, the almost same effect of two attacks in the
              case of α=0.7 may be useful in furthering studies on the control
              and defense of cascading failures in the US power grid.},
  number = {10},
  urldate = {2018-07-26},
  journal = {Safety Science},
  author = {Wang, Jian-Wei and Rong, Li-Li},
  month = dec,
  year = {2009},
  note = {Publisher: Elsevier},
  pages = {1332--1336},
  file = {PDF:/home/jmei/Zotero/storage/NTYT4WZU/full-text.pdf:application/pdf},
}

@article{Kinney2005,
  title = {Modeling cascading failures in the {North} {American} power grid},
  volume = {46},
  issn = {1434-6028},
  url = {http://www.springerlink.com/index/10.1140/epjb/e2005-00237-9},
  doi = {10.1140/epjb/e2005-00237-9},
  number = {1},
  urldate = {2018-07-26},
  journal = {The European Physical Journal B},
  author = {Kinney, R. and Crucitti, P. and Albert, R. and Latora, V.},
  month = jul,
  year = {2005},
  note = {Publisher: EDP Sciences},
  pages = {101--107},
  file = {PDF:/home/jmei/Zotero/storage/J69Y99SL/full-text.pdf:application/pdf},
}

@article{Evans2004,
  title = {Algorithms for computing the distributions of sums of discrete random
           variables},
  volume = {40},
  issn = {0895-7177},
  url = {https://www.sciencedirect.com/science/article/pii/S089571770500004X},
  doi = {10.1016/J.MCM.2005.01.003},
  abstract = {We present algorithms for computing the probablity density
              function of the sum of two independent discrete random variables,
              along with an implementation of the algorithm in a computer algebra
              system. Some examples illustrate the utility of this algorithm.},
  number = {13},
  urldate = {2018-07-26},
  journal = {Mathematical and Computer Modelling},
  author = {Evans, D.L. and Leemis, L.M.},
  month = dec,
  year = {2004},
  note = {Publisher: Pergamon},
  pages = {1429--1452},
  file = {PDF:/home/jmei/Zotero/storage/32RS275W/full-text.pdf:application/pdf},
}

@article{Mercier2007,
  title = {Discrete random bounds for general random variables and applications
           to reliability},
  volume = {177},
  issn = {03772217},
  doi = {10.1016/j.ejor.2005.09.043},
  abstract = {We here propose some new algorithms to compute bounds for (1)
              cumulative density functions of sums of i.i.d. nonnegative random
              variables, (2) renewal functions and (3) cumulative density
              functions of geometric sums of i.i.d. nonnegative random variables.
              The idea is very basic and consists in bounding any general
              nonnegative random variable X by two discrete random variables with
              range in hN, which both converge to X as h goes to 0. Numerical
              experiments are lead on and the results given by the different
              algorithms are compared to theoretical results in case of i.i.d.
              exponentially distributed random variables and to other numerical
              methods in other cases.},
  number = {1},
  urldate = {2018-07-26},
  journal = {European Journal of Operational Research},
  author = {Mercier, Sophie},
  year = {2007},
  pages = {378--405},
  file = {PDF:/home/jmei/Zotero/storage/BXYL8WJN/full-text.pdf:application/pdf},
}

@article{Langner2011,
  title = {Stuxnet: {Dissecting} a cyberwarfare weapon},
  volume = {9},
  issn = {15407993},
  doi = {10.1109/MSP.2011.67},
  abstract = {Ralph Langner, an expert in industrial control system security,
              explores the technical side of Stuxnet, dangerous malware that
              attacks SCADA systems.},
  number = {3},
  journal = {IEEE Security and Privacy},
  author = {Langner, Ralph},
  year = {2011},
  note = {ISBN: 1540-7993},
  keywords = {cyberwarfare, digital code signing, Ralph Langner, SCADA, Stuxnet},
  pages = {49--51},
  file = {PDF:/home/jmei/Zotero/storage/QFZYFX7K/stuxnet.pdf:application/pdf},
}

@article{Nakashima2012,
  title = {U.{S}., {Israel} developed {Flame} computer virus to slow {Iranian}
           nuclear efforts, officials say},
  journal = {The Washington Post},
  author = {Nakashima, Ellen and Miller, Greg and Tate, Julie},
  year = {2012},
  file = {PDF:/home/jmei/Zotero/storage/XDNU4ZQM/full-text.pdf:application/pdf},
}

@inproceedings{Sforna2006,
  title = {Overview of the events and causes of the 2003 {Italian} blackout},
  isbn = {1-4244-0177-1},
  url = {http://ieeexplore.ieee.org/document/4075762/},
  doi = {10.1109/PSCE.2006.296323},
  urldate = {2018-07-26},
  booktitle = {2006 {IEEE} {PES} {Power} {Systems} {Conference} and {Exposition}
               },
  publisher = {IEEE},
  author = {Sforna, M. and Delfanti, M.},
  year = {2006},
  pages = {301--308},
  file = {
          PDF:/home/jmei/Zotero/storage/QZUT4FJN/italian_blackout.pdf:application/pdf
          },
}

@article{Barron2003,
  title = {{THE} {BLACKOUT}: {OVERVIEW}; {LIGHTS} {GO} {ON} {AFTER} {BIGGEST} {
           BLACKOUT}, {BUT} {NOT} {WITHOUT} {2ND} {DAY} {OF} {SUFFERING} - {The}
           {New} {York} {Times}},
  url = {
         https://www.nytimes.com/2003/08/16/nyregion/blackout-overview-lights-go-after-biggest-blackout-but-not-without-2nd-day.html
         },
  urldate = {2018-07-24},
  journal = {The New York Times},
  author = {Barron, James},
  month = aug,
  year = {2003},
}

@article{Kennedy2003,
  title = {{THE} {BLACKOUT} {OF} 2003: {TRANSPORTATION}; {Thousands} {Stranded}
           on {Foot} by {Crippled} {Trains}, {Crawling} {Buses} and {Traffic} {
           Gridlock} - {The} {New} {York} {Times}},
  url = {
         https://www.nytimes.com/2003/08/15/us/blackout-2003-transportation-thousands-stranded-foot-crippled-trains-crawling.html
         },
  urldate = {2018-07-24},
  journal = {The New York Times},
  author = {Kennedy, Randy},
  year = {2003},
}

@article{Marufu2004,
  title = {The 2003 {North} {American} electrical blackout: {An} accidental
           experiment in atmospheric chemistry},
  volume = {31},
  issn = {00948276},
  url = {http://doi.wiley.com/10.1029/2004GL019771},
  doi = {10.1029/2004GL019771},
  number = {13},
  urldate = {2018-07-24},
  journal = {Geophysical Research Letters},
  author = {Marufu, Lackson T. and Taubman, Brett F. and Bloomer, Bryan and
            Piety, Charles A. and Doddridge, Bruce G. and Stehr, Jeffrey W. and
            Dickerson, Russell R.},
  month = jul,
  year = {2004},
  note = {Publisher: Wiley-Blackwell},
  pages = {n/a--n/a},
  file = {PDF:/home/jmei/Zotero/storage/W3AZBRYZ/full-text.pdf:application/pdf},
}

@article{Beatty2006,
  title = {Blackout of 2003: public health effects and emergency response.},
  volume = {121},
  issn = {0033-3549},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/16416696},
  doi = {10.1177/003335490612100109},
  abstract = {We examined the public health effects of the Northeast blackout of
              August 2003 and the emergency response to the blackout by the New
              York City Department of Health and Mental Hygiene (DOHMH). We
              reviewed departmental documents from the DOHMH Emergency Operations
              Center and surveyed DOHMH employees to identify deficiencies in the
              response and elicit suggestions for improvement. DOHMH deployed its
              all-hazards, scalable public health Incident Management System to
              respond to several impacts: (1) failure of multiple hospital
              emergency generators; (2) patients dependent on electrically
              powered equipment; (3) loss of electronic data input to the DOHMH
              syndromic surveillance system from hospital emergency departments;
              (4) potential for vaccine spoilage due to loss of refrigeration;
              (5) beach contamination with untreated sewage; (6) heat-related
              health effects and increase of foodborne disease; and (7) potential
              for an increased rodent population as a result of increased amounts
              of discarded perishables. Areas identified for improvement included
              communications during the event, DOHMH dependence on an external
              source of electricity, facility management during the response, and
              lack of readily available and appropriate emergency supplies.},
  number = {1},
  urldate = {2018-07-23},
  journal = {Public health reports (Washington, D.C. : 1974)},
  author = {Beatty, Mark E and Phelps, Scot and Rohner, Mpha Chris and Weisfuse,
            Mupa Isaac},
  year = {2006},
  pmid = {16416696},
  note = {Publisher: SAGE Publications},
  pages = {36--44},
  file = {PDF:/home/jmei/Zotero/storage/DGDZT7KT/full-text.pdf:application/pdf},
}

@article{Anderson2012,
  title = {Lights out: impact of the {August} 2003 power outage on mortality in
           {New} {York}, {NY}.},
  volume = {23},
  issn = {1531-5487},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/22252408},
  doi = {10.1097/EDE.0b013e318245c61c},
  abstract = {BACKGROUND Little is known about how power outages affect health.
              We investigated mortality effects of the largest US blackout to
              date, 14-15 August 2003 in New York, NY. METHODS We estimated
              mortality risk in New York, NY, using a generalized linear model
              with data from 1987-2005. We incorporated possible confounders,
              including weather and long-term and seasonal mortality trends.
              RESULTS During the blackout, mortality increased for accidental
              deaths (122\% [95\% confidence interval = 28\%-287\%]) and
              nonaccidental (ie, disease-related) deaths (25\% [12\%-41\%]),
              resulting in approximately 90 excess deaths. Increased mortality
              was not from deaths being advanced by a few days; rather, mortality
              risk remained slightly elevated through August 2003. CONCLUSIONS To
              our knowledge, this is the first analysis of power outages and
              nonaccidental mortality. Understanding the impact of power outages
              on human health is relevant, given that increased energy demand and
              climate change are likely to put added strain on power grids.},
  number = {2},
  urldate = {2018-07-23},
  journal = {Epidemiology (Cambridge, Mass.)},
  author = {Anderson, G Brooke and Bell, Michelle L},
  month = mar,
  year = {2012},
  pmid = {22252408},
  note = {Publisher: NIH Public Access},
  pages = {189--93},
  file = {PDF:/home/jmei/Zotero/storage/GHL5EZ2E/full-text.pdf:application/pdf},
}

@article{Tierney2016,
  title = {Biometrika {Trust} {Regions} of {Positive} and {Unimodal} {Series} {
           Expansion} of the {Edgeworth} and {Gram}-{Charlier} {Approximations} {
           Author} ( s ): {Norman} {R} . {Draper} and {David} {E} . {Tierney} {
           Published} by : {Oxford} {University} {Press} on behalf of {Biometrika
           } {Trust} {Stable} {URL} : ht},
  volume = {59},
  number = {2},
  author = {Tierney, David E},
  year = {2016},
  pages = {463--465},
  file = {PDF:/home/jmei/Zotero/storage/VEZH2DZG/2334591.pdf:application/pdf},
}

@misc{Grens,
  title = {Spike in deaths blamed on 2003 {New} {York} blackout {\textbar} {
           Reuters}},
  url = {
         https://www.reuters.com/article/us-blackout-newyork/spike-in-deaths-blamed-on-2003-new-york-blackout-idUSTRE80Q07G20120127
         },
  urldate = {2018-07-23},
  author = {Grens, Kerry},
}

@article{Balitskaya2016,
  title = {Biometrika {Trust} {On} the {Representation} of a {Density} by an {
           Edgeworth} {Series} {Published} by : {Oxford} {University} {Press} on
           behalf of {Biometrika} {Trust} {Stable} {URL} :
           http://www.jstor.org/stable/2336456 {Accessed} : 20-07-2016 05 : 56 {
           UTC} {On} the representation of a d},
  volume = {75},
  abstract = {The conditions for the correct approximation of a density by the
              truncated Edgeworth series for a random variable given in a finite
              interval are found. These conditions restrict the values of
              skewness and kurtosis to achieve a positive approximating function.
              },
  number = {1},
  author = {Balitskaya, Author E O and Zolotuhina, L A},
  year = {2016},
  pages = {185--187},
  file = {PDF:/home/jmei/Zotero/storage/M2G2YGJD/edgeworth.pdf:application/pdf},
}

@article{Brenn,
  title = {A revisit of the {Gram}-{Charlier} and {Edgeworth} series expansions},
  abstract = {Funding information In this paper we make several observations on
              the Gram-Charlier and Edgeworth series, which are methods for
              mod-eling and approximating probability density functions. We
              present a simplified derivation which highlights both the
              similarity and the differences of the series expansions, that are
              often obscured by alternative derivations. We also introduce a
              reformulation of the Edgeworth series in terms of the complete
              exponential Bell polynomials, which make both series easy to
              implement and evaluate. The result is a significantly more
              accessible methodology, in the sense that it is easier to
              understand and to implement. Finally, we also make a remark on the
              Gram-Charlier series with a gamma kernel, providing a novel and
              simple expression for its coefficients. K E Y W O R D S probability
              density functions, series expansions, Edgeworth, Gram-Charlier,
              Bell polynomials, Normal kernel, gamma kernel 1 {\textbar}
              INTRODUCTION The Gram-Charlier and Edgeworth series expansions
              provide attractive alternatives when it comes to probability
              density function (PDF) estimation. They combine the simplicity of
              fitting a two-parameter PDF with the flexibility of correcting for
              higher order moments, often resulting in fast and accurate
              approximations.},
  urldate = {2018-07-22},
  author = {Brenn, Torgeir and Stian, {\textbar} and Anfinsen, Normann},
  keywords = {Abbreviations: CF, characteristic function, FT, Fourier transform,
              IID, independent and identically distributed, PDF, probability
              den-sity function, RV, random variable},
  file = {
          PDF:/home/jmei/Zotero/storage/KTXZSI47/m-api-4adba63d-a110-5423-2e66-a6fd682f50de.pdf:application/pdf
          },
}

@article{Chernozhukov,
  title = {Rearranging {Edgeworth}-{Cornish}-{Fisher} {Expansions}},
  author = {Chernozhukov, Victor and Fernandez-Val, Ivan and Galichon, Alfred},
  file = {PDF:/home/jmei/Zotero/storage/MGXNQFWU/full-text.pdf:application/pdf},
}

@article{Draper1972,
  title = {Regions of {Positive} and {Unimodal} {Series} {Expansion} of the {
           Edgeworth} and {Gram}-{Charlier} {Approximations}},
  volume = {59},
  issn = {00063444},
  url = {https://www.jstor.org/stable/2334591?origin=crossref},
  doi = {10.2307/2334591},
  abstract = {Results given by Barton \& Dennis (1952) on regions of positive
              and unimodal series expansion of the Edgeworth and Gram-Charlier
              approximations to a distribution are re-examined. An apparent
              discrepancy is found in one of their curves, and some additional
              calculations are given.},
  number = {2},
  urldate = {2018-07-22},
  journal = {Biometrika},
  author = {Draper, Norman R. and Tierney, David E.},
  month = aug,
  year = {1972},
  note = {Publisher: Oxford University PressBiometrika Trust},
  pages = {463},
}

@article{Duenas-Osorio2011,
  title = {Reliability {Assessment} of {Lifeline} {Systems} with {Radial} {
           Topology}},
  volume = {26},
  issn = {10939687},
  doi = {10.1111/j.1467-8667.2010.00661.x},
  abstract = {The increased susceptibility of lifeline systems to failure due to
              aging and external hazards requires efficient methods to quantify
              their reliability and related uncertainty. Monte Carlo simulation
              techniques for network-level reliability and uncertainty assessment
              usually require large computational experiments. Also, available
              analytical approaches apply mainly to simple network topologies,
              and are limited to providing average values, low order moments, or
              confidence bounds of reliability metrics. This study introduces a
              closed form technique to obtain the entire probability distribution
              of a reliability metric of customer service availability (CSA) for
              generic radial lifeline systems. A special case of this general
              formulation reduces to a simple sum of products equation, for which
              a recursive algorithm that exploits its structure is presented.
              This special-case algorithm computes the probability mass function
              (PMF) of CSA for systems with M elements in {\textless}file name=\{
              ''\}mice\_661\_mu1.gif\{''\} type=\{''\}gif\{''\}/{\textgreater}
              operations, relative to conventional {\textless}file name=\{''\}
              mice\_661\_mu2.gif\{''\} type=\{''\}gif\{''\}/{\textgreater}
              operations, and opens the possibility of finding recursive
              algorithms for the general radial case. Parametric models that
              approximate the CSA metric are also explored and their errors
              quantified. The proposed radial topology reliability assessment
              tools and resulting probability distributions provide
              infrastructure owners with critical insights for informed operation
              and maintenance decision making under uncertainty.},
  number = {2},
  journal = {Computer-Aided Civil and Infrastructure Engineering},
  author = {Dueñas-Osorio, Leonardo and Rojo, Javier},
  year = {2011},
  note = {ISBN: 10939687},
  pages = {111--128},
  file = {PDF:/home/jmei/Zotero/storage/QHSSHCRI/Reliability Assessment of
          Lifeline Systems with Radial Topology.pdf:application/pdf},
}

@article{Anderson2003,
  title = {Economic {Impact} 2003 {Blackout}},
  number = {517},
  journal = {AEG Working Paper},
  author = {Anderson, Patrick L and Geckil, Ilhan K},
  year = {2003},
  file = {
          PDF:/home/jmei/Zotero/storage/PQ7C5WNZ/aeg-2003_2003-blackout-cost.pdf:application/pdf
          },
}

@article{Pourbeik2006,
  title = {The {Anatomy} of a {Power} {Grid} {Blackout}},
  number = {october},
  author = {Pourbeik, Pouyan and Kundur, Prabha S. and Taylor, Carson W.},
  year = {2006},
  file = {
          PDF:/home/jmei/Zotero/storage/SPCU5KZY/Blackout_Anatomy.pdf:application/pdf
          },
}

@article{Energy2016,
  title = {{EIA} data show average frequency and duration of electric power
           outages - {Today} in {Energy} - {U}.{S}. {Energy} {Information} {
           Administration} ({EIA})},
  url = {https://www.eia.gov/todayinenergy/detail.php?id=27892},
  journal = {US Energy Information Administration},
  author = {Energy, Source U S and Administration, Information and Electric,
            Annual and Industry, Power and Eia-, Report},
  year = {2016},
  pages = {2016--2017},
  file = {
          PDF:/home/jmei/Zotero/storage/RDSFNTHU/eia-2016_power-outage-data.pdf:application/pdf
          },
}

@article{Hines2009,
  title = {Large blackouts in {North} {America}: {Historical} trends and policy
           implications},
  volume = {37},
  issn = {03014215},
  url = {http://linkinghub.elsevier.com/retrieve/pii/S0301421509005667},
  doi = {10.1016/j.enpol.2009.07.049},
  number = {12},
  journal = {Energy Policy},
  author = {Hines, P},
  year = {2009},
  keywords = {Cascading failures, electricity reliability, power system
              blackouts},
  pages = {5249--5259},
  file = {
          PDF:/home/jmei/Zotero/storage/7SFNJQA3/hines-nopub_historical-trends.pdf:application/pdf
          },
}

@techreport{Saha2003,
  title = {The {Economic} {Cost} of the {Blackout}},
  abstract = {The recent power blackout in the northeast has revived the
              discussion on the need to upgrade the transmission infrastructure.
              While that debate has its own merit, a related and a potentially
              more threatening issue to be addressed is the vulnerability of our
              electrical grid to terrorist attacks.},
  institution = {ICF Consulting},
  author = {Saha, Bansari (ICF Consulting) and Moody, Bill (ICF Consulting)},
  year = {2003},
  file = {
          PDF:/home/jmei/Zotero/storage/AYEZFYR4/icf_2003-blackout-cost.pdf:application/pdf
          },
}

@article{Carreras2016,
  title = {North {American} {Blackout} {Time} {Series} {Statistics} and {
           Implications} for {Blackout} {Risk}},
  volume = {31},
  issn = {08858950},
  doi = {10.1109/TPWRS.2015.2510627},
  abstract = {We use North American Electric Reliability Corporation historical
              data to give improved estimates of distributions of blackout size,
              time correlations, and waiting times for the Eastern and Western
              interconnections of the North American grid. We then explain and
              estimate the implications of the power law region (heavy tails) in
              the empirical distribution of blackout size in the historical data
              for the Western interconnection. Annual mean blackout size has high
              variability and the risk of large blackouts exceeds the risk of
              medium size blackouts. Ways to communicate blackout risk are
              discussed.},
  number = {6},
  journal = {IEEE Transactions on Power Systems},
  author = {Carreras, Benjamin A. and Newman, David E. and Dobson, Ian},
  year = {2016},
  note = {ISBN: 0885-8950 VO - 31},
  keywords = {Power transmission reliability, risk analysis},
  pages = {4406--4414},
  file = {
          PDF:/home/jmei/Zotero/storage/W33YFFCT/carreras-2016_heavy-tail.pdf:application/pdf
          },
}

@article{Faza2009,
  title = {Reliability analysis for the advanced electric power grid: {From}
           cyber control and communication to physical manifestations of failure},
  volume = {5775 LNCS},
  issn = {03029743},
  doi = {10.1007/978-3-642-04468-7_21},
  abstract = {The advanced electric power grid is a cyber-physical system
              comprised of physical components such as transmission lines and
              genera- tors and a network of embedded systems deployed for their
              cyber control. The objective of this paper is to qualitatively and
              quantitatively analyze the reliability of this cyber-physical
              system. The original contribution of the approach lies in the scope
              of failures analyzed, which crosses the cyber-physical boundary by
              investigating physical manifestations of fail- ures in cyber
              control. As an example of power electronics deployed to enhance and
              control the operation of the grid, we study Flexible AC
              Transmission System (FACTS) devices, which are used to alter the
              flow of power on specific transmission lines. Through prudent fault
              injection, we enumerate the failure modes of FACTS devices, as
              triggered by their embedded software, and evaluate their effect on
              the reliability of the de- vice and the reliability of the power
              grid on which they are deployed. The IEEE118 bus system is used as
              our case study, where the physical infrastructure is supplemented
              with seven FACTS devices to prevent the occurrence of four
              previously documented potential cascading failures.},
  journal = {Lecture Notes in Computer Science (including subseries Lecture
             Notes in Artificial Intelligence and Lecture Notes in
             Bioinformatics)},
  author = {Faza, Ayman Z. and Sedigh, Sahra and McMillin, Bruce M.},
  year = {2009},
  note = {ISBN: 3642044670},
  keywords = {Cyber-physical, FACTS devices, Failure propagation, Power grid,
              Reliability analysis},
  pages = {257--269},
  file = {
          PDF:/home/jmei/Zotero/storage/639GYMYR/faza-2009_cyber-physical.pdf:application/pdf
          },
}

@article{Kinney2004,
  title = {Modeling {Cascading} {Failures} in the {North} {American} {Power} {
           Grid}},
  url = {
         http://arxiv.org/abs/cond-mat/0410318%0Ahttp://dx.doi.org/10.1140/epjb/e2005-00237-9
         },
  doi = {10.1140/epjb/e2005-00237-9},
  abstract = {The North American power grid is one of the most complex
              technological networks, and its interconnectivity allows both for
              long-distance power transmission and for the propagation of
              disturbances. We model the power grid using its actual topology and
              plausible assumptions about the load and overload of transmission
              substations. Our results indicate that the loss of a single
              substation can lead to a 25\% loss of transmission efficiency by
              triggering an overload cascade in the network. We systematically
              study the damage inflicted by the loss of single nodes, and find
              three universal behaviors, suggesting that 40\% of the transmission
              substations lead to cascading failures when disrupted. While the
              loss of a single node can inflict substantial damage, subsequent
              removals have only incremental effects, in agreement with the
              topological resilience to less than 1\% node loss.},
  number = {1},
  author = {Kinney, Ryan and Crucitti, Paolo and Albert, Reka and Latora, Vito},
  year = {2004},
  note = {arXiv: cond-mat/0410318},
  pages = {1--6},
  file = {
          PDF:/home/jmei/Zotero/storage/AT79B59V/kinney-2004_cascading-failures.pdf:application/pdf
          },
}

@article{Crucitti2004,
  title = {A topological analysis of the {Italian} electric power grid},
  volume = {338},
  issn = {03784371},
  doi = {10.1016/j.physa.2004.02.029},
  abstract = {Large-scale blackouts are an intrinsic drawback of electric power
              transmission grids. Here we analyze the structural vulnerability of
              the Italian GRTN power grid by using a model for cascading failures
              recently proposed in Crucitti et al. (Phys. Rev. E 69 (2004)). ©
              2004 Elsevier B.V. All rights reserved.},
  number = {1-2 SPEC. ISS.},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Crucitti, Paolo and Latora, Vito and Marchiori, Massimo},
  year = {2004},
  note = {ISBN: 0378-4371},
  keywords = {Power grid, Structure of complex networks},
  pages = {92--97},
  file = {
          PDF:/home/jmei/Zotero/storage/LFUYQCJ9/PowerGrid_Italy.pdf:application/pdf
          },
}

@techreport{U.S.-CanadaPowerSystemOutageTaskForce2004,
  title = {Final {Report} on the {August} 14, 2003 {Blackout} in the {United} {
           States} and {Canada}: {Causes} and {Recommendations}},
  abstract = {On August 14, 2003, large portions of the Midwest and Northeast
              United States and Ontario, Canada, experienced an electric power
              blackout. The out- age affected an area with an estimated 50
              million people and 61,800 megawatts (MW) of electric load in the
              states of Ohio, Michigan, Pennsylva- nia, New York, Vermont,
              Massachusetts, Connect- icut, New Jersey and the Canadian province
              of Ontario. The blackout began a few minutes after 4:00 pm Eastern
              Daylight Time (16:00 EDT), and power was not restored for 4 days in
              some parts of the United States. Parts of Ontario suffered rolling
              blackouts for more than a week before full power was restored.
              Estimates of total costs in the United States range between \$4
              billion and \$10 billion (U.S. dollars). In Canada, gross domestic
              product was down 0.7\% in August, there was a net loss of 18.9
              million work hours, and manufacturing ship- ments in Ontario were
              down \$2.3 billion (Cana- dian dollars).},
  author = {{U.S.-Canada Power System Outage Task Force}},
  year = {2004},
  file = {
          PDF:/home/jmei/Zotero/storage/ZY36UWER/m-api-12b269be-8d77-5137-07e5-b3d469ee050a.pdf:application/pdf
          },
}

@article{Kinder1999,
  title = {Emergence of {Scaling} in {Random} {Networks}},
  volume = {286},
  issn = {00368075},
  doi = {10.1126/science.286.5439.509},
  number = {October},
  author = {Kinder, L R and Wong, T M and Meservey, R and Wang, S X and Nickel,
            J H and Meservey, R and Meservey, R and Tedrow, P M and Aoi, K and
            Hehn, M and Vaure, A and Petroff, F and Fert, A},
  year = {1999},
  pmid = {10521342},
  note = {arXiv: cond-mat/9910332 ISBN: 1095-9203 (Electronic)\{\{\}\{\$\}\{\}\}
          {\textbackslash}backslashbackslashbackslash\{{\textbackslash}\{\}{
          \textbackslash}backslash\{{\textbackslash}\{\}\{{\textbackslash}\}\}{
          \textbackslash}backslashbackslash\{{\textbackslash}\{\}{\textbackslash}
          backslash\{{\textbackslash}\$\}\{{\textbackslash}\}\}\{{\textbackslash}
          \{\}\${\textbackslash}backslash\$\{{\textbackslash}\}\}\{{
          \textbackslash}\}\}n0036-8075 (Linking)},
  pages = {509--513},
  file = {
          PDF:/home/jmei/Zotero/storage/KJSCPMH7/PowerGrid_Scaling.pdf:application/pdf
          },
}

@article{Stroker2004,
  title = {{POWER} {GRID} {RELIABILITY} {FOR} {THE} 2004 {IEEE}-{IAS} / {PCA} {
           CEMENT} {CONFERENCE} {By} :},
  author = {Stroker, John J},
  year = {2004},
  note = {ISBN: 0780382633},
  pages = {51--60},
  file = {
          PDF:/home/jmei/Zotero/storage/6DU2HLNK/PowerGrid_Vulnerability.pdf:application/pdf
          },
}

@article{Anghel2007,
  title = {Stochastic {Model} for {Power} {Grid} {Dynamics}},
  issn = {1530-1605},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4076598},
  doi = {10.1109/HICSS.2007.500},
  abstract = {We introduce a stochastic model that describes the quasistatic
              dynamics{\textbackslash}nof an electric transmission network under
              perturbations introduced{\textbackslash}nby random load
              fluctuations, random removing of system components{\textbackslash}
              nfrom service, random repair times for the failed components, and{
              \textbackslash}nrandom response times to implement optimal system
              corrections for{\textbackslash}nremoving line overloads in a
              damaged or stressed transmission network.{\textbackslash}nWe use a
              linear approximation to the network flow equations and apply{
              \textbackslash}nlinear programming techniques that optimize the
              dispatching of generators{\textbackslash}nand loads in order to
              eliminate the network overloads associated{\textbackslash}nwith a
              damaged system. We also provide a simple model for the operator's{
              \textbackslash}nresponse to various contingency events that is not
              always optimal{\textbackslash}ndue to either failure of the state
              estimation system or due to the{\textbackslash}nincorrect
              subjective assessment of the severity associated with these{
              \textbackslash}nevents. This further allows us to use a game
              theoretic framework{\textbackslash}nfor casting the optimization of
              the operator's response into the{\textbackslash}nchoice of the
              optimal strategy which minimizes the operating cost.{\textbackslash
              }nWe use a simple strategy space which is the degree of tolerance
              to{\textbackslash}nline overloads and which is an automatic control
              (optimization) parameter{\textbackslash}nthat can be adjusted to
              trade off automatic load shed without propagating{\textbackslash}
              ncascades versus reduced load shed and an increased risk of
              propagating{\textbackslash}ncascades. The tolerance parameter is
              chosen to describes a smooth{\textbackslash}ntransition from a risk
              averse to a risk taken strategy. We present{\textbackslash}
              nnumerical results comparing the responses of two power grid
              systems{\textbackslash}nto optimization approaches with different
              factors of risk and select{\textbackslash}nthe best blackout
              controlling parameter.},
  journal = {Proceedings of the 40th Annual Hawaii International Conference on
             System Sciences},
  author = {Anghel, Marian and Werley, Kenneth A and Motter, Adilson E},
  year = {2007},
  note = {arXiv: physics/0609217v1 ISBN: 0-7695-2755-8},
  pages = {113--},
  file = {
          PDF:/home/jmei/Zotero/storage/GWNKZ866/PowerGrid_Dynamics.pdf:application/pdf
          },
}

@article{Koc2014,
  title = {The impact of the topology on cascading failures in a power grid
           model},
  volume = {402},
  issn = {03784371},
  url = {http://dx.doi.org/10.1016/j.physa.2014.01.056},
  doi = {10.1016/j.physa.2014.01.056},
  abstract = {Cascading failures are one of the main reasons for large scale
              blackouts in power transmission grids. Secure electrical power
              supply requires, together with careful operation, a robust design
              of the electrical power grid topology. Currently, the impact of the
              topology on grid robustness is mainly assessed by purely
              topological approaches, that fail to capture the essence of
              electric power flow. This paper proposes a metric, the effective
              graph resistance, to relate the topology of a power grid to its
              robustness against cascading failures by deliberate attacks, while
              also taking the fundamental characteristics of the electric power
              grid into account such as power flow allocation according to
              Kirchhoff laws. Experimental verification on synthetic power
              systems shows that the proposed metric reflects the grid robustness
              accurately. The proposed metric is used to optimize a grid topology
              for a higher level of robustness. To demonstrate its applicability,
              the metric is applied on the IEEE 118 bus power system to improve
              its robustness against cascading failures. © 2014 Elsevier B.V. All
              rights reserved.},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Koç, Yakup and Warnier, Martijn and Mieghem, Piet Van and Kooij,
            Robert E. and Brazier, Frances M.T.},
  year = {2014},
  note = {arXiv: 1401.4473v1 Publisher: Elsevier B.V.},
  keywords = {Cascading failures, Power grid, Complex networks, Effective graph
              resistance, Robustness, Topology},
  pages = {169--179},
  file = {
          PDF:/home/jmei/Zotero/storage/X36BJC54/PowerGrid_Robustness.pdf:application/pdf
          },
}

@article{Albert2004,
  title = {Structural {Vulnerability} of the {North} {American} {Power} {Grid}},
  url = {
         http://arxiv.org/abs/cond-mat/0401084%0Ahttp://dx.doi.org/10.1103/PhysRevE.69.025103
         },
  doi = {10.1103/PhysRevE.69.025103},
  abstract = {The magnitude of the August 2003 blackout affecting the United
              States has put the challenges of energy transmission and
              distribution into limelight. Despite all the interest and concerted
              effort, the complexity and interconnectivity of the electric
              infrastructure have so far precluded us from understanding why
              certain events happened. In this paper we study the power grid from
              a network perspective and determine its ability to transfer power
              between generators and consumers when certain nodes are disrupted.
              We find that the power grid is robust to most perturbations, yet
              disturbances affecting key transmision substations greatly reduce
              its ability to function. We emphasize that the global properties of
              the underlying network must be understood as they greatly affect
              local behavior.},
  author = {Albert, Reka and Albert, Istvan and Nakarado, Gary L.},
  year = {2004},
  note = {arXiv: cond-mat/0401084},
  pages = {1--10},
  file = {
          100.pdf:/home/jmei/Zotero/storage/W6EGCMT2/100.pdf:application/pdf;PDF:/home/jmei/Zotero/storage/63PVRKPB/PowerGridPaper.pdf:application/pdf
          },
}

@article{Ipakchi2009,
  title = {Grid of the future},
  volume = {7},
  issn = {15407977},
  doi = {10.1109/MPE.2008.931384},
  abstract = {MANY BELIEVE THE ELECTRIC POWER SYSTEM IS UNDERGOING A PROFOUND
              change driven by a number of needs. There’s the need for
              environmental compliance and energy conservation. We need better
              grid reliability while dealing with an aging infrastructure. And we
              need improved operational effi ciencies and customer service. The
              changes that are happening are particularly signifi cant for the
              electricity distribution grid, where “blind” and manual operations,
              along with the electromechanical components, will need to be
              transformed into a “smart grid.” This transformation will be
              necessary to meet environmental targets, to accommodate a greater
              emphasis on demand response (DR), and to support plug-in hybrid
              electric vehicles (PHEVs) as well as distributed generation and
              storage capabilities. It is safe to say that these needs and
              changes present the power industry with the biggest challenge it
              has ever faced. On one hand, the transition to a smart grid has to
              be evolutionary to keep the lights on; on the other hand, the
              issues surrounding the smart grid are signifi cant enough to demand
              major changes in power systems operating philosophy.},
  number = {2},
  journal = {IEEE Power and Energy Magazine},
  author = {Ipakchi, Ali and Albuyeh, Farrokh},
  year = {2009},
  pmid = {21821573},
  note = {arXiv: 1011.1669v3 ISBN: 1540-7977 VO - 7},
  pages = {52--62},
  file = {PDF:/home/jmei/Zotero/storage/G6TJVB7L/SmartGrid.pdf:application/pdf},
}

@article{Fang2012,
  title = {Smart {Grid} – {The} {New} and {Improved} {Power} {Grid} :},
  volume = {14},
  issn = {1553-877X},
  doi = {10.1109/SURV.2011.101911.00087},
  abstract = {The Smart Grid, regarded as the next generation power grid, uses
              two-way flows of electricity and information to create a widely
              distributed automated energy delivery net- work. In this article,
              we survey the literature till 2011 on the enabling technologies for
              the Smart Grid. We explore three major systems, namely the smart
              infrastructure system, the smart management system, and the smart
              protection system. We also propose possible future directions in
              each system. Specifically, for the smart infrastructure system, we
              explore the smart energy subsystem, the smart information subsystem
              , and the smart communication subsystem. For the smart manage- ment
              system, we explore various management objectives, such as improving
              energy efficiency, profiling demand, maximizing utility, reducing
              cost, and controlling emission. We also explore various management
              methods to achieve these objectives. For the smart protection
              system, we explore various failure protection mechanisms which
              improve the reliability of the Smart Grid, and explore the security
              and privacy issues in the Smart Grid.},
  number = {4},
  journal = {IEEE Communications Surveys and Tutorials},
  author = {Fang, Xi and Misra, Satyajayant and Xue, Guoliang and Yang, Dejun},
  year = {2012},
  keywords = {energy, communications, information, management, power grid,
              privacy, protection, security, Smart grid, survey},
  pages = {944--980},
  file = {
          PDF:/home/jmei/Zotero/storage/9QF3696H/SmartGrid_Survey.pdf:application/pdf
          },
}

@article{Bahmanifirouzi2012,
  title = {A new hybrid hbmo-sfla algorithm for multi-objective distribution
           feeder reconfiguration problem considering distributed generator units
           },
  volume = {36},
  issn = {22286179},
  doi = {10.1002/etep},
  abstract = {Distribution feeder reconfiguration (DFR) is one of the well-known
              and effective strategies adopted in distribution network. The goal
              of DFR problem is to obtain a new topological structure for
              distribution feeders by rearranging the status of switches such
              that an optimal configuration would be obtained. The existence of
              Distributed Generation (DG) can affect the entire power system and
              especially distribution networks. This paper presents an efficient
              approach for multi-objective DFR problem considering the
              simultaneous effect of DG units. The objective functions to be
              investigated are 1) power losses, 2) voltage deviation of buses, 3)
              emission produced by DG units and distribution companies and 4) the
              total cost of the active power generated by DG units and
              distribution companies. The new evolutionary method is based on an
              efficient multi-objective hybrid honey bee mating optimization
              (HBMO) and shuffled frog leaping algorithm (SFLA) called
              MHBMO-SFLA. The proposed hybrid algorithm integrates the
              outstanding characteristics of SFLA to improve the performance of
              HBMO algorithm sufficiently. In the proposed MHBMO-SFLA, an
              external repository is considered to save non-dominated solutions
              which are found during the search process. Also, since the
              objective functions are not the same, a fuzzy clustering technique
              is utilized to control the size of the repository within the
              limits. A distribution test feeder is considered to evaluate the
              feasibility and effectiveness of the proposed approach. © Shiraz
              University.},
  number = {E1},
  journal = {Iranian Journal of Science and Technology - Transactions of
             Electrical Engineering},
  author = {Bahmanifirouzi, B. and Farjah, E. and Niknam, T. and Azad Farsani,
            E.},
  year = {2012},
  pmid = {6546926},
  note = {ISBN: 1546-3109},
  keywords = {Distributed generator (DG), Multi-objective distribution feeder
              reconfiguratio, Multi-objective honey bee mating optimization (MHB,
              Multi-objective shuffled frog leaping algorithm (M},
  pages = {51--66},
  file = {
          PDF:/home/jmei/Zotero/storage/5GRTSLVW/Yamashita_et_al-2008-European_Transactions_on_Electrical_Power.pdf:application/pdf
          },
}

@patent{breiman_statistical_2001,
  title = {Statistical {Modeling}: {The} {Two} {Cultures}},
  abstract = {There are two cultures in the use of statistical modeling to reach
              conclusions from data. One assumes that the data are generated by a
              given stochastic data model. The other uses algorithmic models and
              treats the data mechanism as unknown. The statistical community has
              been committed to the almost exclusive use of data models. This
              commitment has led to irrelevant theory, questionable conclusions,
              and has kept statisticians from working on a large range of
              interesting current problems. Algorithmic modeling, both in theory
              and practice, has developed rapidly in fields outside statistics.
              It can be used both on large complex data sets and as a more
              accurate and informative alternative to data modeling on smaller
              data sets. If our goal as a field is to use data to solve problems,
              then we need to move away from exclusive dependence on data models
              and adopt a more diverse set of tools.},
  urldate = {2019-07-23},
  author = {Breiman, Leo},
  year = {2001},
  note = {Publication Title: Statistical Science Volume: 16 Issue: 3},
  pages = {199--231},
  file = {
          PDF:/home/jmei/Zotero/storage/36Y5LVBE/m-api-4c1faf00-4656-f5e1-52f9-39cf6ea5abd2.pdf:application/pdf
          },
}

@article{Geyer1992,
  title = {The {Interface} between {Statistics} and {Philosophy} of {Science}},
  volume = {10},
  issn = {2168-8745},
  doi = {10.2307/2246134},
  number = {4},
  journal = {Statistical Science},
  author = {I.J., Good},
  year = {1992},
  pmid = {20948974},
  note = {arXiv: 1011.1669v3 ISBN: 0883-4237},
  pages = {354--363},
  file = {
          PDF:/home/jmei/Zotero/storage/WDKQGSM7/Interface_Between_Statistics_and_Philosophy_of_Science.pdf:application/pdf
          },
}

@article{Donoho2000,
  title = {The {Curses} and {Blessings} of {Dimensionality}},
  abstract = {The coming century is surely the century of data. A combination of
              blind faith and serious purpose makes our society invest massively
              in the collection and processing of data of all kinds, on scales
              unimaginable until recently. Hyperspectral Imagery, Internet
              Portals, Financial tick-by-tick data, and DNA Microarrays are just
              a few of the better-known sources, feeding data in torrential
              streams into scientific and business databases worldwide. In
              traditional statistical data analysis, we think of observations of
              instances of par-ticular phenomena (e.g. instance ↔ human being),
              these observations being a vector of values we measured on several
              variables (e.g. blood pressure, weight, height, ...). In
              traditional statistical methodology, we assumed many observations
              and a few, well-chosen variables. The trend today is towards more
              observations but even more so, to radically larger numbers of
              variables – voracious, automatic, systematic collection of
              hyper-informative detail about each observed instance. We are
              seeing examples where the observations gathered on individual
              instances are curves, or spectra, or images, or even movies, so
              that a single observation has dimensions in the thousands or
              billions, while there are only tens or hundreds of instances
              available for study. Classical methods are simply not designed to
              cope with this kind of explosive growth of dimensionality of the
              observation vector. We can say with complete confidence that in the
              coming cen-tury, high-dimensional data analysis will be a very
              significant activity, and completely new methods of
              high-dimensional data analysis will be developed; we just don't
              know what they are yet. Mathematicians are ideally prepared for
              appreciating the abstract issues involved in finding patterns in
              such high-dimensional data. Two of the most influential prin-ciples
              in the coming century will be principles originally discovered and
              cultivated by mathematicians: the blessings of dimensionality and
              the curse of dimensionality. The curse of dimensionality is a
              phrase used by several subfields in the mathematical sciences; I
              use it here to refer to the apparent intractability of
              systematically searching through a high-dimensional space, the
              apparent intractability of accurately approxi-mating a general
              high-dimensional function, the apparent intractability of
              integrating a high-dimensional function. The blessings of
              dimensionality are less widely noted, but they include the
              concen-tration of measure phenomenon (so-called in the geometry of
              Banach spaces), which means that certain random fluctuations are
              very well controlled in high dimensions and the success of
              asymptotic methods, used widely in mathematical statistics and
              statis-tical physics, which suggest that statements about very
              high-dimensional settings may be made where moderate dimensions
              would be too complicated. 1 There is a large body of interesting
              work going on in the mathematical sciences, both to attack the
              curse of dimensionality in specific ways, and to extend the
              benefits of dimensionality. I will mention work in high-dimensional
              approximation theory, in probability theory, and in mathematical
              statistics. I expect to see in the coming decades many further
              mathematical elaborations to our inventory of Blessings and Curses,
              and I expect such contributions to have a broad impact on society's
              ability to extract meaning from the massive datasets it has decided
              to compile. At the end of my talk, I will also draw on my personal
              research experiences. This suggest to me (1) ongoing developments
              in high-dimensional data analysis may lead mathematicians to study
              new problems in for example harmonic analysis; and (2) that many of
              the problems of low dimensional data analysis are unsolved and are
              similar to problems in harmonic analysis which have only recently
              been attacked, and for which only the merest beginnings have been
              made. Both fields can progress together. Dedication. To the Memory
              of John Wilder Tukey 1915-2000.},
  journal = {American Math. Society Lecture-Math Challenges of the 21st Century},
  author = {Donoho, David L},
  year = {2000},
  keywords = {Acknowledgments, Algorithms, Analy-sis, Analysis, Components,
              Computational, Data, Harmonic, Independent, Mining, Multivariate,
              Principal, Randomized, The effort to prepare},
  pages = {1--33},
  file = {
          PDF:/home/jmei/Zotero/storage/ELJDAPLQ/Curses_Blessings_Dimensionality.pdf:application/pdf
          },
}

@article{tadesse_why_2018,
  title = {Why are marine ecosystems biologically more diversified than their
           equivalent terrestrial ecosystems?},
  volume = {3},
  doi = {10.15406/ijawb.2018.03.00105},
  abstract = {Marine ecosystems are ecosystems found in a body of water. They
              are composed of the communities of organisms that are dependent on
              each other and on their environment live in marine ecosystems.
              Marine waters cover two-thirds of the surface of the earth.1 Such
              places are considered ecosystems because the plant life supports
              the animal life and vice-versa. Marine ecosystems are very
              important for the overall health of both marine and terrestrial
              environments. According to the World Resource Center, coastal
              habitats alone account for approximately 1/3 of all marine
              biological productivity, and estuarine ecosystems (i.e. salt
              marshes, sea-grasses, and mangrove forests) are among the most
              productive regions on the planet. In addition, other marine
              ecosystems, such as coral reefs provide food and shelter to the
              highest levels of marine diversity in the world. Marine ecosystems
              usually have a large biodiversity and are, therefore, thought to
              have a good resistance against invasive species. However,
              exceptions have been observed, and the mechanisms responsible in
              determining the success of an invasion are not yet clear.1},
  number = {4},
  journal = {International International Journal of Avian \& Wildlife Biology},
  author = {Tadesse, Solomon Ayele},
  year = {2018},
  pages = {304--305},
  file = {PDF:/home/jmei/Zotero/storage/MFC9SPL3/Why are marine ecosystems
          biologically more diversified than their equivalent terrestrial
          ecosystems.pdf:application/pdf},
}

@article{Mashreghi2015,
  title = {Mathematical {Writing}},
  volume = {1},
  issn = {1098-6596},
  doi = {10.1017/CBO9781107415324.004},
  number = {3},
  journal = {Statewide Agricultural Land Use Baseline 2015},
  author = {Knuth, Donald E. and Larrabee, Tracy and Roberts, Paul M.},
  year = {2015},
  pmid = {25246403},
  note = {arXiv: 1011.1669v3 ISBN: 9788578110796},
  keywords = {icle},
  pages = {12},
  file = {
          PDF:/home/jmei/Zotero/storage/PTSU2D3K/knuth_mathematical_writing.pdf:application/pdf
          },
}

@book{Alon2006,
  title = {An introduction to systems biology: {Design} principles of biological
           circuits},
  isbn = {978-1-4200-1143-2},
  abstract = {Thorough and accessible, this book presents the design principles
              of biological systems, and highlights the recurring circuit
              elements that make up biological networks. It provides a simple
              mathematical framework which can be used to understand and even
              design biological circuits. The textavoids specialist terms,
              focusing instead on several well-studied biological systems that
              concisely demonstrate key principles. An Introduction to Systems
              Biology: Design Principles of Biological Circuits builds a solid
              foundation for the intuitive understanding of general principles.
              It encourages the reader to ask why a system is designed in a
              particular way and then proceeds to answer with simplified models.},
  author = {Alon, Uri},
  year = {2006},
  note = {Publication Title: An Introduction to Systems Biology: Design
          Principles of Biological Circuits},
  file = {PDF:/home/jmei/Zotero/storage/4TASQ25P/Systems
          Biology.pdf:application/pdf},
}

@article{kosa_are_2012,
  title = {Are {Older} {Adults} {Prepared} to {Ensure} {Food} {Safety} {During}
           {Extended} {Power} {Outages} and {Other} {Emergencies}?: {Findings}
           from a {National} {Survey}},
  volume = {38},
  doi = {10.1080/03601277.2011.645436},
  number = {11},
  urldate = {2019-08-31},
  journal = {Educational Gerontology},
  author = {Kosa, Katherine M. and Cates, Sheryl C. and Karns, Shawn},
  year = {2012},
  pages = {763--775},
  file = {
          PDF:/home/jmei/Zotero/storage/PWGWC5YG/m-api-44f824e4-c249-a66d-5753-9d03ae20119e.pdf:application/pdf
          },
}

@article{Casella2006,
  title = {Statistical {Inference}},
  volume = {102},
  issn = {01621459},
  url = {http://books.google.com/books?id=9tv0taI8l6YC},
  doi = {10.1016/j.peva.2007.06.006},
  abstract = {Review From the reviews: .,."There are interesting and
              non-standard topics that are not usually included in a first course
              in measture-theoretic probability including Markov Chains and MCMC,
              the bootstrap, limit theorems for martingales and mixing sequences,
              Brownian motion and Markov processes. The material is well-suported
              with many end-of-chapter problems." D.L. McLeish for Short Book
              Reviews of the ISI, December 2006 "The reader sees not only how
              measure theory is used to develop probability theory, but also how
              probability theory is used in applications. a The discourse is
              delivered in a theorem proof format and thus is better suited for
              classroom a . The authors prose is generally well thought out a .
              will make an attractive choice for a two-semester course on measure
              and probability, or as a second course for students with a semester
              of measure or probability theory under their belt." (Peter C.
              Kiessler, Journal of the American Statistical Association, Vol. 102
              (479), 2007) "The book is a well written self-contained textbook on
              measure and probability theory. It consists of 18 chapters. Every
              chapter contains many well chosen examples and ends with several
              problems related to the earlier developed theory (some with hints).
              a At the very end of the book there is an appendix collecting
              necessary facts from set theory, calculus and metric spaces. The
              authors suggest a few possibilities on how to use their book."
              (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The
              title of the book consists of the names of its two basic parts. The
              booka (TM)s third part is comprised of some special topics from
              probability theory. a The authors suggest using the book
              intwo-semester graduate programs in statistics or a one-semester
              seminar on special topics. The material of the book is standard a
              is clear, comprehensive and a {\textasciitilde}without being
              intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue
              2007 f) Product Description This is a graduate level textbook on
              measure theory and probability theory. The book can be used as a
              text for a two semester sequence of courses in measure theory and
              probability theory, with an option to include supplemental material
              on stochastic processes and special topics. It is intended
              primarily for first year Ph.D. students in mathematics and
              statistics although mathematically advanced students from
              engineering and economics would also find the book useful.
              Prerequisites are kept to the minimal level of an understanding of
              basic real analysis concepts such as limits, continuity,
              differentiability, Riemann integration, and convergence of
              sequences and series. A review of this material is included in the
              appendix. The book starts with an informal introduction that
              provides some heuristics into the abstract concepts of measure and
              integration theory, which are then rigorously developed. The first
              part of the book can be used for a standard real analysis course
              for both mathematics and statistics Ph.D. students as it provides
              full coverage of topics such as the construction of
              Lebesgue-Stieltjes measures on real line and Euclidean spaces, the
              basic convergence theorems, L p spaces, signed measures,
              Radon-Nikodym theorem, Lebesgue's decomposition theorem and the
              fundamental theorem of Lebesgue integration on R, product spaces
              and product measures, and Fubini-Tonelli theorems. It also provides
              an elementary introduction to Banach and Hilbert spaces,
              convolutions, Fourier series and Fourier and Plancherel transforms.
              Thus part I would be particularly useful for students in a typical
              Statistics Ph.D. program if a separate course on real analysis is
              not a standard requirement. Part II (chapters 6-13) provides full
              coverage of standard graduate level probability theory. It starts
              with Kolmogorov's probability model and Kolmogorov's existence
              theorem. It then treats thoroughly the laws of large numbers
              including renewal theory and ergodic theorems with applications and
              then weak convergence of probability distributions, characteristic
              functions, the Levy-Cramer continuity theorem and the central limit
              theorem as well as stable laws. It ends with conditional
              expectations and conditional probability, and an introduction to
              the theory of discrete time martingales. Part III (chapters 14-18)
              provides a modest coverage of discrete time Markov chains with
              countable and general state spaces, MCMC, continuous time discrete
              space jump Markov processes, Brownian motion, mixing sequences,
              bootstrap methods, and branching processes. It could be used for a
              topics/seminar course or as an introduction to stochastic
              processes. From the reviews: "...There are interesting and
              non-standard topics that are not usually included in a first course
              in measture-theoretic probability including Markov Chains and MCMC,
              the bootstrap, limit theorems for martingales and mixing sequences,
              Brownian motion and Markov processes. The material is well-suported
              with many end-of-chapter problems." D.L. McLeish for Short Book
              Reviews of the ISI, December 2006},
  journal = {Design},
  author = {Kiefer, Jack Carl},
  year = {2006},
  pmid = {10911016},
  note = {ISBN: 9780387781884},
  pages = {618},
  file = {PDF:/home/jmei/Zotero/storage/XXJP3435/(Springer Texts in Statistics)
          Jack Carl Kiefer (auth.), Gary Lorden (eds.) - Introduction to
          Statistical Inference-Springer-Verlag New York
          (1987).pdf:application/pdf},
}

@misc{noauthor_principles_of_mathematical_analysis_walter_rudin.pdf_nodate,
  title = {principles\_of\_mathematical\_analysis\_walter\_rudin.pdf},
  file = {
          PDF:/home/jmei/Zotero/storage/P5VRKEEW/principles_of_mathematical_analysis_walter_rudin.pdf:application/pdf
          },
}

@article{Hernandez-Fajardo2013,
  title = {Probabilistic study of cascading failures in complex interdependent
           lifeline systems},
  volume = {111},
  issn = {09518320},
  doi = {10.1016/j.ress.2012.10.012},
  abstract = {The internal complexity of lifeline systems and their standing
              interdependencies can operate in conjunction to amplify the
              negative effects of external disruptions. This paper introduces a
              simulation-based methodology to evaluate the joint impact of
              interdependence, component fragilities, and cascading failures in
              systemic fragility estimates. The proposed strategy uses a graph
              model of interdependent networks, an enhanced betweenness
              centrality for cascading failures approximation, and an
              interdependence model accounting for coupling uncertainty in the
              simulation of damage propagation for probabilistic performance
              assessment. This methodology is illustrated through its application
              to a realistic set of power and water networks subjected to
              earthquake scenarios and random failures. Test case results reveal
              two key insights: (1) the intensity of a perturbation influences
              interdependent systemic fragility by shaping the magnitudes of
              initial component damage and, sometimes counter-intuitively, the
              subsequent interdependence effects and (2) increasing local
              redundancy mitigates the effects of interdependence on systemic
              performance, but such intervention is incapable of eliminating
              interdependent effects completely. The previous insights provide
              basic guidelines for the design of systemic retrofitting policies.
              Additionally, the limitations of local capacity redundancy as a
              fragility control measure highlight the need for a critical
              assessment of intervention strategies in distributed infrastructure
              networks. Future work will assess the fragility-reduction
              efficiency of strategies involving informed manipulation of
              individual systemic topologies and the interdependence interfaces
              connecting them. © 2012 Published by Elsevier B.V. All rights
              reserved.},
  journal = {Reliability Engineering and System Safety},
  author = {Hernandez-Fajardo, Isaac and Dueñas-Osorio, Leonardo},
  year = {2013},
  note = {ISBN: 9780784411704},
  keywords = {Cascading failures, Fragility propagation, Interdependence,
              Lifeline Systems, Local redundancy, Random failures, Seismic hazard
              },
  pages = {260--272},
}

@article{Vershynin2018,
  title = {High-dimensional probability: {An} introduction with applications in
           data science},
  issn = {00223417},
  url = {
         https://www.cambridge.org/gb/academic/subjects/statistics-probability/probability-theory-and-stochastic-processes/high-dimensional-probability-introduction-applications-data-science?format=HB#8FyVeUCeT4PcEjMp.97
         },
  doi = {10.1002/path.1109},
  abstract = {Description Contents Resources Courses About the Authors
              High-dimensional probability offers insight into the behavior of
              random vectors, random matrices, random subspaces, and objects used
              to quantify uncertainty in high dimensions. Drawing on ideas from
              probability, analysis, and geometry, it lends itself to
              applications in mathematics, statistics, theoretical computer
              science, signal processing, optimization, and more. It is the first
              to integrate theory, key tools, and modern applications of
              high-dimensional probability. Concentration inequalities form the
              core, and it covers both classical results such as Hoeffding's and
              Chernoff's inequalities and modern developments such as the matrix
              Bernstein's inequality. It then introduces the powerful methods
              based on stochastic processes, including such tools as Slepian's,
              Sudakov's, and Dudley's inequalities, as well as generic chaining
              and bounds based on VC dimension. A broad range of illustrations is
              embedded throughout, including classical and modern results for
              covariance estimation, clustering, networks, semidefinite
              programming, coding, dimension reduction, matrix completion,
              machine learning, compressed sensing, and sparse regression. Closes
              the gap between the standard probability curriculum and what
              mathematical data scientists need to know Selects the core ideas
              and methods and presents them systematically with modern motivating
              applications to bring readers quickly up to speed Features
              integrated exercises that invite readers to sharpen their skills
              and build practical intuition},
  author = {Vershynin, Roman},
  year = {2018},
  note = {ISBN: 9781108415194},
  pages = {293},
  file = {PDF:/home/jmei/Zotero/storage/Q7HJNAZT/HDP-book.pdf:application/pdf},
}

@article{ClimateScienceSpecialReport2017,
  title = {Fourth {National} {Climate} {Assessment} ({NCA4})},
  volume = {II},
  url = {https://science2017.globalchange.gov/},
  doi = {10.7930/J0J964J6},
  author = {{Climate Science Special Report}},
  year = {2017},
  file = {
          PDF:/home/jmei/Zotero/storage/I5L9KX54/NCA4_Report-in-Brief.pdf:application/pdf
          },
}

@article{Neher2018,
  title = {Progress and open problems in evolutionary dynamics},
  author = {Neher, Richard A and Walczak, Aleksandra M},
  year = {2018},
  note = {arXiv: 1804.07720v1},
  pages = {1--33},
  file = {
          PDF:/home/jmei/Zotero/storage/5JAALJDH/problems_in_evolutionary_dynamics.pdf:application/pdf
          },
}

@article{easley_chapter_2010,
  title = {Chapter 7 {Evolutionary} {Game} {Theory}},
  author = {Easley, By David},
  year = {2010},
  file = {
          PDF:/home/jmei/Zotero/storage/7UVZ5XRB/evolutionary_game_theory.pdf:application/pdf
          },
}

@article{MoraP.aSeuge2006,
  title = {Concentration {Inequalities} and {Model} {Selection}},
  volume = {42},
  issn = {00758434},
  url = {
         http://www.scopus.com/inward/record.url?eid=2-s2.0-33845310048&partnerID=40&md5=70edaf9bb615ef78c6647fc6dfc6f059
         },
  doi = {10.1007/978-3-540-48503-2},
  abstract = {Model selection is a classical topic in statistics. The idea of
              selecting a model via penalizing a log-likelihood type criterion
              goes back to the early seventies with the pioneering works of
              Mallows and Akaike. One can find many consistency results in the
              literature for such criteria. These results are asymptotic in the
              sense that one deals with a given number of models and the number
              of observations tends to infinity. We shall give an overview of a
              non asymptotic theory for model selection which has emerged during
              the past ten years. In various contexts of function estimation it
              is possible to design penalized log-likelihood type criteria with
              penalty terms depending not only on the number of parameters
              defining each model (as for the classical criteria) but also on the
              {\textbackslash}guillemotleft complexity{\textbackslash}
              guillemotright{\textbackslash} of the whole collection of models to
              be considered. The performance of such a criterion is analyzed via
              non asymptotic risk bounds for the corresponding penalized
              estimator which express that it performs almost as well as if the {
              \textbackslash}guillemotleft best model{\textbackslash}
              guillemotright{\textbackslash} (i.e. with minimal risk) were known.
              For practical relevance of these methods, it is desirable to get a
              precise expression of the penalty terms involved in the penalized
              criteria on which they are based. This is why this approach heavily
              relies on concentration inequalities, the prototype being
              Talagrand's inequality for empirical processes. Our purpose will be
              to give an account of the theory and discuss some selected
              applications such as variable selection or change points detection.
              },
  number = {SUPPL. 1},
  journal = {European Journal of Soil Biology},
  author = {Massart, Pascal},
  year = {2006},
  pmid = {19914479},
  note = {ISBN: 3540484973},
  keywords = {Ancistrotermes guineensis, Isoptera, Mangifera ind},
  pages = {S250--S253},
  file = {
          PDF:/home/jmei/Zotero/storage/KVCKI7NM/concentration_inequalities_model_selection.pdf:application/pdf
          },
}

@article{Starzyk1975,
  title = {Topics in {Random} {Matrix} {Theory}},
  volume = {24},
  number = {2},
  journal = {Arch Elektrotech},
  author = {Tao, Terry},
  year = {1975},
  pages = {237--244},
  file = {
          PDF:/home/jmei/Zotero/storage/E4I74Z4C/tao_random_matrix.pdf:application/pdf
          },
}

@article{Luo2011,
  title = {Application of {Random} {Matrix} {Theory} to {Analyze} {Biological} {
           Data}},
  volume = {2},
  doi = {10.1007/978-1-4614-1415-5_28},
  author = {Luo, F and Srimani, P K and Zhou, J Z},
  year = {2011},
  note = {ISBN: 978-1-4614-1414-8},
  pages = {711--732},
  file = {
          PDF:/home/jmei/Zotero/storage/E5CNM8H5/random_matrix_biology.pdf:application/pdf
          },
}

@article{Theses2017,
  title = {A review of random matrix theory with an application to biological
           data {Presented} to the {Graduate} {Faculty} of the {In} {Partial} {
           Fulfillment} of the {Requirements} for the {Degree}},
  author = {Theses, Masters and Marks, Jesse Aaron},
  year = {2017},
  keywords = {Networks, Random Matrix Theory},
  file = {PDF:/home/jmei/Zotero/storage/EXTXMXYS/RMT_biology.pdf:application/pdf
          },
}

@article{Grimmett1997,
  title = {Percolation and disordered systems},
  volume = {1665},
  issn = {0942-9352},
  url = {http://link.springer.com/10.1007/BFb0092620},
  doi = {10.1007/BFb0092620},
  abstract = {Fractals and disordered systems have recently become the focus of
              intense interest in research. This book discusses in great detail
              the effects of disorder on mesoscopic scales (fractures, aggregates
              , colloids, surfaces and interfaces, glasses and polymers) and
              presents tools to describe them in mathematical language. A
              substantial part is devoted to the development of scaling theories
              based on fractal concepts. In ten chapters written by leading
              experts in the field, the reader is introduced to basic concepts
              and techniques in disordered systems and is led to the forefront of
              current research. This second edition has been substantially
              revised and updates the literature in this important field.},
  author = {Grimmett, Geoffrey},
  year = {1997},
  pmid = {3507393},
  note = {arXiv: 1011.1669v3 ISBN: ISBN-10: 3642848702{\textbackslash}nISBN-13:
          978-3642848704},
  pages = {153--300},
  file = {
          PDF:/home/jmei/Zotero/storage/53P6B9TI/Percolation_Theory.pdf:application/pdf
          },
}

@article{Cartin2014,
  title = {Quantifying the {Fermi} paradox in the local {Solar} neighbourhood},
  volume = {67},
  issn = {0007084X},
  abstract = {The Fermi paradox highlights the dichotomy between the lack of
              physical contact with other civilizations and the expectation that
              technological civilizations are assumed likely to evolve in many
              locations in the Milky Way galaxy, given the large number of
              planetary systems within this galaxy. Work by Landis and others has
              modeled this question in terms of percolation theory and cellular
              automata, using this method to parametrize our ignorance about
              possible other civilizations as a function of the probability of
              one system to colonize another, and the maximum number of systems
              reachable from each starting location (i.e. the degree in the
              network used for percolation). These models used a fixed lattice of
              sites to represent a stellar region, so the degree of all sites
              were identical. In this paper, the question is examined again, but
              instead of using a pre-determined lattice, the actual physical
              positions of all known star systems within 40 parsecs of the Solar
              System are used as percolation sites; in addition, the number of
              sites accessible for further colonization from a given system is
              determined by a choice of maximum distance such efforts can travel
              across. The resulting simulations show that extraterrestrial
              colonization efforts may reach the Solar System, but only for
              certain values of the maximum travel distance and probability of an
              occupied system further colonizing other systems. Indeed, large
              numbers of systems may be colonized with either vessels that lack
              insufficient travel distance to reach the Solar System or else have
              a colonization probability where they are statistically unlikely to
              reach us.},
  number = {3},
  journal = {JBIS - Journal of the British Interplanetary Society},
  author = {Cartin, Daniel},
  year = {2014},
  note = {arXiv: 1404.0204},
  keywords = {Fermi paradox, Percolation theory, Solar neighbourhood},
  pages = {119--126},
  file = {
          PDF:/home/jmei/Zotero/storage/XI3Y2ME8/fermi_paradox_percolation.pdf:application/pdf
          },
}

@techreport{gerard_t_hooft_cellular_2015,
  title = {The {Cellular} {Automaton} {Interpretation} of {Quantum} {Mechanics}},
  abstract = {When investigating theories at the tiniest conceivable scales in
              nature, almost all researchers today revert to the quantum language
              , accepting the verdict from the Copenhagen doctrine that the only
              way to describe what is going on will always involve states in
              Hilbert space, controlled by operator equations. Returning to
              classical, that is, non quantum mechanical, descriptions will be
              forever impossible, unless one accepts some extremely contrived
              theoretical constructions that may or may not reproduce the quantum
              mechanical phenomena observed in experiments. Dissatisfied, this
              author investigated how one can look at things differently. This
              book is an overview of older material, but also contains many new
              observations and calculations. Quantum mechanics is looked upon as
              a tool, not as a theory. Examples are displayed of models that are
              classical in essence, but can be analysed by the use of quantum
              techniques, and we argue that even the Standard Model, together
              with gravitational interactions, might be viewed as a quantum
              mechanical approach to analyse a system that could be classical at
              its core. We explain how such thoughts can conceivably be
              reconciled with Bell's theorem, and how the usual objections voiced
              against the notion of 'superdeterminism' can be overcome, at least
              in principle. Our proposal would eradicate the collapse problem and
              the measurement problem. Even the existence of an "arrow of time"
              can perhaps be explained in a more elegant way than usual.},
  urldate = {2018-10-22},
  author = {{Gerard 't Hooft}},
  year = {2015},
  note = {arXiv: 1405.1548v3},
  file = {
          PDF:/home/jmei/Zotero/storage/DKLTTTFL/m-api-073fe5cd-9acb-1bf1-7417-c434416c4ee6.pdf:application/pdf
          },
}

@article{England2013,
  title = {Statistical physics of self-replication},
  volume = {139},
  issn = {00219606},
  doi = {10.1063/1.4818538},
  abstract = {Self-replication is a capacity common to every species of living
              thing, and simple physical intuition dictates that such a process
              must invariably be fueled by the production of entropy. Here, we
              undertake to make this intuition rigorous and quantitative by
              deriving a lower bound for the amount of heat that is produced
              during a process of self-replication in a system coupled to a
              thermal bath. We find that the minimum value for the physically
              allowed rate of heat production is determined by the growth rate,
              internal entropy, and durability of the replicator, and we discuss
              the implications of this finding for bacterial cell division, as
              well as for the pre-biotic emergence of self-replicating nucleic
              acids.},
  number = {12},
  journal = {Journal of Chemical Physics},
  author = {England, Jeremy L.},
  year = {2013},
  pmid = {24089735},
  note = {arXiv: 1209.1179 ISBN: 1089-7690 (Electronic){\textbackslash}
          r0021-9606 (Linking)},
  file = {
          PDF:/home/jmei/Zotero/storage/BL7WMK42/StatisticalPhysics_of_SelfReplication.pdf:application/pdf
          },
}

@article{Adami2012,
  title = {The use of information theory in evolutionary biology.},
  volume = {1256},
  issn = {1749-6632},
  doi = {10.1111/j.1749-6632.2011.06422.x},
  abstract = {Information is a key concept in evolutionary biology. Information
              stored in a biological organism's genome is used to generate the
              organism and to maintain and control it. Information is also that
              which evolves. When a population adapts to a local environment,
              information about this environment is fixed in a representative
              genome. However, when an environment changes, information can be
              lost. At the same time, information is processed by animal brains
              to survive in complex environments, and the capacity for
              information processing also evolves. Here, I review applications of
              information theory to the evolution of proteins and to the
              evolution of information processing in simulated agents that adapt
              to perform a complex task.},
  journal = {Ann. N. Y. Acad. Sci.},
  author = {Adami, Christoph},
  year = {2012},
  pmid = {22320231},
  note = {arXiv: 1112.3867 ISBN: 1749-6632},
  pages = {49--65},
  file = {
          PDF:/home/jmei/Zotero/storage/4XRNS4C5/Information_Theory_Biology.pdf:application/pdf
          },
}

@book{Reddy1998,
  title = {Introductory {Functional} {Analysis}},
  volume = {27},
  isbn = {978-1-4612-6824-6},
  url = {http://link.springer.com/10.1007/978-1-4612-0575-3},
  abstract = {Mathematics is playing an ever more important role in the physical
              and biological sciences, provoking a blurring of boundaries between
              scientific dis- ciplines and a resurgence of interest in the modern
              as weil as the classical techniques of applied mathematics. This
              renewal of interest, both in research and teaching, has led to the
              establishment of the series: Texts in Applied Mathematics (TAM).
              The development of new courses is a natural consequence of a .high
              level of excitement on the research frontier as newer techniques,
              such as numerical and symbolic computer systems, dynamical systems,
              and chaos, mix with and reinforce the traditional methods of
              applied mathematics. Thus, the purpose of this textbook series is
              to meet the current and future needs of these advances and
              encourage the teaching of new courses. TAM will publish textbooks
              suitable für use in advanced undergraduate and beginning graduate
              courses, and will complement the Applied Mathematical Sciences
              (AMS) series, which will focus on advanced textbooks and research
              level monographs.},
  author = {Kreszig, Erwin},
  year = {1998},
  doi = {10.1007/978-1-4612-0575-3},
  file = {PDF:/home/jmei/Zotero/storage/Y9VGCNPW/Kreyszig - Introductory
          Functional Analysis with Applications.pdf:application/pdf},
}

@misc{noauthor_ieee_nodate,
  title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=761856},
  urldate = {2018-08-16},
}

@misc{noauthor_ieee_nodate-1,
  title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1339369},
  urldate = {2018-08-16},
}

@misc{noauthor_ieee_nodate-2,
  title = {{IEEE} {Xplore} {Full}-{Text} {PDF}:},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1339369},
  urldate = {2018-08-16},
}

@inproceedings{santos_demand-reduction_2003,
  address = {Reston, VA},
  title = {Demand-{Reduction} {Input}-{Output} ({I}-{O}) {Analysis} for {
           Modeling} {Interconnectedness}},
  isbn = {978-0-7844-0694-6},
  url = {http://ascelibrary.org/doi/abs/10.1061/40694%282003%299},
  doi = {10.1061/40694(2003)9},
  urldate = {2018-07-27},
  booktitle = {Risk-{Based} {Decisionmaking} in {Water} {Resources} {X}},
  publisher = {American Society of Civil Engineers},
  author = {Santos, Joost R. and Haimes, Yacov Y.},
  month = sep,
  year = {2003},
  pages = {104--118},
}

@article{langner_stuxnet:_2011,
  title = {Stuxnet: {Dissecting} a {Cyberwarfare} {Weapon}},
  volume = {9},
  issn = {1540-7993},
  url = {http://ieeexplore.ieee.org/document/5772960/},
  doi = {10.1109/MSP.2011.67},
  number = {3},
  urldate = {2018-07-26},
  journal = {IEEE Security \& Privacy Magazine},
  author = {Langner, Ralph},
  month = may,
  year = {2011},
  pages = {49--51},
  file = {
          PDF:/home/jmei/Zotero/storage/LGUI7LLN/italian_blackout.pdf:application/pdf
          },
}

@article{chateau_gram-charlier_2017,
  title = {Gram-{Charlier} {Processes} and {Applications} to {Option} {Pricing}},
  volume = {2017},
  issn = {1687-952X},
  url = {https://www.hindawi.com/journals/jps/2017/8690491/},
  doi = {10.1155/2017/8690491},
  abstract = {{\textless}p{\textgreater}A Gram-Charlier distribution has a
              density that is a polynomial times a normal density. For option
              pricing this retains the tractability of the normal distribution
              while allowing nonzero skewness and excess kurtosis. Properties of
              the Gram-Charlier distributions are derived, leading to the
              definition of a process with independent Gram-Charlier increments,
              as well as formulas for option prices and their sensitivities. A
              procedure for simulating Gram-Charlier distributions and processes
              is given. Numerical illustrations show the effect of skewness and
              kurtosis on option prices.{\textless}/p{\textgreater}},
  urldate = {2018-07-20},
  journal = {Journal of Probability and Statistics},
  author = {Chateau, Jean-Pierre and Dufresne, Daniel},
  month = feb,
  year = {2017},
  note = {Publisher: Hindawi},
  pages = {1--19},
  file = {PDF:/home/jmei/Zotero/storage/CBMM5FFN/8690491.pdf:application/pdf},
}

@article{Matei2009,
  title = {Survival analysis for the unemployment duration},
  volume = {I},
  number = {Volume I},
  author = {Matei, Monica},
  year = {2009},
  note = {ISBN: 9789604742400},
  pages = {354--359},
  file = {
          PDF:/home/jmei/Zotero/storage/22ZM6Q8P/2009-Survival_analysis_for_the_unemployment_duration.pdf:application/pdf
          },
}

@article{Candidate2011,
  title = {E {Lectrical} {S} {Ubmersible} {P} {Ump} {S} {Urvival} a {Nalysis}},
  author = {Candidate, Masters Degree},
  year = {2011},
  file = {
          PDF:/home/jmei/Zotero/storage/RPF5DBC8/2011-E_Lectrical_S_Ubmersible_P_Ump_S_Urvival_a_Nalysis.pdf:application/pdf
          },
}

@article{Diermeier2004,
  title = {A {Political} {Economy} {Model} of {Congressional} {Careers}: {
           Supplmentary} {Material}},
  url = {http://ssrn.com/abstract=598844},
  abstract = {This paper contains additional details about the model in our
              paper "Political Economy Model of Congressional Career" (Diermeier,
              Keane and Merlo (2004)), as well as the computational methods we
              use to solve and estimate the model, and the construction of the
              data set.},
  number = {04-038},
  author = {Diermeier, Daniel and Keane, Michael and Merlo, Antonio},
  year = {2004},
  file = {
          PDF:/home/jmei/Zotero/storage/49XLP4EL/2004-A_Political_Economy_Model_of_Congressional_Careers_Supplmentary_Material.pdf:application/pdf
          },
}

@article{Investigators2007a,
  title = {The {Final} 10-{Year} {Follow}-{Up} {Results} {From} the {BARI} {
           Randomized} {Trial}},
  volume = {49},
  issn = {07351097},
  doi = {10.1016/j.jacc.2006.11.048},
  abstract = {Objectives: We sought to compare 10-year clinical outcomes in the
              BARI (Bypass Angioplasty Revascularization Investigation) trial
              patients who were randomly assigned to percutaneous transluminal
              coronary balloon angioplasty (PTCA) versus coronary artery bypass
              grafting (CABG). Background: Angioplasty and bypass surgery have
              been compared in numerous studies, but long-term clinical outcomes
              are limited. Methods: Symptomatic patients with multivessel
              coronary artery disease (n = 1,829) were randomly assigned to
              initial treatment with PTCA or CABG and followed up for an average
              of 10.4 years. Analyses were conducted on an intention-to-treat
              basis. Results: The 10-year survival was 71.0\% for PTCA and 73.5\%
              for CABG (p = 0.18). At 10 years, the PTCA group had substantially
              higher subsequent revascularization rates than the CABG group (76.8
              \% vs. 20.3\%, p {\textless} 0.001), but angina rates for the 2
              groups were similar. In the subgroup of patients with no treated
              diabetes, survival rates were nearly identical by randomization
              (PTCA 77.0\% vs. CABG 77.3\%, p = 0.59). In the subgroup with
              treated diabetes, the CABG assigned group had higher survival than
              the PTCA assigned group (PTCA 45.5\% vs. CABG 57.8\%, p = 0.025).
              Conclusions: There was no significant long-term disadvantage
              regarding mortality or myocardial infarction associated with an
              initial strategy of PTCA compared with CABG. Among patients with
              treated diabetes, CABG conferred long-term survival benefit,
              whereas the 2 initial strategies were equivalent regarding survival
              for patients without diabetes. ?? 2007 American College of
              Cardiology Foundation.},
  number = {15},
  journal = {Journal of the American College of Cardiology},
  author = {Investigators, The Bari},
  year = {2007},
  pmid = {17433949},
  note = {ISBN: 1558-3597 (Electronic){\textbackslash}r0735-1097 (Linking)},
  pages = {1600--1606},
  file = {
          PDF:/home/jmei/Zotero/storage/IVGHWBAK/2007-The_Final_10-Year_Follow-Up_Results_From_the_BARI_Randomized_Trial.pdf:application/pdf
          },
}

@article{trust_nonparametric_2016,
  title = {Nonparametric {Maximum} {Likelihood} {Estimation} of {Survival} {
           Functions} with a {General} {Stochastic} {Ordering} and {Its} {Dual}},
  volume = {76},
  number = {2},
  author = {Trust, Biometrika},
  year = {2016},
  pages = {331--341},
}

@article{Berry2009,
  title = {The {Application} of {Survival} {Analysis} to {Customer}-{Centric} {
           Forecasting}},
  journal = {NESUG - Applications Big \& Small},
  author = {Berry, Michael J A and Miners, Data},
  year = {2009},
  pages = {1--10},
  file = {
          PDF:/home/jmei/Zotero/storage/58SKUESL/2009-The_Application_of_Survival_Analysis_to_Customer-Centric_Forecasting.pdf:application/pdf
          },
}

@article{mei_additional_2021,
  title = {Additional {Document} {Copies}.pdf},
  number = {265},
  author = {Mei, Jeffrey},
  year = {2021},
  pages = {2420393},
  file = {PDF:/home/jmei/Zotero/storage/CSX8ZX64/Mei - 2021 - Additional
          Document Copies.pdf.pdf:application/pdf},
}

@article{mei_draft_2022,
  title = {Draft 1 {Proposed} {Plan} of {Graduate} {Study} and {Research}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--3},
  file = {PDF:/home/jmei/Zotero/storage/24PTGJHS/Mei - 2022 - Draft 1 Proposed
          Plan of Graduate Study and Research.pdf:application/pdf},
}

@article{mei_hw1_2022,
  title = {{HW1}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--7},
  file = {Mei - 2022 - HW1(3).pdf:/home/jmei/Zotero/storage/YAM5HYUV/Mei - 2022
          - HW1(3).pdf:application/pdf;Mei - 2022 -
          HW1(3).pdf:/home/jmei/Zotero/storage/8SQTSMU9/Mei - 2022 -
          HW1(3).pdf:application/pdf},
}

@article{mei_hw1_2022-1,
  title = {{HW1}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--7},
  file = {PDF:/home/jmei/Zotero/storage/7A3PMUB6/Mei - 2022 -
          HW1(2).pdf:application/pdf},
}

@article{mei_hw1_2022-2,
  title = {{HW1}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--7},
  file = {PDF:/home/jmei/Zotero/storage/6CRJZLKD/Mei - 2022 -
          HW1.pdf:application/pdf},
}

@article{zhao_new_2015,
  title = {New {Statistical} {Learning} {Methods} for {Estimating} {Optimal} {
           Dynamic} {Treatment} {Regimes}},
  volume = {110},
  issn = {1537274X},
  url = {/pmc/articles/PMC4517946/},
  doi = {10.1080/01621459.2014.937488},
  abstract = {Dynamic treatment regimes (DTRs) are sequential decision rules for
              individual patients that can adapt over time to an evolving
              illness. The goal is to accommodate heterogeneity among patients
              and find the DTR which will produce the best long-term outcome if
              implemented. We introduce two new statistical learning methods for
              estimating the optimal DTR, termed backward outcome weighted
              learning (BOWL), and simultaneous outcome weighted learning (SOWL).
              These approaches convert individualized treatment selection into an
              either sequential or simultaneous classification problem, and can
              thus be applied by modifying existing machine learning techniques.
              The proposed methods are based on directly maximizing over all DTRs
              a nonparametric estimator of the expected long-term outcome; this
              is fundamentally different than regression-based methods, for
              example, Q-learning, which indirectly attempt such maximization and
              rely heavily on the correctness of postulated regression models. We
              prove that the resulting rules are consistent, and provide finite
              sample bounds for the errors using the estimated rules. Simulation
              results suggest the proposed methods produce superior DTRs compared
              with Q-learning especially in small samples. We illustrate the
              methods using data from a clinical trial for smoking cessation.
              Supplementary materials for this article are available online.},
  number = {510},
  urldate = {2022-04-14},
  journal = {Journal of the American Statistical Association},
  author = {Zhao, Ying Qi and Zeng, Donglin and Laber, Eric B. and Kosorok,
            Michael R.},
  month = apr,
  year = {2015},
  pmid = {26236062},
  note = {Publisher: NIH Public Access},
  keywords = {Classification, Personalized medicine, Q-learning, Reinforcement
              learning, Risk bound, Support vector machine},
  pages = {583},
}

@article{truong_selective_2018,
  title = {Selective review of offline change point detection methods},
  volume = {167},
  url = {http://arxiv.org/abs/1801.00718},
  doi = {10.1016/j.sigpro.2019.107299},
  abstract = {This article presents a selective survey of algorithms for the
              offline detection of multiple change points in multivariate time
              series. A general yet structuring methodological strategy is
              adopted to organize this vast body of work. More precisely,
              detection algorithms considered in this review are characterized by
              three elements: a cost function, a search method and a constraint
              on the number of changes. Each of those elements is described,
              reviewed and discussed separately. Implementations of the main
              algorithms described in this article are provided within a Python
              package called ruptures.},
  urldate = {2022-06-04},
  journal = {Signal Processing},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  month = jan,
  year = {2018},
  note = {arXiv: 1801.00718v3 Publisher: Elsevier B.V.},
  keywords = {Change point detection, Segmentation, Statistical signal
              processing},
  file = {PDF:/home/jmei/Zotero/storage/PXK3H53N/full-text.pdf:application/pdf},
}

@article{niu_multiple_2016,
  title = {Multiple change-point detection: {A} selective overview},
  volume = {31},
  issn = {08834237},
  doi = {10.1214/16-STS587},
  abstract = {Very long and noisy sequence data arise from biological sciences
              to social science including high throughput data in genomics and
              stock prices in econometrics. Often such data are collected in
              order to identify and understand shifts in trends, for example,
              from a bull market to a bear market in finance or from a normal
              number of chromosome copies to an excessive number of chromosome
              copies in genetics. Thus, identifying multiple change points in a
              long, possibly very long, sequence is an important problem. In this
              article, we review both classical and new multiple change-point
              detection strategies. Considering the long history and the
              extensive literature on the change-point detection, we provide an
              in-depth discussion on a normal mean change-point model from
              aspects of regression analysis, hypothesis testing, consistency and
              inference. In particular, we present a strategy to gather and
              aggregate local information for change-point detection that has
              become the cornerstone of several emerging methods because of its
              attractiveness in both computational and theoretical properties.},
  number = {4},
  urldate = {2022-05-23},
  journal = {Statistical Science},
  author = {Niu, Yue S. and Hao, Ning and Zhang, Heping},
  year = {2016},
  note = {arXiv: 1512.04093 Publisher: Institute of Mathematical Statistics},
  keywords = {Binary segmentation, Consistency, Multiple testing, Normal mean
              change-point model, Regression, Screening and ranking algorithm},
  pages = {611--623},
  file = {PDF:/home/jmei/Zotero/storage/5YX6JKIN/full-text.pdf:application/pdf},
}

@article{lu_variable_2013,
  title = {Variable {Selection} for {Optimal} {Treatment} {Decision}},
  doi = {10.1177/0962280211428383},
  abstract = {In decision-making on optimal treatment strategies, it is of great
              importance to identify variables that are involved in the decision
              rule, i.e. those interacting with the treatment. Effective variable
              selection helps to improve the prediction accuracy and enhance the
              interpretability of the decision rule. We propose a new penalized
              regression framework which can simultaneously estimate the optimal
              treatment strategy and identify important variables. The advantages
              of the new approach include: (i) it does not require the estimation
              of the baseline mean function of the response, which greatly
              improves the robustness of the estimator; (ii) the convenient
              loss-based framework makes it easier to adopt shrinkage methods for
              variable selection, which greatly facilitates implementation and
              statistical inferences for the estimator. The new procedure can be
              easily implemented by existing state-of-art software packages like
              LARS. Theoretical properties of the new estimator are studied. Its
              empirical performance is evaluated using simulation studies and
              further illustrated with an application to an AIDS clinical trial.},
  urldate = {2021-11-14},
  author = {Lu, Wenbin and Helen Zhang, Hao and Zeng, Donglin},
  year = {2013},
  keywords = {A-learning, Optimal treatment strategy, Personalized drugs,
              Shrinkage method, Variable selection},
  file = {PDF:/home/jmei/Zotero/storage/RIARJL3I/full-text.pdf:application/pdf},
}

@article{confessore_how_2022,
  title = {How {Tucker} {Carlson} {Stoked} {White} {Fear} to {Conquer} {Cable}},
  author = {Confessore, By Nicholas},
  year = {2022},
  pages = {1--24},
  file = {PDF:/home/jmei/Zotero/storage/PV7LXJSH/Confessore - 2022 - How Tucker
          Carlson Stoked White Fear to Conquer Cable.pdf:application/pdf},
}

@article{confessore_how_2022-1,
  title = {How {Tucker} {Carlson} {Reshaped} {Fox} {News} — and {Became} {Trump}
           ʼ s {Heir}},
  author = {Confessore, By Nicholas},
  year = {2022},
  pages = {1--24},
  file = {PDF:/home/jmei/Zotero/storage/L99JFMHU/Confessore - 2022 - How Tucker
          Carlson Reshaped Fox News — and Became Trump ʼ s
          Heir.pdf:application/pdf},
}

@article{rosenbaum_central_1083,
  title = {The central role of the propensity score in observational studies for
           causal effects},
  volume = {70},
  url = {https://academic.oup.com/biomet/article/70/1/41/240879},
  abstract = {The propensity score is the conditional probability of assignment
              to a particular treatment given a vector of observed covariates.
              Both large and small sample theory show that adjustment for the
              scalar propensity score is sufficient to remove bias due to all
              observed covariates. Applications include: (i) matched sampling on
              the univariate propensity score, which is a generalization of
              discriminant matching, (ii) multivariate adjustment by
              subclassification on the propensity score where the same subclasses
              are used to estimate treatment effects for all outcome variables
              and in all subpopulations, and (iii) visual representation of
              multivariate covariance adjustment by a two-dimensional plot.},
  number = {1},
  urldate = {2022-03-15},
  author = {Rosenbaum, Paul R and Rubin, Donald B},
  year = {1083},
  keywords = {Direct adjustment, Discriminant matching, Matched sampling, Same
              hey words: Covariance adjustment, Standardization, Stratification,
              Subclassification, Xonrandomized study},
  pages = {41--55},
  file = {PDF:/home/jmei/Zotero/storage/WKWQKVN7/full-text.pdf:application/pdf},
}

@article{zhang_estimating_2012,
  title = {Estimating {Optimal} {Treatment} {Regimes} from a {Classification} {
           Perspective}},
  volume = {1},
  issn = {20491573},
  url = {/pmc/articles/PMC3640350/},
  doi = {10.1002/STA.411},
  abstract = {A treatment regime maps observed patient characteristics to a
              recommended treatment. Recent technological advances have increased
              the quality, accessibility, and volume of patient-level data;
              consequently, there is a growing need for powerful and flexible
              estimators of an optimal treatment regime that can be used with
              either observational or randomized clinical trial data. We propose
              a novel and general framework that transforms the problem of
              estimating an optimal treatment regime into a classification
              problem wherein the optimal classifier corresponds to the optimal
              treatment regime. We show that commonly employed parametric and
              semi-parametric regression estimators, as well as recently proposed
              robust estimators of an optimal treatment regime can be represented
              as special cases within our framework. Furthermore, our approach
              allows any classification procedure that can accommodate case
              weights to be used without modification to estimate an optimal
              treatment regime. This introduces a wealth of new and powerful
              learning algorithms for use in estimating treatment regimes. We
              illustrate our approach using data from a breast cancer clinical
              trial.},
  number = {1},
  urldate = {2022-04-14},
  journal = {Stat},
  author = {Zhang, Baqun and Tsiatis, Anastasios A. and Davidian, Marie and
            Zhang, Min and Laber, Eric},
  month = oct,
  year = {2012},
  pmid = {23645940},
  note = {Publisher: NIH Public Access},
  keywords = {Classification, Personalized medicine, Doubly robust estimator,
              Inverse probability weighting, Potential outcomes, Propensity score
              },
  pages = {103},
  file = {PDF:/home/jmei/Zotero/storage/EL492FN9/full-text.pdf:application/pdf},
}

@article{robins_optimal_nodate,
  title = {Optimal {Structural} {Nested} {Models} for {Optimal} {Sequential} {
           Decisions}},
  abstract = {I describe two new methods for estimating the optimal treatment
              regime (equivalently, protocol, plan or strategy) from very high
              dime-sional observational and experimental data: (i) g-estimation
              of an optimal double-regime structural nested mean model (drSNMM)
              and (ii) g-estimation of a standard single regime SNMM combined
              with sequential dynamic-programming (DP) regression. These methods
              are compared to certain regression methods found in the sequential
              decision and reinforcement learning literatures and to the regret
              modelling methods of Murphy (2003). I consider both Bayesian and
              frequentist inference. In particular, I propose a novel "
              Bayes-frequentist compromise" that combines honest subjective
              non-or semi-parametric Bayesian inference with good frequentist
              behavior, even in cases where the model is so large and the
              likelihood function so complex that standard (uncompromised) Bayes
              procedures have poor frequentist performance.},
  urldate = {2022-04-14},
  author = {Robins, James M},
  file = {PDF:/home/jmei/Zotero/storage/JSQ9CEN7/full-text.pdf:application/pdf},
}

@article{xu_regularized_2015,
  title = {Regularized {Outcome} {Weighted} {Subgroup} {Identification} for {
           Differential} {Treatment} {Effects}},
  volume = {71},
  issn = {15410420},
  url = {/pmc/articles/PMC5395466/},
  doi = {10.1111/BIOM.12322},
  abstract = {To facilitate comparative treatment selection when there is
              substantial heterogeneity of treatment effectiveness, it is
              important to identify subgroups that exhibit differential treatment
              effects. Existing approaches model outcomes directly and then
              define subgroups according to interactions between treatment and
              covariates. Because outcomes are affected by both the
              covariate-treatment interactions and covariate main effects, direct
              modeling outcomes can be hard due to model misspecification,
              especially in presence of many covariates. Alternatively one can
              directly work with differential treatment effect estimation. We
              propose such a method that approximates a target function whose
              value directly reflects correct treatment assignment for patients.
              The function uses patient outcomes as weights rather than modeling
              targets. Consequently, our method can deal with binary, continuous,
              time-to-event, and possibly contaminated outcomes in the same
              fashion. We first focus on identifying only directional estimates
              from linear rules that characterize important subgroups. We further
              consider estimation of comparative treatment effects for identified
              subgroups. We demonstrate the advantages of our method in
              simulation studies and in analyses of two real data sets.},
  number = {3},
  urldate = {2022-04-11},
  journal = {Biometrics},
  author = {Xu, Yaoyao and Yu, Menggang and Zhao, Ying Qi and Li, Quefeng and
            Wang, Sijian and Shao, Jun},
  month = sep,
  year = {2015},
  pmid = {25962845},
  note = {Publisher: NIH Public Access},
  keywords = {Variable selection, Comparative effectiveness, Heterogeneity of
              treatment effectiveness, Regularization, Subgroup},
  pages = {645},
  file = {PDF:/home/jmei/Zotero/storage/7LYDUN5N/full-text.pdf:application/pdf},
}

@article{chakraborty_inference_2013,
  title = {Inference for {Optimal} {Dynamic} {Treatment} {Regimes} using an {
           Adaptive} m-out-of-n {Bootstrap} {Scheme}},
  volume = {69},
  url = {http://cran.r-project.org/web/packages/qLearn/.},
  doi = {10.1111/biom.12052},
  abstract = {A dynamic treatment regime consists of a set of decision rules
              that dictate how to individualize treatment to patients based on
              available treatment and covariate history. A common method for
              estimating an optimal dynamic treatment regime from data is
              Q-learning which involves nonsmooth operations of the data. This
              nonsmoothness causes standard asymptotic approaches for inference
              like the bootstrap or Taylor series arguments to breakdown if
              applied without correction. Here, we consider the m-out-of-n
              bootstrap for constructing confidence intervals for the parameters
              indexing the optimal dynamic regime. We propose an adaptive choice
              of m and show that it produces asymptotically correct confidence
              sets under fixed alternatives. Furthermore, the proposed method has
              the advantage of being conceptually and computationally much more
              simple than competing methods possessing this same theoretical
              property. We provide an extensive simulation study to compare the
              proposed method with currently available inference procedures. The
              results suggest that the proposed method delivers nominal coverage
              while being less conservative than alternatives. The proposed
              methods are implemented in the qLearn R-package and have been made
              available on the Comprehensive R-Archive Network
              (http://cran.r-project.org/). Analysis of the Sequenced Treatment
              Alternatives to Relieve Depression (STAR*D) study is used as an
              illustrative example.},
  number = {3},
  urldate = {2022-04-11},
  journal = {Biometrics},
  author = {Chakraborty, Bibhas and Laber, Eric B and Zhao, Yingqi},
  year = {2013},
  keywords = {Q-learning, dynamic treatment regime, m-out-of-n bootstrap,
              non-regularity},
  file = {PDF:/home/jmei/Zotero/storage/89YDKEQ6/full-text.pdf:application/pdf},
}

@article{laber_tree-based_2015,
  title = {Tree-based methods for individualized treatment regimes},
  volume = {102},
  issn = {14643510},
  url = {/pmc/articles/PMC4755313/},
  doi = {10.1093/BIOMET/ASV028},
  abstract = {Individualized treatment rules recommend treatments on the basis
              of individual patient characteristics. A high-quality treatment
              rule can produce better patient outcomes, lower costs and less
              treatment burden. If a treatment rule learned from data is to be
              used to inform clinical practice or provide scientific insight, it
              is crucial that it be interpretable; clinicians may be unwilling to
              implement models they do not understand, and black-box models may
              not be useful for guiding future research. The canonical example of
              an interpretable prediction model is a decision tree. We propose a
              method for estimating an optimal individualized treatment rule
              within the class of rules that are representable as decision trees.
              The class of rules we consider is interpretable but expressive. A
              novel feature of this problem is that the learning task is
              unsupervised, as the optimal treatment for each patient is unknown
              and must be estimated. The proposed method applies to both
              categorical and continuous treatments and produces favourable
              marginal mean outcomes in simulation experiments. We illustrate it
              using data from a study of major depressive disorder.},
  number = {3},
  urldate = {2022-04-11},
  journal = {Biometrika},
  author = {Laber, E. B. and Zhao, Y. Q.},
  month = aug,
  year = {2015},
  pmid = {26893526},
  note = {Publisher: NIH Public Access},
  keywords = {Personalized medicine, Continuous treatment, Exploratory analysis,
              Treatment regime, Tree-based method},
  pages = {501},
  file = {PDF:/home/jmei/Zotero/storage/GQ3MWNB4/full-text.pdf:application/pdf},
}

@article{van_der_laan_super_2007,
  title = {Super {Learner}},
  volume = {6},
  abstract = {When trying to learn a model for the prediction of an outcome
              given a set of covariates, a statistician has many estimation
              procedures in their toolbox. A few examples of these candidate
              learners are: least squares, least angle regression, random forests
              , and spline regression. Previous articles (van der Laan and Dudoit
              (2003); van der Laan et al. (2006); Sinisi et al. (2007))
              theoretically validated the use of cross validation to select an
              optimal learner among many candidate learners. Motivated by this
              use of cross validation, we propose a new prediction method for
              creating a weighted combination of many candidate learners to build
              the super learner. This article proposes a fast algorithm for
              constructing a super learner in prediction which uses V-fold
              cross-validation to select weights to combine an initial set of
              candidate learners. In addition, this paper contains a practical
              demonstration of the adaptivity of this so called super learner to
              various true data generating distributions. This approach for
              construction of a super learner generalizes to any parameter which
              can be defined as a minimizer of a loss function.},
  number = {1},
  urldate = {2022-04-08},
  journal = {Statistical Applications in Genetics and Molecular Biology},
  author = {Van Der Laan, Mark J and Polley, Eric C and Hubbard, Alan E},
  year = {2007},
  keywords = {cross-validation, loss-based estimation, machine learning,
              prediction},
  file = {PDF:/home/jmei/Zotero/storage/CA93W5IF/full-text.pdf:application/pdf},
}

@article{luedtke_statistical_nodate,
  title = {{STATISTICAL} {INFERENCE} {FOR} {THE} {MEAN} {OUTCOME} {UNDER} {A} {
           POSSIBLY} {NON}-{UNIQUE} {OPTIMAL} {TREATMENT} {STRATEGY}},
  doi = {10.1214/15-AOS1384SUPP},
  abstract = {We consider challenges that arise in the estimation of the mean
              outcome under an optimal individualized treatment strategy defined
              as the treatment rule that maximizes the population mean outcome,
              where the candidate treatment rules are restricted to depend on
              baseline covariates. We prove a necessary and sufficient condition
              for the pathwise differentiability of the optimal value, a key
              condition needed to develop a regular and asymptotically linear
              (RAL) estimator of the optimal value. The stated condition is
              slightly more general than the previous condition implied in the
              literature. We then describe an approach to obtain root-n rate
              confidence intervals for the optimal value even when the parameter
              is not pathwise differentiable. We provide conditions under which
              our estimator is RAL and asymptotically efficient when the mean
              outcome is pathwise differentiable. We also outline an extension of
              our approach to a multiple time point problem. All of our results
              are supported by simulations.},
  urldate = {2022-04-08},
  author = {Luedtke, Alexander R and Van Der Laan, Mark J},
  keywords = {AMS 2000 subject classifications Primary 62G05, non-regular
              inference, online estimation, optimal treatment, optimal value,
              pathwise differentiability, secondary 62N99 Key words and phrases
              Efficient estimator, semi parametric model},
  file = {PDF:/home/jmei/Zotero/storage/758K4AY9/full-text.pdf:application/pdf},
}

@article{kessler_machine_2019,
  title = {Machine learning methods for developing precision treatment rules
           with observational data {HHS} {Public} {Access}},
  volume = {120},
  doi = {10.1016/j.brat.2019.103412},
  abstract = {Clinical trials have identified a variety of predictor variables
              for use in precision treatment protocols, ranging from clinical
              biomarkers and symptom profiles to self-report measures of various
              sorts. Although such variables are informative collectively, none
              has proven sufficiently powerful to guide optimal treatment
              selection individually. This has prompted growing interest in the
              development of composite precision treatment rules (PTRs) that are
              constructed by combining information across a range of predictors.
              But this work has been hampered by the generally small samples in
              randomized clinical trials and the use of suboptimal analysis
              methods to analyze the resulting data. In this paper, we propose to
              address the sample size problem by: working with large
              observational electronic medical record databases rather than
              controlled clinical trials to develop preliminary PTRs; validating
              these preliminary PTRs in subsequent pragmatic trials; and using
              ensemble machine learning methods rather than individual algorithms
              to carry out statistical analyses to develop the PTRs. The major
              challenges in this proposed approach are that treatment are not
              randomly assigned in observational databases and that these
              databases often lack measures of key prescriptive predictors and
              mental disorder treatment outcomes. We proposed a tiered case},
  urldate = {2022-04-07},
  journal = {Behav Res Ther},
  author = {Kessler, Ronald C and Bossarte, Robert M and Luedtke, Alex and
            Zaslavsky, Alan M and Zubizarreta, Jose R},
  year = {2019},
  pages = {103412},
  file = {PDF:/home/jmei/Zotero/storage/7P5HGS57/full-text.pdf:application/pdf},
}

@article{zhao_recent_2012,
  title = {Recent development on statistical methods for personalized medicine
           discovery},
  doi = {10.1007/s11684-013-0245-7},
  abstract = {It is well documented that patients can show significant
              heterogeneous responses to treatments so the best treatment
              strategies may require adaptation over individuals and time.
              Recently, a number of new statistical methods have been developed
              to tackle the important problem of estimating personalized
              treatment rules using single-stage or multiple-stage clinical data.
              In this paper, we provide an overview of these methods and list a
              number of challenges.},
  urldate = {2022-01-23},
  author = {Zhao, Yingqi and Zeng, Donglin},
  year = {2012},
  note = {ISBN: 1168401302457},
  keywords = {Q-learning, dynamic treatment regimes, personalized medicine,
              reinforcement learning},
  file = {PDF:/home/jmei/Zotero/storage/9GU5QMFH/full-text.pdf:application/pdf},
}

@article{xiao_robust_2019,
  title = {Robust {Regression} for {Optimal} {Individualized} {Treatment} {Rules
           }},
  doi = {10.1002/sim.8102},
  abstract = {Because different patients may respond quite differently to the
              same drug or treatment, there is increasing interest in discovering
              individualized treatment rules. In particular, there is an emerging
              need to find optimal individualized treatment rules which would
              lead to the "best" clinical outcome. In this paper, we propose a
              new class of loss functions and estimators based on robust
              regression to estimate the optimal individualized treatment rules.
              Compared to existing estimation methods in the literature, the new
              estimators are novel and advantageous in the following aspects:
              first, they are robust against skewed, heterogeneous, heavy-tailed
              errors or outliers in data; second, they are robust against a
              misspecification of the baseline function; third, under some
              general situations, the new estimator coupled with the pinball loss
              approximately maximizes the outcome's conditional quantile instead
              of the conditional mean, which leads to a more robust optimal
              individualized treatment rule than traditional mean-based
              estimators. Consistency and asymptotic normality of the proposed
              estimators are established. Their empirical performance is
              demonstrated via extensive simulation studies and an analysis of an
              AIDS data set.},
  urldate = {2022-01-24},
  author = {Xiao, W and Zhang, H H and Lu, W},
  year = {2019},
  keywords = {Personalized medicine, Optimal individualized treatment rules,
              Quantile regression, Robust regression},
  file = {PDF:/home/jmei/Zotero/storage/IFVBZ49F/full-text.pdf:application/pdf},
}

@article{song_semiparametric_nodate,
  title = {Semiparametric {Single}-{Index} {Model} for {Estimating} {Optimal} {
           Individualized} {Treatment} {Strategy}},
  doi = {10.1214/17-EJS1226},
  abstract = {Different from the standard treatment discovery framework which is
              used for finding single treatments for a homogenous group of
              patients, personalized medicine involves finding therapies that are
              tailored to each individual in a heterogeneous group. In this paper
              , we propose a new semiparametric additive single-index model for
              estimating individualized treatment strategy. The model assumes a
              flexible and nonparametric link function for the interaction
              between treatment and predictive covariates. We estimate the rule
              via monotone B-splines and establish the asymptotic properties of
              the estimators. Both simulations and an real data application
              demonstrate that the proposed method has a competitive performance.
              },
  urldate = {2022-01-24},
  author = {Song, Rui and Luo, Shikai and Zeng, Donglin and Helen Zhang, Hao and
            Lu, Wenbin and Li, Zhiguo},
  keywords = {Personalized medicine, Semiparametric inference, Single index
              model},
}

@article{geng_optimal_2010,
  title = {On {Optimal} {Treatment} {Regimes} {Selection} for {Mean} {Survival}
           {Time}},
  doi = {10.1002/sim.6397},
  abstract = {In clinical studies with time-to-event as a primary endpoint, one
              main interest is to find the best treatment strategy to maximize
              patients' mean survival time. Due to patient's heterogeneity in
              response to treatments, great efforts have been devoted to
              developing optimal treatment regimes by integrating individuals'
              clinical and genetic information. A main challenge arises in the
              selection of important variables that can help to build reliable
              and interpretable optimal treatment regimes since the dimension of
              predictors may be high. In this paper, we propose a robust
              loss-based estimation framework that can be easily coupled with
              shrinkage penalties for both estimation of optimal treatment
              regimes and variable selection. The asymptotic properties of the
              proposed estimators are studied. Moreover, a model-free estimator
              of restricted mean survival time under the derived optimal
              treatment regime is developed and its asymptotic property is
              studied. Simulations are conducted to assess the empirical
              performance of the proposed method for parameter estimation,
              variable selection, and optimal treatment decision. An application
              to an AIDS clinical trial data set is given to illustrate the
              method.},
  urldate = {2022-01-24},
  author = {Geng, Yuan and Helen Zhang, Hao and Lu, Wenbin},
  year = {2010},
  keywords = {Adaptive LASSO, censored regression, mean survival time, optimal
              treatment regime, variable selection},
  file = {PDF:/home/jmei/Zotero/storage/F8IR7KYH/full-text.pdf:application/pdf},
}

@article{murphy_optimal_2003,
  title = {Optimal dynamic treatment regimes},
  volume = {65},
  issn = {13697412},
  doi = {10.1111/1467-9868.00389},
  abstract = {A dynamic treatment regime is a list of decision rules, one per
              time interval, for how the level of treatment will be tailored
              through time to an individual's changing status. The goal of this
              paper is to use experimental or observational data to estimate
              decision regimes that result in a maximal mean response. To
              explicate our objective and to state the assumptions, we use the
              potential outcomes model. The method proposed makes smooth
              parametric assumptions only on quantities that are directly
              relevant to the goal of estimating the optimal rules. We illustrate
              the methodology proposed via a small simulation.},
  number = {2},
  urldate = {2022-03-07},
  journal = {Journal of the Royal Statistical Society. Series B: Statistical
             Methodology},
  author = {Murphy, S. A.},
  year = {2003},
  note = {Publisher: Blackwell Publishing Ltd},
  keywords = {Adaptive strategies, Causal inference, Dynamic programming,
              Multistage decisions},
  pages = {331--355},
  file = {PDF:/home/jmei/Zotero/storage/D9CJ5PCE/full-text.pdf:application/pdf},
}

@article{watkins_technical_1992,
  title = {Technical {Note} {Q},-{Learning}},
  volume = {8},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how
              to act optimally in controlled Markovian domains. It amounts to an
              incremental method for dynamic programming which imposes limited
              computational demands. It works by successively improving its
              evaluations of the quality of particular actions at particular
              states. This paper presents and proves in detail a convergence
              theorem for Q,-learning based on that outlined in Watkins (1989).
              We show that Q-learning converges to the optimum action-values with
              probability 1 so long as all actions are repeatedly sampled in all
              states and the action-values are represented discretely. We also
              sketch extensions to the cases of non-discounted, but absorbing,
              Markov environments, and where many Q values can be changed each
              iteration, rather than just one.},
  urldate = {2022-03-03},
  author = {Watkins, Christopher J C H and Dayan, Peter},
  year = {1992},
  keywords = {Q-learning, reinforcement learning, asynchronous dynamic
              programming, temporal differences},
  pages = {279--292},
  file = {PDF:/home/jmei/Zotero/storage/KVSPNNY7/full-text.pdf:application/pdf},
}

@article{moodie_demystifying_2003,
  title = {Demystifying {Optimal} {Dynamic} {Treatment} {Regimes}},
  volume = {63},
  doi = {10.1111/j.1541-0420.2006.00686.x},
  abstract = {A dynamic regime is a function that takes treatment and covariate
              history and baseline co-variates as inputs and returns a decision
              to be made.326) have proposed models and developed semiparametric
              methods for making inference about the optimal regime in a
              multi-interval trial that provide clear advantages over traditional
              parametric approaches. We show that Murphy's model is a special
              case of Robins's and that the methods are closely related but not
              equivalent. Interesting features of the methods are highlighted
              using the Multicenter AIDS Cohort Study and through simulation.},
  urldate = {2022-03-09},
  journal = {Journal of the Royal Statistical Society, Series B},
  author = {Moodie, Erica E M and Richardson, Thomas S and Stephens, David A},
  year = {2003},
  keywords = {Optimal dynamic regimes, Optimal structural nested mean models,
              Randomized controlled trials, Sequential randomization, Treatment
              algorithms},
  pages = {189},
  file = {PDF:/home/jmei/Zotero/storage/CHU28TUW/full-text.pdf:application/pdf},
}

@article{zhao_new_2015-1,
  title = {New {Statistical} {Learning} {Methods} for {Estimating} {Optimal} {
           Dynamic} {Treatment} {Regimes} {HHS} {Public} {Access}},
  volume = {110},
  doi = {10.1080/01621459.2014.937488},
  abstract = {Dynamic treatment regimes (DTRs) are sequential decision rules for
              individual patients that can adapt over time to an evolving
              illness. The goal is to accommodate heterogeneity among patients
              and find the DTR which will produce the best long term outcome if
              implemented. We introduce two new statistical learning methods for
              estimating the optimal DTR, termed backward outcome weighted
              learning (BOWL), and simultaneous outcome weighted learning (SOWL).
              These approaches convert individualized treatment selection into an
              either sequential or simultaneous classification problem, and can
              thus be applied by modifying existing machine learning techniques.
              The proposed methods are based on directly maximizing over all DTRs
              a nonparametric estimator of the expected long-term outcome; this
              is fundamentally different than regression-based methods, for
              example Q-learning, which indirectly attempt such maximization and
              rely heavily on the correctness of postulated regression models. We
              prove that the resulting rules are consistent, and provide finite
              sample bounds for the errors using the estimated rules. Simulation
              results suggest the proposed methods produce superior DTRs compared
              with Q-learning especially in small samples. We illustrate the
              methods using data from a clinical trial for smoking cessation.},
  number = {510},
  urldate = {2022-03-19},
  journal = {J Am Stat Assoc},
  author = {Zhao, Ying-Qi and Zeng, Donglin and Laber, Eric B and Kosorok,
            Michael R},
  year = {2015},
  keywords = {Classification, Personalized medicine, Q-learning, Reinforcement
              learning, Support vector machine, Dynamic treatment regimes, Risk
              Bound},
  pages = {583--598},
  file = {PDF:/home/jmei/Zotero/storage/CDGDM2Z8/full-text.pdf:application/pdf},
}

@article{qian_performance_2011,
  title = {Performance guarantees for individualized treatment rules},
  volume = {39},
  doi = {10.1214/10-AOS864},
  abstract = {Because many illnesses show heterogeneous response to treatment,
              there is increasing interest in individualizing treatment to
              patients [11]. An individualized treatment rule is a decision rule
              that recommends treatment according to patient characteristics. We
              consider the use of clinical trial data in the construction of an
              individualized treatment rule leading to highest mean response.
              This is a difficult computational problem because the objective
              function is the expectation of a weighted indicator function that
              is non-concave in the parameters. Furthermore there are frequently
              many pretreatment variables that may or may not be useful in
              constructing an optimal individualized treatment rule yet cost and
              interpretability considerations imply that only a few variables
              should be used by the individualized treatment rule. To address
              these challenges we consider estimation based on l1 penalized least
              squares. This approach is justified via a finite sample upper bound
              on the difference between the mean response due to the estimated
              individualized treatment rule and the mean response due to the
              optimal individualized treatment rule.},
  number = {2},
  urldate = {2022-03-26},
  journal = {The Annals of Statistics},
  author = {Qian, Min and Murphy, Susan A.},
  month = may,
  year = {2011},
  note = {Publisher: Institute of Mathematical Statistics},
  file = {PDF:/home/jmei/Zotero/storage/T5ZP7BPR/full-text.pdf:application/pdf},
}

@article{zhao_estimating_nodate,
  title = {Estimating {Individualized} {Treatment} {Rules} {Using} {Outcome} {
           Weighted} {Learning}},
  doi = {10.1080/01621459.2012.695674},
  abstract = {There is increasing interest in discovering individualized
              treatment rules for patients who have heterogeneous responses to
              treatment. In particular, one aims to find an optimal
              individualized treatment rule which is a deterministic function of
              patient specific characteristics maximizing expected clinical
              outcome. In this paper, we first show that estimating such an
              optimal treatment rule is equivalent to a classification problem
              where each subject is weighted proportional to his or her clinical
              outcome. We then propose an outcome weighted learning approach
              based on the support vector machine framework. We show that the
              resulting estimator of the treatment rule is consistent. We further
              obtain a finite sample bound for the difference between the
              expected outcome using the estimated individualized treatment rule
              and that of the optimal treatment rule. The performance of the
              proposed approach is demonstrated via simulation studies and an
              analysis of chronic depression data.},
  urldate = {2022-03-24},
  author = {Zhao, Yingqi and Zeng, Donglin and Kosorok, Michael R},
  keywords = {Risk Bound, Bayes Classifier, Cross Validation, Dynamic Treatment
              Regime, Individualized Treatment Rule, RKHS, Weighted Support
              Vector Machine},
  file = {PDF:/home/jmei/Zotero/storage/A5XQ59DR/full-text.pdf:application/pdf},
}

@article{moodie_precision_2020,
  title = {Precision medicine: {Statistical} methods for estimating adaptive
           treatment strategies},
  volume = {55},
  url = {https://doi.org/10.1038/s41409-020-0871-z},
  doi = {10.1038/s41409-020-0871-z},
  urldate = {2022-04-01},
  journal = {Bone Marrow Transplantation},
  author = {Moodie, Erica E M and Krakow, Elizabeth F},
  year = {2020},
  pages = {1890--1896},
  file = {PDF:/home/jmei/Zotero/storage/TAMIE8DE/full-text.pdf:application/pdf},
}

@article{luedtke_evaluating_2016,
  title = {Evaluating {Optimal} {Individualized} {Treatment} {Rules}},
  urldate = {2022-03-30},
  author = {Luedtke, Alexander Ryan},
  year = {2016},
  file = {PDF:/home/jmei/Zotero/storage/5YB4G6PN/full-text.pdf:application/pdf},
}

@article{liu_use_2014,
  title = {Use of personalized {Dynamic} {Treatment} {Regimes} ({DTRs}) and {
           Sequential} {Multiple} {Assignment} {Randomized} {Trials} ({SMARTs})
           in mental health studies},
  volume = {26},
  issn = {10020829},
  url = {/pmc/articles/PMC4311115/},
  doi = {10.11919/J.ISSN.1002-0829.214172},
  abstract = {Summary: Dynamic treatment regimens (DTRs) are sequential decision
              rules tailored at each point where a clinical decision is made
              based on each patient's time-varying characteristics and
              intermediate outcomes observed at earlier points in time. The
              complexity, patient heterogeneity, and chronicity of mental
              disorders call for learning optimal DTRs to dynamically adapt
              treatment to an individual's response over time. The Sequential
              Multiple Assignment Randomized Trial (SMARTs) design allows for
              estimating causal effects of DTRs. Modern statistical tools have
              been developed to optimize DTRs based on personalized variables and
              intermediate outcomes using rich data collected from SMARTs; these
              statistical methods can also be used to recommend tailoring
              variables for designing future SMART studies. This paper introduces
              DTRs and SMARTs using two examples in mental health studies,
              discusses two machine learning methods for estimating optimal DTR
              from SMARTs data, and demonstrates the performance of the
              statistical methods using simulated data.},
  number = {6},
  urldate = {2022-04-03},
  journal = {Shanghai Archives of Psychiatry},
  author = {Liu, Ying and Zeng, Donglin and Wang, Yuanjia},
  month = dec,
  year = {2014},
  pmid = {25642116},
  note = {Publisher: Shanghai Mental Health Center},
  keywords = {Personalized medicine, Q-learning, Dynamic treatment regimes,
              Double robust estimation, O-learning, SMART},
  pages = {376},
  file = {PDF:/home/jmei/Zotero/storage/W6RMD5VP/full-text.pdf:application/pdf},
}

@article{cohen_incorporating_2013,
  title = {Incorporating multidimensional patient-reported outcomes of symptom
           severity, functioning, and quality of life in the individual burden of
           illness index for depression to measure treatment impact and recovery
           in {MDD}},
  volume = {70},
  issn = {2168622X},
  doi = {10.1001/JAMAPSYCHIATRY.2013.286},
  abstract = {Context: The National Institute of Mental Health Affective
              Disorders Workgroup identified the assessment of an individual's
              burden of illness as an important need. The Individual Burden of
              Illness Index for Depression (IBI-D) metric was developed to meet
              this need. Objective: To assess the use of the IBI-D for
              multidimensional assessment of treatment efficacy for depressed
              patients. Design, Setting, and Patients: Complete data on
              depressive symptom severity, functioning, and quality of life (QOL)
              from depressed patients (N=2280) at entry and exit of level 1 of
              the Sequenced Treatment Alternatives to Relieve Depression (STAR*D)
              study (12-week citalopram treatment) were used as the basis for
              calculating IBI-D and self-rating scale changes. Results: Principal
              component analysis of patient responses at the end of level 1 of
              STAR*D yielded a single principal component, IBI-D, with a nearly
              identical eigenvector to that previously reported. While changes in
              symptom severity (Quick Inventory of Depressive Symptomatology-
              Self Report) accounted for only 50\% of the variance in changes in
              QOL (Quality of Life Enjoyment and Satisfaction Questionnaire-Short
              Form ) and 47\% of the variance in changes in functioning (Work and
              Social Adjustment Scale ), changes in IBI-D captured 83\% of the
              variance in changes in QOL and 80\% in functioning, while also
              capturing 79\% of the variance in change in symptom severity (Quick
              Inventory of Depressive Symptomatology-Self Report). Most
              importantly, the changes in IBI-D of the 36.6\% of remitters who
              had abnormalQOLand/ or functioning (mean [SD], 2.98 [0.35]) were
              significantly less than the changes in IBI-D of those who reported
              normal QOL and functioning (IBID= 1.97; t=32.6; P{\textless}10$^{
              \textrm{8}}$) with an effect size of a Cohen d of 2.58. In contrast
              , differences in symptom severity, while significant, had a Cohen d
              of only 0.78. Conclusions: Remission in depressed patients, as
              defined by a reduction in symptom severity, does not denote normal
              QOL or functioning. By incorporating multidimensional
              patient-reported outcomes, the IBI-D provides a single measure that
              adequately captures the full burden of illness in depression both
              prior to and following treatment; therefore, it offers a more
              accurate metric of recovery. © 2013 American Medical Association.
              All rights reserved.},
  number = {3},
  urldate = {2022-04-04},
  journal = {JAMA Psychiatry},
  author = {Cohen, Robert M. and Greenberg, Jared M. and IsHak, Waguih William},
  year = {2013},
  note = {Publisher: American Medical Association},
  pages = {343--350},
  file = {PDF:/home/jmei/Zotero/storage/XSEICFKU/full-text.pdf:application/pdf},
}

@article{collins_multiphase_2007,
  title = {The {Multiphase} {Optimization} {Strategy} ({MOST}) and the {
           Sequential} {Multiple} {Assignment} {Randomized} {Trial} ({SMART}): {
           New} {Methods} for {More} {Potent} {eHealth} {Interventions}},
  volume = {32},
  abstract = {In this article two new methods for building and evaluating
              e-health interventions are described. The first is the Multiphase
              Optimization Strategy (MOST). MOST consists of a screening phase,
              in which intervention components are efficiently identified for
              selection for inclusion in an intervention or rejection, based on
              their performance; a refining phase, in which the selected
              components are fine-tuned, and questions such as optimal component
              dosage are investigated; and a confirming phase, in which the
              optimized intervention, consisting of optimal doses of the selected
              components, is evaluated in a standard randomized confirmatory
              trial. The second is the Sequential Multiple Assignment Randomized
              Trial (SMART) which is an innovative research design especially
              suited for building time-varying adaptive interventions. A SMART
              trial can be used to identify the best tailoring variables and
              decision rules for an adaptive intervention empirically. Both the
              MOST and SMART approaches use randomized experimentation to enable
              valid inferences. When properly implemented, these approaches will
              lead to the development of more potent e-health interventions.
              There are good reasons to believe that interventions based on
              e-health principles have the potential for considerable public
              health impact. Perhaps the most obvious reason is the reach of
              these interventions. Once an electronic intervention has been
              designed and programmed, delivery occurs via methods such as the
              Internet or by mailing a CD, and therefore is extremely convenient.
              Moreover, the incremental cost of delivering an intervention to
              additional people is usually negligible, certainly in comparison to
              traditional interventions where in order to reach more recipients
              it becomes necessary to add additional physicians, therapists,
              health educators, peer counselors, and so on to deliver the
              program. The limiting factor for reach of an e-intervention is less
              likely to be a shortage of resources for delivering the program
              electronically than access to computers on the part of potential
              recipients. However, access to computers continues to increase in
              all strata of American society, suggesting that e-health
              interventions hold growing promise. 1},
  number = {5},
  urldate = {2022-04-04},
  journal = {Am J Prev Med},
  author = {Collins, Linda M and Murphy, Susan A and Strecher, Victor},
  year = {2007},
  pages = {112--118},
  file = {PDF:/home/jmei/Zotero/storage/8PHDE8SH/full-text.pdf:application/pdf},
}

@article{lavori_adaptive_2008,
  title = {Adaptive {Treatment} {Strategies} in {Chronic} {Disease}},
  volume = {59},
  issn = {00664219},
  url = {/pmc/articles/PMC2739674/},
  doi = {10.1146/ANNUREV.MED.59.062606.122232},
  abstract = {An adaptive treatment strategy (ATS) is a rule for adapting a
              treatment plan to a patient's history of previous treatments and
              the response to those treatments. The ongoing management of chronic
              disease defines an ATS, which may be implicit and hidden or
              explicit and well-specified. The ATS is characterized by the use of
              intermediate, early markers of response to dynamically alter
              treatment decisions, in order to achieve a favorable ultimate
              outcome. We illustrate the ATS concept and describe how the effect
              of initial treatment decisions depends on the performance of
              subsequent decisions at later stages. We show how to compare two or
              more ATSs, or to determine an optimal ATS, using a sequential
              multiple assignment randomized (SMAR) trial. Designers of clinical
              trials might find the ATS concept useful in improving the
              efficiency and ecological relevance of clinical trials. Copyright ©
              2008 by Annual Reviews. All rights reserved.},
  urldate = {2022-04-04},
  journal = {Annual review of medicine},
  author = {Lavori, Philip W. and Dawson, Ree},
  year = {2008},
  pmid = {17914924},
  note = {Publisher: NIH Public Access},
  keywords = {Dynamic treatment, Treatment policy},
  pages = {443},
  file = {PDF:/home/jmei/Zotero/storage/JAWIH7GY/full-text.pdf:application/pdf},
}

@article{fernandes_new_2017,
  title = {The new field of 'precision psychiatry'},
  volume = {15},
  issn = {17417015},
  doi = {10.1186/S12916-017-0849-X},
  abstract = {Background: Precision medicine is a new and important topic in
              psychiatry. Psychiatry has not yet benefited from the advanced
              diagnostic and therapeutic technologies that form an integral part
              of other clinical specialties. Thus, the vision of precision
              medicine as applied to psychiatry - 'precision psychiatry' -
              promises to be even more transformative than in other fields of
              medicine, which have already lessened the translational gap.
              Discussion: Herein, we describe 'precision psychiatry' and how its
              several implications promise to transform the psychiatric
              landscape. We pay particular attention to biomarkers and to how the
              development of new technologies now makes their discovery possible
              and timely. The adoption of the term 'precision psychiatry' will
              help propel the field, since the current term 'precision medicine',
              as applied to psychiatry, is impractical and does not appropriately
              distinguish the field. Naming the field 'precision psychiatry' will
              help establish a stronger, unique identity to what promises to be
              the most important area in psychiatry in years to come. Conclusion:
              In summary, we provide a wide-angle lens overview of what this new
              field is, suggest how to propel the field forward, and provide a
              vision of the near future, with 'precision psychiatry' representing
              a paradigm shift that promises to change the landscape of how
              psychiatry is currently conceived.},
  number = {1},
  urldate = {2022-04-06},
  journal = {BMC Medicine},
  author = {Fernandes, Brisa S. and Williams, Leanne M. and Steiner, Johann and
            Leboyer, Marion and Carvalho, André F. and Berk, Michael},
  month = apr,
  year = {2017},
  pmid = {28403846},
  note = {Publisher: BioMed Central Ltd.},
  keywords = {Big data, Biomarkers, Omics, Personalised medicine, Precision
              medicine, Precision psychiatry, Research domain criteria, Systems
              biology},
  file = {PDF:/home/jmei/Zotero/storage/Z62SI9QG/full-text.pdf:application/pdf},
}

@article{yang_model-free_2016,
  title = {Model-free {Variable} {Selection} in {Reproducing} {Kernel} {Hilbert}
           {Space}},
  volume = {17},
  abstract = {Variable selection is popular in high-dimensional data analysis to
              identify the truly informative variables. Many variable selection
              methods have been developed under various model assumptions.
              Whereas success has been widely reported in literature, their
              performances largely depend on validity of the assumed models, such
              as the linear or additive models. This article introduces a
              model-free variable selection method via learning the gradient
              functions. The idea is based on the equivalence between whether a
              variable is informative and whether its corresponding gradient
              function is substantially non-zero. The proposed variable selection
              method is then formulated in a framework of learning gradients in a
              flexible reproducing kernel Hilbert space. The key advantage of the
              proposed method is that it requires no explicit model assumption
              and allows for general variable effects. Its asymptotic estimation
              and selection consistencies are studied, which establish the
              convergence rate of the estimated sparse gradients and assure that
              the truly informative variables are correctly identified in
              probability. The effectiveness of the proposed method is also
              supported by a variety of simulated examples and two real-life
              examples.},
  urldate = {2022-03-28},
  journal = {Journal of Machine Learning Research},
  author = {Yang, Lei and Wang, Junhui},
  year = {2016},
  keywords = {variable selection, group Lasso, high-dimensional data, kernel
              regression, learning gradients, reproducing kernel Hilbert space
              (RKHS)},
  pages = {1--24},
  file = {PDF:/home/jmei/Zotero/storage/P78IMNRW/full-text.pdf:application/pdf},
}

@article{heckerman_bayesian_nodate,
  title = {A {Bayesian} {Approach} to {Learning} {Causal} {Networks}},
  abstract = {Whereas acausal Bayesian networks represent probabilistic
              independence, causal Bayesian networks represent causal
              relationships. In this paper, we examine Bayesian methods for
              learning both types of networks. Bayesian methods for learning
              acausal networks are fairly well developed. These methods often
              employ assumptions to facilitate the construction of priors,
              including the assumptions of parameter independence, parameter
              modularity, and likelihood equivalence. We show that although these
              assumptions also can be appropriate for learning causal networks,
              we need additional assumptions in order to learn causal networks.
              We introduce two sufficient assumptions, called mechanism
              independence and component independence. We show that these new
              assumptions , when combined with parameter independence, parameter
              modularity, and likelihood equivalence, allow us to apply methods
              for learning acausal networks to learn causal networks.},
  urldate = {2022-03-16},
  author = {Heckerman, David},
  file = {PDF:/home/jmei/Zotero/storage/C2AW7WZ6/full-text.pdf:application/pdf},
}

@article{zou_adaptive_2006,
  title = {The {Adaptive} {Lasso} and {Its} {Oracle} {Properties}},
  doi = {10.1198/016214506000000735},
  abstract = {The lasso is a popular technique for simultaneous estimation and
              variable selection. Lasso variable selection has been shown to be
              consistent under certain conditions. In this work we derive a
              necessary condition for the lasso variable selection to be
              consistent. Consequently, there exist certain scenarios where the
              lasso is inconsistent for variable selection. We then propose a new
              version of the lasso, called the adaptive lasso, where adaptive
              weights are used for penalizing different coefficients in the 1
              penalty. We show that the adaptive lasso enjoys the oracle
              properties; namely, it performs as well as if the true underlying
              model were given in advance. Similar to the lasso, the adaptive
              lasso is shown to be near-minimax optimal. Furthermore, the
              adaptive lasso can be solved by the same efficient algorithm for
              solving the lasso. We also discuss the extension of the adaptive
              lasso in generalized linear models and show that the oracle
              properties still hold under mild regularity conditions. As a
              byproduct of our theory, the nonnegative garotte is shown to be
              consistent for variable selection.},
  urldate = {2022-01-26},
  author = {Zou, Hui},
  year = {2006},
  keywords = {Lasso, Variable selection, Asymptotic normality, Minimax, Oracle
              inequality, Oracle procedure},
  file = {PDF:/home/jmei/Zotero/storage/6Y6N8UZS/full-text.pdf:application/pdf},
}

@article{fan_statistical_2006,
  title = {Statistical challenges with high dimensionality: {Feature} selection
           in knowledge discovery},
  volume = {3},
  doi = {10.4171/022-3/31},
  abstract = {Technological innovations have revolutionized the process of
              scientific research and knowledge discovery. The availability of
              massive data and challenges from frontiers of research and
              development have reshaped statistical thinking, data analysis and
              theoretical studies. The challenges of high-dimensionality arise in
              diverse fields of sciences and the humanities, ranging from
              computational biology and health studies to financial engineering
              and risk management. In all of these fields, variable selection and
              feature extraction are crucial for knowledge discovery. We first
              give a comprehensive overview of statistical challenges with high
              dimensionality in these diverse disciplines. We then approach the
              problem of variable selection and feature extraction using a
              unified framework: penalized likelihood methods. Issues relevant to
              the choice of penalty functions are addressed. We demonstrate that
              for a host of statistical problems, as long as the dimensionality
              is not excessively large, we can estimate the model parameters as
              well as if the best model is known in advance. The persistence
              property in risk minimization is also addressed. The applicability
              of such a theory and method to diverse statistical problems is
              demonstrated. Other related problems with high-dimensionality are
              also discussed. © 2006 European Mathematical Society.},
  journal = {International Congress of Mathematicians, ICM 2006},
  author = {Fan, Jianqing and Li, Runze},
  year = {2006},
  note = {arXiv: math/0602133},
  keywords = {AIC, BIC, Bioinformatics, Financial econometrics, LASSO, Model
              selection, Oracle property, Penalized likelihood, Persistent, SCAD,
              Statistical learning},
  pages = {595--622},
  file = {PDF:/home/jmei/Zotero/storage/JWY3YXNC/Fan, Li - 2006 - Statistical
          challenges with high dimensionality Feature selection in knowledge
          discovery.pdf:application/pdf},
}

@article{hastie_statistical_2016,
  title = {Statistical {Learning} with {Sparsity} {Monographs} on {Statistics}
           and {Applied} {Probability} 143 143 copy to come from copywriter for
           review},
  abstract = {Statistics K25103 w w w . c r c p r e s s . c o m K25103\_
              cover.indd 1 2/24/15 1:35 PM},
  author = {Hastie, Trevor and Tibshirani, Robert and Hastie, Martin Wainwright
            and Tibshirani, @bullet and Wainwright, @bullet},
  year = {2016},
  pages = {362},
  file = {PDF:/home/jmei/Zotero/storage/9P9QP86Q/Hastie et al. - 2016 -
          Statistical Learning with Sparsity Monographs on Statistics and Applied
          Probability 143 143 copy to come from cop.pdf:application/pdf},
}

@article{andersen_coxs_1982,
  title = {Cox's {Regression} {Model} for {Counting} {Processes}: {A} {Large} {
           Sample} {Study}},
  volume = {10},
  issn = {0090-5364},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-10/issue-4/Coxs-Regression-Model-for-Counting-Processes--A-Large-Sample/10.1214/aos/1176345976.full
         },
  doi = {10.1214/AOS/1176345976},
  abstract = {The Cox regression model for censored survival data specifies that
              covariates have a proportional effect on the hazard function of the
              life-time distribution of an individual. In this paper we discuss
              how this model can be extended to a model where covariate processes
              have a proportional effect on the intensity process of a
              multivariate counting process. This permits a statistical
              regression analysis of the intensity of a recurrent event allowing
              for complicated censoring patterns and time dependent covariates.
              Furthermore, this formulation gives rise to proofs with very simple
              structure using martingale techniques for the asymptotic properties
              of the estimators from such a model. Finally an example of a
              statistical analysis is included.},
  number = {4},
  urldate = {2022-01-29},
  journal = {https://doi.org/10.1214/aos/1176345976},
  author = {Andersen, P. K. and Gill, R. D.},
  month = dec,
  year = {1982},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {62F12, 62G05, 62M99, 62P10, Censoring, intensity, martingale,
              Survival analysis, time dependent covariates},
  pages = {1100--1120},
  file = {PDF:/home/jmei/Zotero/storage/FJ83ZUL8/full-text.pdf:application/pdf},
}

@article{knight_asymptotics_2000,
  title = {Asymptotics for {Lasso}-type estimators},
  volume = {28},
  issn = {00905364},
  doi = {10.1214/aos/1015957397},
  abstract = {We consider the asymptotic behavior of regression estimators that
              minimize the residual sum of squares plus a penalty proportional to
              Σ {\textbackslash}βj{\textbar}γ for some γ {\textgreater} 0. These
              estimators include the Lasso as a special case when γ = 1. Under
              appropriate conditions, we show that the limiting distributions can
              have positive probability mass at 0 when the true value of the
              parameter is 0. We also consider asymptotics for "nearly singular"
              designs.},
  number = {5},
  journal = {Annals of Statistics},
  author = {Knight, Keith and Fu, Wenjiang},
  year = {2000},
  keywords = {Lasso, Epi-convergence in distribution, Penalized regression,
              Shrinkage estimation},
  pages = {1356--1378},
  file = {PDF:/home/jmei/Zotero/storage/6KJHMEGJ/Knight, Fu - 2000 - Asymptotics
          for Lasso-type estimators.pdf:application/pdf},
}

@article{rubin_estimating_1974,
  title = {{ESTIMATING} {CAUSAL} {EFFECTS} {OF} {TREATMENTS} {IN} {RANDOMIZED} {
           AND} {NONRANDOMIZED} {STUDIES} 1},
  volume = {66},
  number = {5},
  urldate = {2022-01-23},
  journal = {Journal of Educational Psychology},
  author = {Rubin, Donald B},
  year = {1974},
  pages = {688--701},
  file = {PDF:/home/jmei/Zotero/storage/H5A9Q2ZL/full-text.pdf:application/pdf},
}

@article{ibrahim_fixed_2011,
  title = {Fixed and {Random} {Effects} {Selection} in {Mixed} {Effects} {Models
           }},
  volume = {67},
  doi = {10.1111/j.1541-0420.2010.01463.x},
  abstract = {We consider selecting both fixed and random effects in a general
              class of mixed effects models using maximum penalized likelihood
              (MPL) estimation along with the smoothly clipped absolute deviation
              (SCAD) and adaptive least absolute shrinkage and selection operator
              (ALASSO) penalty functions. The MPL estimates are shown to possess
              consistency and sparsity properties and asymptotic normality. A
              model selection criterion, called the IC Q statistic, is proposed
              for selecting the penalty parameters (Ibrahim, Zhu, and Tang, 2008,
              Journal of the American Statistical Association 103, 1648-1658).
              The variable selection procedure based on IC Q is shown to
              consistently select important fixed and random effects. The
              methodology is very general and can be applied to numerous
              situations involving random effects, including generalized linear
              mixed models. Simulation studies and a real data set from a Yale
              infant growth study are used to illustrate the proposed
              methodology.},
  urldate = {2021-11-14},
  journal = {Biometrics},
  author = {Ibrahim, Joseph G and Zhu, Hongtu and Garcia, Ramon I and Guo,
            Ruixin},
  year = {2011},
  keywords = {Penalized likelihood, SCAD, ALASSO, Cholesky decomposition, EM
              algorithm, IC Q criterion, Mixed effects selection},
  pages = {495--503},
  file = {PDF:/home/jmei/Zotero/storage/WMD2UIZU/full-text.pdf:application/pdf},
}

@misc{noauthor_asa_nodate,
  title = {The {ASA} president's task force statement on statistical
           significance and replicability - {AOAS2106}-{007RA0}.pdf},
  file = {PDF:/home/jmei/Zotero/storage/3P4LVUDT/Unknown - Unknown - The ASA
          president's task force statement on statistical significance and
          replicability - AOAS2106-007RA0.pdf.pdf:application/pdf},
}

@misc{noauthor_modelselection_quadregressionpdf_nodate,
  title = {{ModelSelection}\_QuadRegression.pdf},
  file = {PDF:/home/jmei/Zotero/storage/Y86A69K7/Unknown - Unknown -
          ModelSelection_QuadRegression.pdf.pdf:application/pdf},
}

@techreport{Lee2016,
  title = {Analysis of the {Cyber} {Attack} on the {Ukrainian} {Power} {Grid} {
           Defense} {Use} {Case}},
  urldate = {2018-07-24},
  institution = {E-ISAC},
  author = {Lee, Robert M. (SANS) and Assante, Michael J. (SANS) and Conway, Tim
            (SANS)},
  year = {2016},
  file = {
          PDF:/home/jmei/Zotero/storage/MU85F27T/m-api-7497126b-cb20-04db-3b82-bee9e6440c0e.pdf:application/pdf
          },
}

@article{Borkowska1974,
  title = {Probabilistic {Load} {Flow}},
  volume = {PAS-93},
  issn = {0018-9510},
  doi = {10.1109/TPAS.1974.293973},
  abstract = {The paper describes a method for evaluation of power flow which
              takes into consideration uncertainty of node data. The essence of
              the method is that the net loads are given as a set of values
              together with additional information on the frequency of its
              accuracy. The described mathematical model and the practtical
              application are discussed and an example given.},
  number = {3},
  journal = {IEEE Transactions on Power Apparatus and Systems},
  author = {Borkowska, B.},
  year = {1974},
  note = {ISBN: 0018-9510},
  pages = {1--6},
  file = {PDF:/home/jmei/Zotero/storage/EKTUDQXZ/PLF.pdf:application/pdf},
}

@article{Billinton1999,
  title = {Teaching {Distribution} {System} {Reliability} {Evaluation} {Using} {
           Monte} {Carlo} {Simulation}},
  volume = {14},
  issn = {08858950},
  doi = {10.1109/59.761856},
  abstract = {Analytical techniques for distribution system reliability
              assessment can be effectively used to evaluate the mean values of a
              wide range of system reliability indices. This approach is usually
              used when teaching the basic oncepts of distribution system
              reliability evaluation. The mean or expected value, however, does
              not provide any information on the inherent variability of an
              index. Appreciation of this inherent variability is an important
              parameter in comprehending the actual reliability experienced by a
              customer and should be recognized when teaching distribution system
              reliability evaluation. This paper presents a time sequential Monte
              Carlo simulation technique which can be used in complex
              distribution system evaluation, and describes a computer program
              developed to implement this technique. General distribution system
              elements, operating models and radial configurations are considered
              in the program. The results obtained using both analytical and
              simulation methods are compared. The mean values and the
              probability distributions for both load point and system indices
              are illustrated using a practical test system.},
  number = {2},
  journal = {IEEES Transactions on Power Systems},
  author = {Billinton, Roy and Wang, Pen},
  year = {1999},
  note = {ISBN: 0885-8950},
  keywords = {monte carlo, power distribution system, reliability},
  pages = {397--403},
  file = {
          PDF:/home/jmei/Zotero/storage/TATJF3HK/reliability_monte_carlo.pdf:application/pdf
          },
}

@article{Duenas-Osorio,
  title = {Interdependent {Response} of {Networked} {Systems}},
  volume = {13},
  issn = {1076-0342},
  url = {
         http://ascelibrary.org/doi/10.1061/%28ASCE%291076-0342%282007%2913%3A3%28185%29
         },
  doi = {10.1061/(ASCE)1076-0342(2007)13:3(185)},
  abstract = {Continuous functionality of critical infrastructure systems is
              essential to support the social and economic organization of
              productive sectors within a country. Electric power, potable water,
              natural gas, telecommunications, and transportation are examples of
              these critical systems, whose nature makes them suitable for
              network analysis. This study presents the topological
              characterization of two interdependent small-sized real networks.
              The same properties are calculated for ideal models of comparable
              size to the real networks. Selected topological properties are
              monitored for these interdependent systems when subjected to
              external or internal disruptions e.g., deliberate attacks,
              malfunction due to aging, or lack of maintenance . This study
              introduces a simple rule to establish interdependencies among
              network elements based upon geographical proximity. The effect of
              the degree of coupling between networks is investigated with a
              tunable parameter that drives the networks from independence to
              complete interdependence. Network detrimental responses are ob-
              served to be larger when interdependencies are considered after
              disturbances. Effective mitigation actions could take advantage of
              the same network interconnectedness that facilitates cascading
              failures.},
  number = {3},
  urldate = {2018-07-19},
  journal = {Journal of Infrastructure Systems},
  author = {Dueñas-Osorio, Leonardo and Craig, James I and Goodno, Barry J. and
            Bostrom, Ann},
  year = {2007},
  note = {ISBN: IS/2005/022470},
  keywords = {Networks, CE Database subject headings: Infrastructure, Risk
              management, Simulation models},
  pages = {185--194},
  file = {
          PDF:/home/jmei/Zotero/storage/6SGE8G2E/m-api-4d5aa1aa-a14b-e915-12d2-4edf1a2ea5fe.pdf:application/pdf
          },
}

@article{KLEINMAN1985,
  title = {{THE} {EFFECTS} {OF} {MATERNAL} {SMOKING}, {PHYSICAL} {STATURE}, {AND
           } {EDUCATIONAL} {ATTAINMENT} {ON} {THE} {INCIDENCE} {OF} {LOW} {BIRTH}
           {WEIGHT}},
  volume = {121},
  issn = {1476-6256},
  url = {https://academic.oup.com/aje/article/92300/THE},
  doi = {10.1093/oxfordjournals.aje.a114055},
  number = {6},
  urldate = {2017-05-01},
  journal = {American Journal of Epidemiology},
  author = {KLEINMAN, JOEL C. and MADANS, JENNIFER H.},
  month = jun,
  year = {1985},
  note = {Publisher: Oxford University Press},
  keywords = {cigarettes, educational status, hispanics or latinos, live birth,
              low birth weight infant, marriage, life event, maternal age,
              mothers, pregnancy, smoke, smoking, smoking in pregnancy, telephone
              },
  pages = {843--855},
  file = {PDF:/home/jmei/Zotero/storage/USEUJ3SH/full-text.pdf:application/pdf},
}

@article{obrien_statistical_2016,
  title = {Statistical {Learning} with {Sparsity}: {The} {Lasso} and {
           Generalizations}},
  volume = {84},
  doi = {10.1111/insr.12167},
  abstract = {Discover New Methods for Dealing with High-Dimensional Data A
              sparse statistical model has only a small number of nonzero
              parameters or weights; therefore, it is much easier to estimate and
              interpret than a dense model. Statistical Learning with Sparsity:
              The Lasso and Generalizations presents methods that exploit
              sparsity to help recover the underlying signal in a set of data.
              Top experts in this rapidly evolving field, the authors describe
              the lasso for linear regression and a simple coordinate descent
              algorithm for its computation. They discuss the application of l1
              penalties to generalized linear models and support vector machines,
              cover generalized penalties such as the elastic net and group lasso
              , and review numerical methods for optimization. They also present
              statistical inference methods for fitted (lasso) models, including
              the bootstrap, Bayesian methods, and recently developed approaches.
              In addition, the book examines matrix decomposition, sparse
              multivariate analysis, graphical models, and compressed sensing. It
              concludes with a survey of theoretical results for the lasso. In
              this age of big data, the number of features measured on a person
              or object can be large and might be larger than the number of
              observations. This book shows how the sparsity assumption allows us
              to tackle these problems and extract useful and reproducible
              patterns from big datasets. Data analysts, computer scientists, and
              theorists will appreciate this thorough and up-to-date treatment of
              sparse statistical modeling.},
  number = {1},
  journal = {International Statistical Review},
  author = {O'Brien, Carl M.},
  year = {2016},
  pages = {156--157},
  file = {PDF:/home/jmei/Zotero/storage/7HRCSQMJ/O'Brien - 2016 - Statistical
          Learning with Sparsity The Lasso and
          Generalizations.pdf:application/pdf},
}

@article{friedman_pathwise_2007,
  title = {Pathwise coordinate optimization},
  volume = {1},
  issn = {1941-7330},
  doi = {10.1214/07-aoas131},
  abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms
              for a class of convex optimization problems. An algorithm of this
              kind has been proposed for the \$L\_1\$-penalized regression
              (lasso) in the literature, but it seems to have been largely
              ignored. Indeed, it seems that coordinate-wise algorithms are not
              often used in convex optimization. We show that this algorithm is
              very competitive with the well-known LARS (or homotopy) procedure
              in large lasso problems, and that it can be applied to related
              methods such as the garotte and elastic net. It turns out that
              coordinate-wise descent does not work in the ``fused lasso,''
              however, so we derive a generalized algorithm that yields the
              solution in much less time that a standard convex optimizer.
              Finally, we generalize the procedure to the two-dimensional fused
              lasso, and demonstrate its performance on some image smoothing
              problems.},
  number = {2},
  journal = {The Annals of Applied Statistics},
  author = {Friedman, Jerome and Hastie, Trevor and Höfling, Holger and
            Tibshirani, Robert},
  year = {2007},
  note = {arXiv: 0708.1485},
  keywords = {and phrases, convex optimization, coordinate descent, lasso},
  pages = {302--332},
  file = {PDF:/home/jmei/Zotero/storage/A3VZ9ERD/Friedman et al. - 2007 -
          Pathwise coordinate optimization.pdf:application/pdf},
}

@article{friedman_regularization_2010,
  title = {Regularization {Paths} for {Generalized} {Linear} {Models} via {
           Coordinate} {Descent}},
  volume = {33},
  issn = {0014-4886},
  abstract = {We develop fast algorithms for estimation of generalized linear
              models with convex penalties. The models include linear regression,
              two-class logistic regression, and multi- nomial regression
              problems while the penalties include ?1 (the lasso), ?2 (ridge
              regression) and mixtures of the two (the elastic net). The
              algorithms use cyclical coordinate descent, computed along a
              regularization path. The methods can handle large problems and can
              also deal efficiently with sparse features. In comparative timings
              we find that the new algorithms are considerably faster than
              competing methods. Keywords:},
  number = {1},
  journal = {Journal of Statistical Software},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  pmid = {18291371},
  note = {arXiv: 0908.3817 ISBN: 0387329072},
  keywords = {lasso, elastic net, logistic regression},
  pages = {1--22},
  file = {PDF:/home/jmei/Zotero/storage/5JK3XAEE/Geostatistics, Diggle, 間違っている．
          - 2010 - Regularization Paths for Generalized Linear Models via
          Coordinate Descent.pdf:application/pdf},
}

@article{david_ideal_1994,
  title = {Ideal spatial adaptation by wavelet shrinkage},
  volume = {81},
  abstract = {SUMMARY With ideal spatial adaptation , an oracle furnishes
              information about how best to adapt a spatially variable estimator,
              whether piecewise constant, piecewise polynomial, variable knot
              spline, or variable bandwidth kernel, to the unknown function.
              Estimation ...},
  number = {3},
  journal = {Biometrika},
  author = {David, Donoho and Jain, Johnstone},
  year = {1994},
  keywords = {Bases of, Compact, Knot, Orthogonal, Piecewise-, Polynomial
              lttingg, Spline, Supportt, Variable-, Wavelet},
  pages = {425--455},
  file = {
          PDF:/home/jmei/Zotero/storage/CPFXIV8E/m-api-14e0f237-0887-dfd9-3aa8-14a25db38c52.pdf:application/pdf
          },
}

@article{wainwright_sharp_2009,
  title = {Sharp thresholds for high-dimensional and noisy sparsity recovery
           using ℓ1-constrained quadratic programming ({Lasso})},
  volume = {55},
  issn = {00189448},
  doi = {10.1109/TIT.2009.2016018},
  abstract = {The problem of consistently estimating the sparsity pattern of a
              vector β* ∈ ℝp based on observations contaminated by noise arises
              in various contexts, including signal denoising, sparse
              approximation, compressed sensing, and model selection. We analyze
              the behavior of ℓ1-constrained quadratic programming (QP), also
              referred to as the Lasso, for recovering the sparsity pattern. Our
              main result is to establish precise conditions on the problem
              dimension p, the number k of nonzero elements in β*, and the number
              of observations n that are necessary and sufficient for sparsity
              pattern recovery using the Lasso. We first analyze the case of
              observations made using deterministic design matrices and
              sub-Gaussian additive noise, and provide sufficient conditions for
              support recovery and ℓ∞-error bounds, as well as results showing
              the necessity of incoherence and bounds on the minimum value. We
              then turn to the case of random designs, in which each row of the
              design is drawn from a N(0, Σ) ensemble. For a broad class of
              Gaussian ensembles satisfying mutual incoherence conditions, we
              compute explicit values of thresholds 0 {\textless} θℓ(Σ) ≤ θu (Σ)
              {\textless} + ∞ with the following properties: for any δ {
              \textgreater} 0 {\textgreater}, if n {\textgreater} 2 (θu + δ)k log
              (p - k), then the Lasso succeeds in recovering the sparsity pattern
              with probability converging to one for large problems, whereas for
              n {\textless} 2(θℓ - δ)k log (p - k), then the probability of
              successful recovery converges to zero. For the special case of the
              uniform Gaussian ensemble (Σ = Ip×p), we show that (θ ℓ) = θu = 1,
              so that the precise threshold n = 2k log (p - k) is exactly
              determined. © 2009 IEEE.},
  number = {5},
  journal = {IEEE Transactions on Information Theory},
  author = {Wainwright, Martin J.},
  year = {2009},
  keywords = {Model selection, Compressed sensing, Convex relaxation,
              High-dimensional inference, ℓ1-constraints, Phase transitions,
              Signal denoising, Sparse approximation, Subset selection},
  pages = {2183--2202},
  file = {PDF:/home/jmei/Zotero/storage/APWEE2YV/Wainwright - 2009 - Sharp
          thresholds for high-dimensional and noisy sparsity recovery using
          ℓ1-constrained quadratic programming (Lass.pdf:application/pdf},
}

@article{efron_bootstrap_1986,
  title = {Bootstrap {Methods} for {Standard} {Errors}, {Confidence} {Intervals}
           , and {Other} {Measures} of {Statistical} {Accuracy}},
  volume = {1},
  issn = {0883-4237},
  url = {http://projecteuclid.org/euclid.ss/1177013815},
  doi = {10.1214/ss/1177013815},
  abstract = {This is a review of bootstrap methods, concentrating on basic
              ideas and applications rather than theoretical considerations. It
              begins with an exposition of the bootstrap estimate of standard
              error for one-sample situations. Several examples, some involving
              quite complicated statistical procedures, are given. The bootstrap
              is then extended to other measures of statistical accuracy such as
              bias and prediction error, and to complicated data structures such
              as time series, censored data, and regression models. Several more
              examples are presented illustrating these ideas. The last third of
              the paper deals mainly with bootstrap confidence intervals. © 1986,
              Statistical Science. All rights reserved.},
  number = {1},
  urldate = {2021-03-19},
  journal = {Statistical Science},
  author = {Efron, B. and Tibshirani, R.},
  month = feb,
  year = {1986},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {Approximate confidence intervals, Bootstrap method, Estimated
              standard errors, Nonparametric methods},
  pages = {54--75},
  file = {PDF:/home/jmei/Zotero/storage/UMTC7LS5/full-text.pdf:application/pdf},
}

@article{rochon_test_2012,
  title = {To test or not to test: {Preliminary} assessment of normality when
           comparing two independent samples},
  volume = {12},
  issn = {14712288},
  url = {http://www.biomedcentral.com/1471-2288/12/81},
  doi = {10.1186/1471-2288-12-81},
  abstract = {Background: Students two-sample t test is generally used for
              comparing the means of two independent samples, for example, two
              treatment arms. Under the null hypothesis, the t test assumes that
              the two samples arise from the same normally distributed population
              with unknown variance. Adequate control of the Type I error
              requires that the normality assumption holds, which is often
              examined by means of a preliminary Shapiro-Wilk test. The following
              two-stage procedure is widely accepted: If the preliminary test for
              normality is not significant, the t test is used; if the
              preliminary test rejects the null hypothesis of normality, a
              nonparametric test is applied in the main analysis. Methods:
              Equally sized samples were drawn from exponential, uniform, and
              normal distributions. The two-sample t test was conducted if either
              both samples (Strategy I) or the collapsed set of residuals from
              both samples (Strategy II) had passed the preliminary Shapiro-Wilk
              test for normality; otherwise, Mann-Whitneys U test was conducted.
              By simulation, we separately estimated the conditional Type I error
              probabilities for the parametric and nonparametric part of the
              two-stage procedure. Finally, we assessed the overall Type I error
              rate and the power of the two-stage procedure as a whole. Results:
              Preliminary testing for normality seriously altered the conditional
              Type I error rates of the subsequent main analysis for both
              parametric and nonparametric tests. We discuss possible
              explanations for the observed results, the most important one being
              the selection mechanism due to the preliminary test. Interestingly,
              the overall Type I error rate and power of the entire two-stage
              procedure remained within acceptable limits. Conclusion: The
              two-stage procedure might be considered incorrect from a formal
              perspective; nevertheless, in the investigated examples, this
              procedure seemed to satisfactorily maintain the nominal
              significance level and had acceptable power properties. © 2012
              Rochon et al.; licensee BioMed Central Ltd.},
  number = {1},
  urldate = {2021-07-03},
  journal = {BMC Medical Research Methodology},
  author = {Rochon, Justine and Gondan, Matthias and Kieser, Meinhard},
  month = jun,
  year = {2012},
  pmid = {22712852},
  note = {Publisher: BioMed Central},
  keywords = {Mann-Whitneys U test, Studentst test, Testing for normality},
  pages = {1--11},
  file = {PDF:/home/jmei/Zotero/storage/Q8UZUVFJ/full-text.pdf:application/pdf},
}

@article{hao_model_2015,
  title = {Model {Selection} for {High} {Dimensional} {Quadratic} {Regression}
           via {Regularization} {arXiv} : 1501 . 00049v1 [ stat . {ME} ] 31 {Dec}
           2014},
  volume = {85721},
  author = {Hao, Ning and Zhang, Hao Helen},
  year = {2015},
  note = {arXiv: 1501.00049v1},
  keywords = {variable selection, lasso, generalized quadratic models, high
              dimensional data, interaction selection, marginality principle},
  pages = {1--33},
  file = {PDF:/home/jmei/Zotero/storage/VL4REHNK/Hao, Zhang - 2015 - Model
          Selection for High Dimensional Quadratic Regression via Regularization
          arXiv 1501 . 00049v1 stat . ME 31 De.pdf:application/pdf},
}

@misc{noauthor_svm_nodate,
  title = {{SVM} {Tutorial}.pdf},
  file = {PDF:/home/jmei/Zotero/storage/RAQF87E6/Unknown - Unknown - SVM
          Tutorial.pdf.pdf:application/pdf},
}

@misc{noauthor_schizophrenia_selectionpdf_nodate,
  title = {Schizophrenia\_Selection.pdf},
  file = {PDF:/home/jmei/Zotero/storage/2DQN2YUF/Unknown - Unknown -
          Schizophrenia_Selection.pdf.pdf:application/pdf},
}

@misc{noauthor_rmt4lssvm-icassppdf_nodate,
  title = {{RMT4LSSVM}-{ICASSP}.pdf},
  file = {PDF:/home/jmei/Zotero/storage/UHBL2V9L/Unknown - Unknown -
          RMT4LSSVM-ICASSP.pdf.pdf:application/pdf},
}

@misc{noauthor_plant_machinelearningpdf_nodate,
  title = {Plant\_MachineLearning.pdf},
  file = {PDF:/home/jmei/Zotero/storage/WQPYQL44/Unknown - Unknown -
          Plant_MachineLearning.pdf.pdf:application/pdf},
}

@misc{noauthor_neuroscience_statisticspdf_nodate,
  title = {Neuroscience\_Statistics.pdf},
  file = {PDF:/home/jmei/Zotero/storage/4T5QV3NZ/Unknown - Unknown -
          Neuroscience_Statistics.pdf.pdf:application/pdf},
}

@misc{noauthor_insample_errorpdf_nodate,
  title = {insample\_error.pdf},
  file = {PDF:/home/jmei/Zotero/storage/GIHN2942/Unknown - Unknown -
          insample_error.pdf.pdf:application/pdf},
}

@misc{noauthor_elements_of_statistical_learning_soln_manualpdf_nodate,
  title = {Elements\_of\_Statistical\_Learning\_Soln\_manual.pdf},
  file = {PDF:/home/jmei/Zotero/storage/DJFJVCDP/Unknown - Unknown -
          Elements_of_Statistical_Learning_Soln_manual.pdf.pdf:application/pdf},
}

@article{lockhart_mathematicians_2008,
  title = {A {Mathematician}’s {Lament} ({Lockhart}'s {Lament})},
  abstract = {This month's column is devoted to an article called A
              Mathematician's Lament, written by Paul Lockhart in 2002. Paul is a
              mathematics teacher at Saint Ann's School in Brooklyn, New York.
              His article has been circulating through parts of the mathematics
              and math ed communities ever since, but he never published it. I
              came across it by accident a few months ago, and decided at once I
              wanted to give it wider exposure. I contacted Paul, and he agreed
              to have me publish his "lament" on MAA Online. It is, quite frankly
              , one of the best critiques of current K-12 mathematics education I
              have ever seen. Written by a first-class research mathematician who
              elected to devote his teaching career to K-!2 education. Paul
              became interested in mathematics when he was about 14 (outside of
              the school math class, he points out) and read voraciously,
              becoming especially interested in analytic number theory. He
              dropped out of college after one semester to devote himself to math
              , supporting himself by working as a computer programmer and as an
              elementary school teacher. Eventually he started working with Ernst
              Strauss at UCLA, and the two published a few papers together.
              Strauss introduced him to Paul Erdos, and they somehow arranged it
              so that he became a graduate student there. He ended up getting a
              Ph.D. from Columbia in 1990, and went on to be a fellow at MSRI and
              an assistant professor at Brown. He also taught at UC Santa Cruz.
              His main research interests were, and are, automorphic forms and
              Diophantine geometry. After several years teaching university
              mathematics, Paul eventually tired of it and decided he wanted to
              get back to teaching children. He secured a position at Saint Ann's
              School, where he says "I have happily been subversively teaching
              mathematics (the real thing) since 2000." He teaches all grade
              levels at Saint Ann's (K-12), and says he is especially interested
              in bringing a mathematician's point of view to very young children.
              "I want them to understand that there is a playground in their
              minds and that that is where mathematics happens. So far I have met
              with tremendous enthusiasm among the parents and kids, less so
              among the mid-level administrators," he wrote in an email to me.
              Now where have I heard that kind of thing before? But enough of my
              words. Read Paul's dynamite essay. It's a 25-page PDF file.},
  number = {March 2008},
  journal = {MAA Online},
  author = {Lockhart, Paul},
  year = {2008},
  pages = {25},
  file = {
          PDF:/home/jmei/Zotero/storage/RBH5NQKG/m-api-7fd6c026-5d06-c6ff-17f6-7598ab8f1772.pdf:application/pdf
          },
}

@article{michener_ecoinformatics_2012,
  title = {Ecoinformatics: {Supporting} ecology as a data-intensive science},
  volume = {27},
  issn = {01695347},
  doi = {10.1016/j.tree.2011.11.016},
  abstract = {Ecology is evolving rapidly and increasingly changing into a more
              open, accountable, interdisciplinary, collaborative and
              data-intensive science. Discovering, integrating and analyzing
              massive amounts of heterogeneous data are central to ecology as
              researchers address complex questions at scales from the gene to
              the biosphere. Ecoinformatics offers tools and approaches for
              managing ecological data and transforming the data into information
              and knowledge. Here, we review the state-of-the-art and recent
              advances in ecoinformatics that can benefit ecologists and
              environmental scientists as they tackle increasingly challenging
              questions that require voluminous amounts of data across
              disciplines and scales of space and time. We also highlight the
              challenges and opportunities that remain. © 2011 Elsevier Ltd.},
  number = {2},
  journal = {Trends in Ecology and Evolution},
  author = {Michener, William K. and Jones, Matthew B.},
  year = {2012},
  pmid = {22240191},
  pages = {85--93},
  file = {PDF:/home/jmei/Zotero/storage/T5FB5NF5/Michener, Jones - 2012 -
          Ecoinformatics Supporting ecology as a data-intensive
          science.pdf:application/pdf},
}

@misc{noauthor_tutorialcorrdvi_nodate,
  title = {{tutorialCorr}.dvi - svmtutorial.pdf},
  file = {PDF:/home/jmei/Zotero/storage/JYEMCW3M/Unknown - Unknown -
          tutorialCorr.dvi - svmtutorial.pdf.pdf:application/pdf},
}

@article{burges_tutorial_1997,
  title = {A {Tutorial} on {Support} {Vector} {Machines} for {Pattern} {
           Recognition}},
  volume = {43},
  author = {Burges, Christopher J C},
  year = {1997},
  keywords = {121-167, 1998, appeared in, data mining and knowledge, discovery 2
              , pattern recognition, statistical learning theory, support vector
              machines, vc dimension},
  pages = {1--43},
  file = {PDF:/home/jmei/Zotero/storage/URLNS66M/Burges - 1997 - A Tutorial on
          Support Vector Machines for Pattern Recognition.pdf:application/pdf},
}

@article{kaliyappan_microarray_2012,
  title = {Microarray and its applications},
  volume = {4},
  issn = {0975-7406},
  url = {/pmc/articles/PMC3467903/},
  doi = {10.4103/0975-7406.100283},
  abstract = {Microarray is one of the most recent advances being used for
              cancer research; it provides assistance in pharmacological approach
              to treat various diseases including oral lesions. Microarray helps
              in analyzing large amount of samples which have either been
              recorded previously or new samples; it even helps to test the
              incidence of a particular marker in tumors. Till recently,
              microarray's usage in dentistry has been very limited, but in
              future, as the technology becomes affordable, there may be increase
              in its usage. Here, we discuss the various techniques and
              applications of microarray or DNA chip.},
  number = {6},
  urldate = {2021-06-12},
  journal = {Journal of Pharmacy and Bioallied Sciences},
  author = {Kaliyappan, Karunakaran and Palanisamy, Murugesan and Govindarajan,
            Rajeshwar and Duraiyan, Jeyapradha},
  year = {2012},
  note = {Publisher: Medknow},
  pages = {310},
}

@techreport{paul_random_2013,
  title = {Random {Projections} for {Support} {Vector} {Machines}},
  abstract = {Let X be a data matrix of rank ρ, representing n points in
              d-dimensional space. The linear support vector machine constructs a
              hyperplane separator that maximizes the 1-norm soft margin. We
              develop a new oblivious dimension reduction technique which is
              precomputed and can be applied to any input matrix X. We prove that
              , with high probability , the margin and minimum enclosing ball in
              the feature space are preserved to within-relative error, ensuring
              comparable generalization as in the original space. We present
              extensive experiments with real and synthetic data to support our
              theory.},
  urldate = {2021-04-23},
  author = {Paul, Saurabh and Boutsidis, Christos and Magdon-Ismail, Malik and
            Drineas, Petros},
  year = {2013},
  file = {PDF:/home/jmei/Zotero/storage/U5X537QB/full-text.pdf:application/pdf},
}

@techreport{jakkula_tutorial_nodate,
  title = {Tutorial on {Support} {Vector} {Machine} ({SVM})},
  abstract = {In this tutorial we present a brief introduction to SVM, and we
              discuss about SVM from published papers, workshop materials \&
              material collected from books and material available online on the
              World Wide Web. In the beginning we try to define SVM and try to
              talk as why SVM, with a brief overview of statistical learning
              theory. The mathematical formulation of SVM is presented, and
              theory for the implementation of SVM is briefly discussed. Finally
              some conclusions on SVM and application areas are included. Support
              Vector Machines (SVMs) are competing with Neural Networks as tools
              for solving pattern recognition problems. This tutorial assumes you
              are familiar with concepts of Linear Algebra, real analysis and
              also understand the working of neural networks and have some
              background in AI.},
  urldate = {2021-04-16},
  author = {Jakkula, Vikramaditya},
  file = {PDF:/home/jmei/Zotero/storage/SQK66EY9/full-text.pdf:application/pdf},
}

@article{howard_sleep_2005,
  title = {Sleep {Deprivation} and {Physician} {Performance}: {Why} {Should} {I}
           {Care}?},
  volume = {18},
  url = {
         https://www.tandfonline.com/action/journalInformation?journalCode=ubmc20
         },
  doi = {10.1080/08998280.2005.11928045},
  number = {2},
  author = {Howard, Steven K},
  year = {2005},
  pages = {108--112},
  file = {PDF:/home/jmei/Zotero/storage/AEH7TLS7/full-text.pdf:application/pdf},
}

@techreport{noauthor_backpropagation_nodate,
  title = {The {Backpropagation} {Algorithm} 7.1 {Learning} as gradient descent},
  abstract = {We saw in the last chapter that multilayered networks are capable
              of computing a wider range of Boolean functions than networks with
              a single layer of computing units. However the computational effort
              needed for finding the correct combination of weights increases
              substantially when more parameters and more complicated topologies
              are considered. In this chapter we discuss a popular learning
              method capable of handling such large learning problems-the
              backpropagation algorithm. This numerical method was used by
              different research communities in different contexts, was
              discovered and rediscovered, until in 1985 it found its way into
              connectionist AI mainly through the work of the PDP group [382]. It
              has been one of the most studied and used algorithms for neural
              networks learning ever since. In this chapter we present a proof of
              the backpropagation algorithm based on a graphical approach in
              which the algorithm reduces to a graph labeling problem. This
              method is not only more general than the usual analytical
              derivations, which handle only the case of special network
              topologies, but also much easier to follow. It also shows how the
              algorithm can be efficiently implemented in computing systems in
              which only local information can be transported through the
              network. 7.1.1 Differentiable activation functions The
              backpropagation algorithm looks for the minimum of the error
              function in weight space using the method of gradient descent. The
              combination of weights which minimizes the error function is
              considered to be a solution of the learning problem. Since this
              method requires computation of the gradient of the error function
              at each iteration step, we must guarantee the continuity and
              differentiability of the error function. Obviously we have to use a
              kind of activation function other than the step function used in
              perceptrons,},
  urldate = {2021-04-10},
  file = {PDF:/home/jmei/Zotero/storage/LUS3RCI5/full-text.pdf:application/pdf},
}

@techreport{noauthor_noble_nodate,
  title = {The {Noble} {Eightfold} {Path} to {Linear} {Regression}},
  urldate = {2021-04-10},
  file = {PDF:/home/jmei/Zotero/storage/2I5LRFLD/full-text.pdf:application/pdf},
}

@article{jollife_principal_2016,
  title = {Principal component analysis: {A} review and recent developments},
  volume = {374},
  issn = {1364503X},
  url = {http://dx.doi.org/10.1098/rsta.2015.0202},
  doi = {10.1098/rsta.2015.0202},
  abstract = {Large datasets are increasingly common and are often difficult to
              interpret. Principal component analysis (PCA) is a technique for
              reducing the dimensionality of such datasets, increasing
              interpretability but at the same time minimizing information loss.
              It does so by creating new uncorrelated variables that successively
              maximize variance. Finding such new variables, the principal
              components, reduces to solving an eigenvalue/eigenvector problem,
              and the new variables are defined by the dataset at hand, not a
              priori, hence making PCA an adaptive data analysis technique. It is
              adaptive in another sense too, since variants of the technique have
              been developed that are tailored to various different data types
              and structures. This article will begin by introducing the basic
              ideas of PCA, discussing what it can and cannot do. It will then
              describe some variants of PCA and their application.},
  number = {2065},
  urldate = {2021-04-03},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical,
             Physical and Engineering Sciences},
  author = {Jollife, Ian T. and Cadima, Jorge},
  month = apr,
  year = {2016},
  note = {Publisher: Royal Society of London},
  keywords = {Dimension reduction, Eigenvectors, Multivariate analysis,
              Palaeontology},
  file = {PDF:/home/jmei/Zotero/storage/VPE5HVUC/full-text.pdf:application/pdf},
}

@article{lonnberg_growth_2020,
  title = {The growth, spread, and mutation of internet phenomena: {A} study of
           memes},
  volume = {6},
  issn = {25900374},
  url = {https://doi.org/10.1016/j.rinam.2020.100092},
  doi = {10.1016/j.rinam.2020.100092},
  abstract = {An internet memes is defined as “an image, video, piece of text,
              etc., typically humorous in nature, that is copied and spread
              rapidly by Internet users, often with slight variations” (Oxford
              Living Dictionary, 2018). Such units of information are spread
              rapidly by internet users, often for a relatively short period of
              time. The modeling of this spread is of interest for a variety of
              reasons, especially since large amounts of information are spread
              via this method. In this study, we attempt to evaluate some
              previously-implemented models of memetic diffusion and improve upon
              them when possible. In particular, we implement an SIR-based system
              of stochastic differential equations.},
  urldate = {2021-04-05},
  journal = {Results in Applied Mathematics},
  author = {Lonnberg, Adam and Xiao, Pengcheng and Wolfinger, Kathryn},
  month = may,
  year = {2020},
  note = {Publisher: Elsevier B.V.},
  keywords = {Memes, SIR Model, Stochatic differential equations},
  file = {PDF:/home/jmei/Zotero/storage/G9SE8PGY/full-text.pdf:application/pdf},
}

@misc{Johnson,
  title = {Applied {Multivariate} {Statistical} {Analysis}},
  author = {Johnson, Richard A. and Wichern, Dean W.},
  file = {
          PDF:/home/jmei/Zotero/storage/KH3T4KGL/MultivariateAnalysis.pdf:application/pdf
          },
}

@article{Hauert2008,
  title = {Evolutionary dynamics},
  issn = {18746500},
  doi = {10.1007/978-1-4020-8761-5-3},
  abstract = {Feel like writing a review for The Mathematical Intelligencer? You
              are welcome to submit an unsolicited review of a book of your
              choice; or, if you would welcome being assigned a book to review,
              please write us, telling us your expertise and your predilections.},
  journal = {NATO Science for Peace and Security Series B: Physics and
             Biophysics},
  author = {Hauert, Christoph},
  year = {2008},
  pmid = {20008382},
  note = {ISBN: 9781402087608},
  keywords = {Continuous games, Evolutionary branching, Evolutionary game theory
              , Evolutionary graph theory, Prisoner's dilemma, Snowdrift game,
              Social dilemmas, Structured populations},
  pages = {11--44},
  file = {
          PDF:/home/jmei/Zotero/storage/7T6J6XBC/Evolutionary_Dynamics.pdf:application/pdf
          },
}

@book{America,
  title = {Linear {Algebra} and {Its} {Applications}, {Fourth} {Edition} {
           Gilbert} {Strang} {Spain} (including {Portugal})},
  volume = {1},
  isbn = {0-03-010567-6},
  url = {http://www.thomsonrights.com.},
  author = {America, Latin and Paraninfo, Thomson},
  pmid = {13889148},
  doi = {10.1016/B978-0-12-387000-1.01001-9},
  note = {arXiv: 0712.0689 Publication Title: Thomson IJearning Academic
          Resource Center ISSN: 0036-1445},
  file = {
          PDF:/home/jmei/Zotero/storage/4G8ZP6GP/Linear_Algebra.pdf:application/pdf
          },
}

@techreport{noauthor_part_2016,
  title = {Part {II}-{Probability} and {Measure}},
  abstract = {These notes are not endorsed by the lecturers, and I have modified
              them (often significantly) after lectures. They are nowhere near
              accurate representations of what was actually lectured, and in
              particular, all errors are almost surely mine.},
  urldate = {2018-08-21},
  year = {2016},
  file = {
          PDF:/home/jmei/Zotero/storage/R6LC59M8/m-api-1ae1bf25-bffb-c453-3397-3f181f1953da.pdf:application/pdf
          },
}

@techreport{haimes_leontief-based_nodate,
  title = {{LEONTIEF}-{BASED} {MODEL} {OF} {RISK} {IN} {COMPLEX} {INTERCONNECTED
           } {INFRASTRUCTURES}},
  abstract = {Wassily Leontief received the 1973 Nobel Price in Economics for
              developing what came to be known as the Leontief input-output model
              of the economy. Leontief's model enables understanding the
              inter-connectedness among the various sectors of an economy and
              forecasting the effect on one segment of a change in another. A
              Leontief-based infrastructure input-output model is developed here
              to enable an accounting of the intraconnectedness within each
              critical infrastructure as well as the interconnectedness among
              them. The linear input/output model is then generalized into a
              generic risk model with the former as the first-order
              approximation. A preliminary study of the dynamics of risk of
              inoperability is discussed, using a Leontief-based dynamic model.
              Several examples are presented to illustrate the theory and its
              applications.},
  author = {Haimes, Yacov Y and Jiang, Pu},
  file = {PDF:/home/jmei/Zotero/storage/6Y46TWKR/full-text.pdf:application/pdf},
}

@article{shah_part_2017,
  title = {Part {III} — {Modern} {Statistical} {Methods}},
  author = {Shah, R D},
  year = {2017},
  pages = {1--67},
  file = {
          PDF:/home/jmei/Zotero/storage/JE4LUUKS/modern_statistical_methods.pdf:application/pdf
          },
}

@article{evans_part_2015,
  title = {Part {IB} — {Quantum} {Mechanics}},
  author = {Evans, J M},
  year = {2015},
  file = {
          PDF:/home/jmei/Zotero/storage/BXYA75U8/quantum_mechanics.pdf:application/pdf
          },
}

@article{Chua2017,
  title = {Part {III} — {Percolation} and {Random} {Walks} on {Graphs}},
  author = {Chua, Dexter},
  year = {2017},
  file = {
          PDF:/home/jmei/Zotero/storage/26E7746Q/percolation_and_random_walks_on_graphs.pdf:application/pdf
          },
}

@techreport{noauthor_part_2017,
  title = {Part {III}-{Advanced} {Probability}},
  abstract = {These notes are not endorsed by the lecturers, and I have modified
              them (often significantly) after lectures. They are nowhere near
              accurate representations of what was actually lectured, and in
              particular, all errors are almost surely mine. The aim of the
              course is to introduce students to advanced topics in modern
              probability theory. The emphasis is on tools required in the
              rigorous analysis of stochastic processes, such as Brownian motion,
              and in applications where probability theory plays an important
              role. Review of measure and integration: sigma-algebras, measures
              and filtrations; integrals and expectation; convergence theorems;
              product measures, independence and Fubini's theorem. Conditional
              expectation: Discrete case, Gaussian case, conditional density
              functions; existence and uniqueness; basic properties. Martingales:
              Martingales and submartingales in discrete time; optional stopping;
              Doob's inequalities, upcrossings, martingale convergence theorems;
              applications of martingale techniques. Stochastic processes in
              continuous time: Kolmogorov's criterion, regularization of paths;
              martingales in continuous time. Weak convergence: Definitions and
              characterizations; convergence in distribution, tightness,
              Prokhorov's theorem; characteristic functions, Lévy's continuity
              theorem. Sums of independent random variables: Strong laws of large
              numbers; central limit theorem; Cramér's theory of large
              deviations. Brownian motion: Wiener's existence theorem, scaling
              and symmetry properties; martingales associated with Brownian
              motion, the strong Markov property, hitting times; properties of
              sample paths, recurrence and transience; Brownian motion and the
              Dirichlet problem; Donsker's invariance principle. Poisson random
              measures: Construction and properties; integrals. Lévy processes:
              Lévy-Khinchin theorem. Prerequisites A basic familiarity with
              measure theory and the measure-theoretic formulation of probability
              theory is very helpful. These foundational topics will be reviewed
              at the beginning of the course, but students unfamiliar with them
              are expected to consult the literature (for instance, Williams'
              book) to strengthen their understanding.},
  urldate = {2018-08-21},
  year = {2017},
  file = {
          PDF:/home/jmei/Zotero/storage/GS94FY4P/m-api-5cb1c5f1-8d94-6ed5-1e26-8d7b9f49da9e.pdf:application/pdf
          },
}

@article{morris_using_2017,
  title = {Using simulation studies to evaluate statistical methods},
  issn = {23318422},
  abstract = {Simulation studies are computer experiments that involve creating
              data by pseudo-random sampling. A key strength of simulation
              studies is the ability to understand the behaviour of statistical
              methods because some ‘truth’ (usually some parameter/s of interest)
              is known from the process of generating the data. This allows us to
              consider properties of methods, such as bias. While widely used,
              simulation studies are often poorly designed, analysed and
              reported. This tutorial outlines the rationale for using simulation
              studies and offers guidance for design, execution, analysis,
              reporting and presentation. In particular, this tutorial provides:
              a structured approach for planning and reporting simulation studies
              , which involves defining aims, data-generating mechanisms,
              estimands, methods and performance measures (‘ADEMP’); coherent
              terminology for simulation studies; guidance on coding simulation
              studies; a critical discussion of key performance measures and
              their estimation; guidance on structuring tabular and graphical
              presentation of results; and new graphical presentations. With a
              view to describing recent practice, we review 100 articles taken
              from Volume 34 of Statistics in Medicine that included at least one
              simulation study and identify areas for improvement.},
  journal = {arXiv},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2017},
  keywords = {Monte Carlo, Graphics for simulation, Simulation design,
              Simulation reporting, Simulation studies},
  file = {PDF:/home/jmei/Zotero/storage/2TGXQN2P/Using simulation studies to
          evaluate statistical methods.pdf:application/pdf},
}

@article{morris_using_2019,
  title = {Using simulation studies to evaluate statistical methods},
  doi = {10.1002/sim.8086},
  abstract = {Simulation studies are computer experiments that involve creating
              data by pseudo-random sampling. A key strength of simulation
              studies is the ability to understand the behavior of statistical
              methods because some "truth" (usually some parameter/s of interest)
              is known from the process of generating the data. This allows us to
              consider properties of methods, such as bias. While widely used,
              simulation studies are often poorly designed, analyzed, and
              reported. This tutorial outlines the rationale for using simulation
              studies and offers guidance for design, execution, analysis,
              reporting, and presentation. In particular, this tutorial provides
              a structured approach for planning and reporting simulation studies
              , which involves defining aims, data-generating mechanisms,
              estimands, methods, and performance measures ("ADEMP"); coherent
              terminology for simulation studies; guidance on coding simulation
              studies; a critical discussion of key performance measures and
              their estimation; guidance on structuring tabular and graphical
              presentation of results; and new graphical presentations. With a
              view to describing recent practice, we review 100 articles taken
              from Volume 34 of Statistics in Medicine, which included at least
              one simulation study and identify areas for improvement.},
  author = {Morris, Tim P and White, Ian R and Crowther, Michael J},
  year = {2019},
  keywords = {Monte Carlo, graphics for simulation, simulation design,
              simulation reporting, simulation studies},
  file = {PDF:/home/jmei/Zotero/storage/EQEQDQ3J/full-text.pdf:application/pdf},
}

@article{trotter_institute_1986,
  title = {Institute of {Mathematical} {Statistics} is collaborating with {JSTOR
           } to digitize, preserve, and extend access to {Statistical} {Science}.
           ® www.jstor.org},
  volume = {2},
  number = {1},
  journal = {Statistical Science},
  author = {Trotter, H. F.},
  year = {1986},
  pages = {45--52},
  file = {
          PDF:/home/jmei/Zotero/storage/WYJUNYID/m-api-04c9c83a-b1a4-2b1d-2f3a-36707883548a.pdf:application/pdf
          },
}

@article{hswen_association_2021,
  title = {Association of “\#covid19” {Versus} “\#chinesevirus” {With} {Anti}-{
           Asian} {Sentiments} on {Twitter}: {March} 9–23, 2020},
  abstract = {Objectives. To examine the extent to which the phrases, "COVID-19"
              and "Chinese virus" were associated with anti-Asian sentiments.
              Methods.},
  author = {Hswen, Yulin and Xu, Xiang and Hing, Anna and Hawkins, Jared B and
            Brownstein, John S and Gee, Gilbert C},
  year = {2021},
  doi = {10.2105/AJPH.2021.306154},
  file = {PDF:/home/jmei/Zotero/storage/D4PGNWCD/full-text.pdf:application/pdf},
}

@incollection{ioannidis_why_2018,
  title = {Why most published research findings are false},
  volume = {2},
  isbn = {978-3-319-51358-4},
  url = {/pmc/articles/PMC1182327/},
  abstract = {There is increasing concern that most current published research
              findings are false. The probability that a research claim is true
              may depend on study power and bias, the number of other studies on
              the same question, and, importantly, the ratio of true to no
              relationships among the relationships probed in each scientific
              field. In this framework, a research finding is less likely to be
              true when the studies conducted in a field are smaller; when effect
              sizes are smaller; when there is a greater number and lesser
              preselection of tested relationships; where there is greater
              flexibility in designs, definitions, outcomes, and analytical
              modes; when there is greater financial and other interest and
              prejudice; and when more teams are involved in a scientific field
              in chase of statistical significance. Simulations show that for
              most study designs and settings, it is more likely for a research
              claim to be false than true. Moreover, for many current scientific
              fields, claimed research findings may often be simply accurate
              measures of the prevailing bias. In this essay, I discuss the
              implications of these problems for the conduct and interpretation
              of research.},
  urldate = {2021-03-19},
  booktitle = {Getting to {Good}: {Research} {Integrity} in the {Biomedical} {
               Sciences}},
  publisher = {Springer International Publishing},
  author = {Ioannidis, John P.A.},
  month = jul,
  year = {2018},
  pmid = {16060722},
  doi = {10.1371/journal.pmed.0020124},
  note = {Issue: 8 ISSN: 15491277},
  pages = {2--8},
  file = {PDF:/home/jmei/Zotero/storage/3CWQS8NZ/full-text.pdf:application/pdf},
}

@techreport{izenman_statistics_nodate,
  title = {Statistics {Modern} {Multivariate} {Statistical} {Techniques} {
           Regression}, {Classification}, and {Manifold} {Learning}},
  url = {www.springer.com/series/417},
  urldate = {2021-03-17},
  author = {Izenman, Alan Julian},
  file = {PDF:/home/jmei/Zotero/storage/MDPAR4KK/full-text.pdf:application/pdf},
}

@article{Lax2018,
  title = {Linear {Algebra}},
  author = {Lax, Peter D.},
  year = {2018},
  note = {ISBN: 9780471751564},
  pages = {1--22},
  file = {
          PDF:/home/jmei/Zotero/storage/8MXMDUYE/Linear_Algebra.pdf:application/pdf
          },
}

@misc{Berger,
  title = {Casella-{Berger}},
  author = {Berger, Casell},
  file = {
          PDF:/home/jmei/Zotero/storage/VJABPJWB/StatisticalInference.pdf:application/pdf
          },
}

@book{Cover1991,
  title = {Elements of {Information} {Theory}},
  isbn = {0-471-06259-6},
  author = {Cover, Thomas M and Thomas, Joy a and Bellamy, John and Freeman,
            Roger L and Liebowitz, Jay},
  year = {1991},
  doi = {10.1002/0471200611},
  file = {
          PDF:/home/jmei/Zotero/storage/LZ49QY49/information_theory_cover.pdf:application/pdf
          },
}

@article{sipser_introduction_nodate,
  title = {Introduction to the {Theory} of {Computation}},
  author = {Sipser, Michael},
  file = {PDF:/home/jmei/Zotero/storage/QRMV6C7R/full-text.pdf:application/pdf},
}

@book{boyd_convex_nodate,
  title = {Convex {Optimization}},
  isbn = {978-0-521-83378-3},
  url = {http://www.cambridge.org},
  urldate = {2021-03-17},
  author = {Boyd, Stephen and Vandenberghe, Lieven},
  keywords = {()},
  file = {PDF:/home/jmei/Zotero/storage/QDTBRXZ2/full-text.pdf:application/pdf},
}

@article{shahhosseini_functional_2022,
  title = {Functional {Connectivity} {Methods} and {Their} {Applications} in {
           fMRI} {Data}},
  volume = {24},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn = {1099-4300},
  url = {https://www.mdpi.com/1099-4300/24/3/390},
  doi = {10.3390/e24030390},
  abstract = {The availability of powerful non-invasive neuroimaging techniques
              has given rise to various studies that aim to map the human brain.
              These studies focus on not only ﬁnding brain activation signatures
              but also on understanding the overall organization of functional
              communication in the brain network. Based on the principle that
              distinct brain regions are functionally connected and continuously
              share information with each other, various approaches to ﬁnding
              these functional networks have been proposed in the literature. In
              this paper, we present an overview of the most common methods to
              estimate and characterize functional connectivity in fMRI data. We
              illustrate these methodologies with resting-state functional MRI
              data from the Human Connectome Project, providing details of their
              implementation and insights on the interpretations of the results.
              We aim to guide researchers that are new to the ﬁeld of
              neuroimaging by providing the necessary tools to estimate and
              characterize brain circuitry.},
  language = {en},
  number = {3},
  urldate = {2024-07-29},
  journal = {Entropy},
  author = {Shahhosseini, Yasaman and Miranda, Michelle F.},
  month = mar,
  year = {2022},
  pages = {390},
  file = {Shahhosseini and Miranda - 2022 - Functional Connectivity Methods and
          Their Applicat.pdf:/home/jmei/Zotero/storage/7LHLHSGM/Shahhosseini and
          Miranda - 2022 - Functional Connectivity Methods and Their
          Applicat.pdf:application/pdf},
}

@article{xu_dynamic_2015,
  title = {Dynamic connectivity detection: an algorithm for determining
           functional connectivity change points in {fMRI} data},
  volume = {9},
  issn = {1662-453X},
  shorttitle = {Dynamic connectivity detection},
  url = {
         http://journal.frontiersin.org/Article/10.3389/fnins.2015.00285/abstract
         },
  doi = {10.3389/fnins.2015.00285},
  abstract = {Recently there has been an increased interest in using fMRI data
              to study the dynamic nature of brain connectivity. In this setting,
              the activity in a set of regions of interest (ROIs) is often
              modeled using a multivariate Gaussian distribution, with a mean
              vector and covariance matrix that are allowed to vary as the
              experiment progresses, representing changing brain states. In this
              work, we introduce the Dynamic Connectivity Detection (DCD)
              algorithm, which is a data-driven technique to detect temporal
              change points in functional connectivity, and estimate a graph
              between ROIs for data within each segment deﬁned by the change
              points. DCD builds upon the framework of the recently developed
              Dynamic Connectivity Regression (DCR) algorithm, which has proven
              efﬁcient at detecting changes in connectivity for problems
              consisting of a small to medium ({\textless}50) number of regions,
              but which runs into computational problems as the number of regions
              becomes large ({\textgreater}100). The newly proposed DCD method is
              faster, requires less user input, and is better able to handle
              high-dimensional data. It overcomes the shortcomings of DCR by
              adopting a simpliﬁed sparse matrix estimation approach and a
              different hypothesis testing procedure to determine change points.
              The application of DCD to simulated data, as well as fMRI data,
              illustrates the efﬁcacy of the proposed method.},
  language = {en},
  urldate = {2024-07-26},
  journal = {Frontiers in Neuroscience},
  author = {Xu, Yuting and Lindquist, Martin A.},
  month = sep,
  year = {2015},
  file = {Xu and Lindquist - 2015 - Dynamic connectivity detection an algorithm
          for d.pdf:/home/jmei/Zotero/storage/ID9TMVLR/Xu and Lindquist - 2015 -
          Dynamic connectivity detection an algorithm for d.pdf:application/pdf},
}

@inproceedings{dai_testing_2016,
  address = {Las Vegas, NV, USA},
  title = {Testing {Stationarity} of {Brain} {Functional} {Connectivity} {Using}
           {Change}-{Point} {Detection} in {fMRI} {Data}},
  isbn = {978-1-5090-1437-8},
  url = {http://ieeexplore.ieee.org/document/7789616/},
  doi = {10.1109/CVPRW.2016.126},
  abstract = {This paper studies two questions: (1) Does the functional
              connectivity (FC) in a human brain remain stationary during
              performance of a task? (2) If it is non-stationary, how can one
              evaluate and estimate dynamic FC? The framework presented here
              relies on pre-segmented brain regions to represent instantaneous FC
              as symmetric, positive-deﬁnite matrices (SPDMs), with entries
              denoting covariances of fMRI signals across regions. The time
              series of such SPDMs is tested for change point detection using two
              important ideas: (1) a convenient Riemannian structure on the space
              of SPDMs for calculating geodesic distances and sample statistics,
              and (2) a graph-based approach, for testing similarity of
              distributions, that uses pairwise distances and a minimal spanning
              tree. This hypothesis test results in a temporal segmentation of
              observation interval into parts with stationary connectivity and an
              estimation of graph displaying FC during each such interval. We
              demonstrate these ideas using fMRI data from HCP database.},
  language = {en},
  urldate = {2024-07-18},
  booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {
               Recognition} {Workshops} ({CVPRW})},
  publisher = {IEEE},
  author = {Dai, Mengyu and Zhang, Zhengwu and Srivastava, Anuj},
  month = jun,
  year = {2016},
  pages = {981--989},
  file = {Dai et al. - 2016 - Testing Stationarity of Brain Functional
          Connectiv.pdf:/home/jmei/Zotero/storage/RVKNDQCI/Dai et al. - 2016 -
          Testing Stationarity of Brain Functional Connectiv.pdf:application/pdf},
}

@article{mohanty_rethinking_2020,
  title = {Rethinking {Measures} of {Functional} {Connectivity} via {Feature} {
           Extraction}},
  volume = {10},
  issn = {2045-2322},
  url = {https://www.nature.com/articles/s41598-020-57915-w},
  doi = {10.1038/s41598-020-57915-w},
  abstract = {Abstract Functional magnetic resonance imaging (fMRI)-based
              functional connectivity (FC) commonly characterizes the functional
              connections in the brain. Conventional quantification of FC by
              Pearson's correlation captures linear, time-domain dependencies
              among blood-oxygen-level-dependent (BOLD) signals. We examined
              measures to quantify FC by investigating: (i) Is Pearson's
              correlation sufficient to characterize FC? (ii) Can alternative
              measures better quantify FC? (iii) What are the implications of
              using alternative FC measures? FMRI analysis in healthy adult
              population suggested that: (i) Pearson's correlation cannot
              comprehensively capture BOLD inter-dependencies. (ii) Eight
              alternative FC measures were similarly consistent between task and
              resting-state fMRI, improved age-based classification and provided
              better association with behavioral outcomes. (iii) Formulated
              hypotheses were: first, in lieu of Pearson’s correlation, an
              augmented, composite and multi-metric definition of FC is more
              appropriate; second, canonical large-scale brain networks may
              depend on the chosen FC measure. A thorough notion of FC promises
              better understanding of variations within a given population.},
  language = {en},
  number = {1},
  urldate = {2024-07-17},
  journal = {Scientific Reports},
  author = {Mohanty, Rosaleena and Sethares, William A. and Nair, Veena A. and
            Prabhakaran, Vivek},
  month = jan,
  year = {2020},
  pages = {1298},
  file = {Mohanty et al. - 2020 - Rethinking Measures of Functional Connectivity
          via.pdf:/home/jmei/Zotero/storage/XWJGLQUT/Mohanty et al. - 2020 -
          Rethinking Measures of Functional Connectivity via.pdf:application/pdf},
}

@article{cribben_dynamic_2012,
  title = {Dynamic connectivity regression: {Determining} state-related changes
           in brain connectivity},
  volume = {61},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  issn = {10538119},
  shorttitle = {Dynamic connectivity regression},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811912003515},
  doi = {10.1016/j.neuroimage.2012.03.070},
  abstract = {Most statistical analyses of fMRI data assume that the nature,
              timing and duration of the psychological processes being studied
              are known. However, often it is hard to specify this information a
              priori. In this work we introduce a data-driven technique for
              partitioning the experimental time course into distinct temporal
              intervals with different multivariate functional connectivity
              patterns between a set of regions of interest (ROIs). The technique
              , called Dynamic Connectivity Regression (DCR), detects temporal
              change points in functional connectivity and estimates a graph, or
              set of relationships between ROIs, for data in the temporal
              partition that falls between pairs of change points. Hence, DCR
              allows for estimation of both the time of change in connectivity
              and the connectivity graph for each partition, without requiring
              prior knowledge of the nature of the experimental design.
              Permutation and bootstrapping methods are used to perform inference
              on the change points. The method is applied to various simulated
              data sets as well as to an fMRI data set from a study (N=26) of a
              state anxiety induction using a socially evaluative threat
              challenge. The results illustrate the method’s ability to observe
              how the networks between different brain regions changed with
              subjects’ emotional state.},
  language = {en},
  number = {4},
  urldate = {2024-07-13},
  journal = {NeuroImage},
  author = {Cribben, Ivor and Haraldsdottir, Ragnheidur and Atlas, Lauren Y. and
            Wager, Tor D. and Lindquist, Martin A.},
  month = jul,
  year = {2012},
  pages = {907--920},
  file = {Cribben et al. - 2012 - Dynamic connectivity regression Determining
          state.pdf:/home/jmei/Zotero/storage/KD8VNCS3/Cribben et al. - 2012 -
          Dynamic connectivity regression Determining state.pdf:application/pdf},
}

@article{baek_detecting_2023,
  title = {Detecting {Changes} in {Correlation} {Networks} with {Application} to
           {Functional} {Connectivity} of {fMRI} {Data}},
  volume = {88},
  issn = {0033-3123, 1860-0980},
  url = {https://link.springer.com/10.1007/s11336-023-09908-7},
  doi = {10.1007/s11336-023-09908-7},
  language = {en},
  number = {2},
  urldate = {2024-07-12},
  journal = {Psychometrika},
  author = {Baek, Changryong and Leinwand, Benjamin and Lindquist, Kristen A.
            and Jeong, Seok-Oh and Hopfinger, Joseph and Gates, Katheleen M. and
            Pipiras, Vladas},
  month = jun,
  year = {2023},
  pages = {636--655},
  file = {Baek et al. - 2023 - Detecting Changes in Correlation Networks with
          App.pdf:/home/jmei/Zotero/storage/W3VJMFMG/Baek et al. - 2023 -
          Detecting Changes in Correlation Networks with App.pdf:application/pdf},
}

@article{noauthor_comparative_2023,
  title = {A {Comparative} {Study} of {Correlation} {Methods} in {Functional} {
           Connectivity} {Analysis} {Using} {fMRI} {Data} of {Alzheimer}’s {
           Patients}},
  volume = {13},
  issn = {22517200},
  url = {https://jbpe.sums.ac.ir/article_47498.html},
  doi = {10.31661/jbpe.v0i0.2007-1134},
  abstract = {Background: Functional Magnetic Resonance Imaging (fMRI) is a
              non-invasive neuroimaging tool, used in brain function research and
              is also a low-frequency signal, showing brain activation by means
              of Oxygen consumption. Objective: One of the reliable methods in
              brain functional connectivity analysis is the correlation method.
              In correlation analysis, the relationship between two timeseries
              has been investigated. In fMRI analysis, the Pearson correlation is
              used while there are other methods. This study aims to investigate
              the different correlation methods in functional connectivity
              analysis. Material and Methods: In this analytical research, based
              on fMRI signals of Alzheimer’s Disease (AD) and healthy individuals
              from the ADNI database, brain functional networks were generated
              using correlation techniques, including Pearson, Kendall, and
              Spearman. Then, the global and nodal measures were calculated in
              the whole brain and in the most important resting-state network
              called Default Mode Network (DMN). The statistical analysis was
              performed using non-parametric permutation test. Results: Results
              show that although in nodal analysis, the performance of
              correlation methods was almost similar, in global features, the
              Spearman and Kendall were better in distinguishing AD subjects.
              Note that, nodal analysis reveals that the functional connectivity
              of the posterior areas in the brain was more damaged because of AD
              in comparison to frontal areas. Moreover, the functional
              connectivity of the dominant hemisphere was disrupted more.
              Conclusion: Although the Pearson method has limitations in
              capturing non-linear relationships, it is the most prevalent
              method. To have a comprehensive analysis, investigating non-linear
              methods such as distance correlation is recommended.},
  language = {en},
  number = {2},
  urldate = {2024-07-12},
  journal = {Journal of Biomedical Physics and Engineering},
  month = apr,
  year = {2023},
  file = {2023 - A Comparative Study of Correlation Methods in
          Func.pdf:/home/jmei/Zotero/storage/VBS5KEK2/2023 - A Comparative Study
          of Correlation Methods in Func.pdf:application/pdf},
}

@article{fu_adaptive_2014,
  title = {Adaptive {Covariance} {Estimation} of {Non}-{Stationary} {Processes}
           and its {Application} to {Infer} {Dynamic} {Connectivity} {From} {fMRI
           }},
  volume = {8},
  issn = {1940-9990},
  url = {https://ieeexplore.ieee.org/document/6800130/?arnumber=6800130},
  doi = {10.1109/TBCAS.2014.2306732},
  abstract = {Time-varying covariance is an important metric to measure the
              statistical dependence between non-stationary biological processes.
              Time-varying covariance is conventionally estimated from short-time
              data segments within a window having a certain bandwidth, but it is
              difficult to choose an appropriate bandwidth to estimate covariance
              with different degrees of non-stationarity. This paper introduces a
              local polynomial regression (LPR) method to estimate time-varying
              covariance and performs an asymptotic analysis of the LPR
              covariance estimator to show that both the estimation bias and
              variance are functions of the bandwidth and there exists an optimal
              bandwidth to minimize the mean square error (MSE) locally. A
              data-driven variable bandwidth selection method, namely the
              intersection of confidence intervals (ICI), is adopted in LPR for
              adaptively determining the local optimal bandwidth that minimizes
              the MSE. Experimental results on simulated signals show that the
              LPR-ICI method can achieve robust and reliable performance in
              estimating time-varying covariance with different degrees of
              variations and under different noise scenarios, making it a
              powerful tool to study the dynamic relationship between
              non-stationary biomedical signals. Further, we apply the LPR-ICI
              method to estimate time-varying covariance of functional magnetic
              resonance imaging (fMRI) signals in a visual task for the inference
              of dynamic functional brain connectivity. The results show that the
              LPR-ICI method can effectively capture the transient connectivity
              patterns from fMRI.},
  number = {2},
  urldate = {2024-07-11},
  journal = {IEEE Transactions on Biomedical Circuits and Systems},
  author = {Fu, Zening and Chan, Shing-Chow and Di, Xin and Biswal, Bharat and
            Zhang, Zhiguo},
  month = apr,
  year = {2014},
  note = {Conference Name: IEEE Transactions on Biomedical Circuits and Systems},
  keywords = {Bandwidth, Biological processes, Biomedical measurement, Dynamic
              functional connectivity, Estimation, functional magnetic resonance
              imaging (fMRI), Kernel, local polynomial regression, locally
              stationary processes, Noise, Polynomials, time-varying covariance},
  pages = {228--239},
  file = {IEEE Xplore Abstract
          Record:/home/jmei/Zotero/storage/E7I4H8TI/6800130.html:text/html;IEEE
          Xplore Full Text PDF:/home/jmei/Zotero/storage/K42F4CL3/Fu et al. -
          2014 - Adaptive Covariance Estimation of Non-Stationary
          P.pdf:application/pdf},
}

@article{cribben_detecting_2013,
  title = {Detecting functional connectivity change points for single-subject {
           fMRI} data},
  volume = {7},
  issn = {1662-5188},
  url = {
         http://journal.frontiersin.org/article/10.3389/fncom.2013.00143/abstract
         },
  doi = {10.3389/fncom.2013.00143},
  abstract = {Recently in functional magnetic resonance imaging (fMRI) studies
              there has been an increased interest in understanding the dynamic
              manner in which brain regions communicate with one another, as
              subjects perform a set of experimental tasks or as their
              psychological state changes. Dynamic Connectivity Regression (DCR)
              is a data-driven technique used for detecting temporal change
              points in functional connectivity between brain regions where the
              number and location of the change points are unknown a priori.
              After ﬁnding the change points, DCR estimates a graph or set of
              relationships between the brain regions for data that falls between
              pairs of change points. In previous work, the method was
              predominantly validated using multi-subject data. In this paper, we
              concentrate on single-subject data and introduce a new DCR
              algorithm. The new algorithm increases accuracy for individual
              subject data with a small number of observations and reduces the
              number of false positives in the estimated undirected graphs. We
              also introduce a new Likelihood Ratio test for comparing sparse
              graphs across (or within) subjects; thus allowing us to determine
              whether data should be combined across subjects. We perform an
              extensive simulation analysis on vector autoregression (VAR) data
              as well as to an fMRI data set from a study (n = 23) of a state
              anxiety induction using a socially evaluative threat challenge. The
              focus on single-subject data allows us to study the variation
              between individuals and may provide us with a deeper knowledge of
              the workings of the brain.},
  language = {en},
  urldate = {2024-07-11},
  journal = {Frontiers in Computational Neuroscience},
  author = {Cribben, Ivor and Wager, Tor D. and Lindquist, Martin A.},
  year = {2013},
  file = {Cribben et al. - 2013 - Detecting functional connectivity change
          points fo.pdf:/home/jmei/Zotero/storage/GHM3U7AR/Cribben et al. - 2013
          - Detecting functional connectivity change points
          fo.pdf:application/pdf},
}

@article{brett_introduction_nodate,
  title = {An {Introduction} to {Random} {Field} {Theory}},
  language = {en},
  author = {Brett, Matthew and Penny, Will and Kiebel, Stefan},
  file = {Brett et al. - An Introduction to Random Field
          Theory.pdf:/home/jmei/Zotero/storage/AZ79CCYS/Brett et al. - An
          Introduction to Random Field Theory.pdf:application/pdf},
}

@article{cao_new_2016,
  title = {A new change-detection method in high-resolution remote sensing
           images based on a conditional random field model},
  volume = {37},
  issn = {0143-1161, 1366-5901},
  url = {https://www.tandfonline.com/doi/full/10.1080/01431161.2016.1148284},
  doi = {10.1080/01431161.2016.1148284},
  abstract = {A new change-detection method for remote sensing images based on a
              conditional random ﬁeld (CRF) model is proposed in this paper. The
              method artfully uses memberships of Fuzzy C-means as unary
              potentials in the fully connected CRF (FCCRF) model without
              training parameters, and pairwise potentials of the CRF model are
              deﬁned by a linear combination of Gaussian kernels, with which a
              highly eﬃcient approximate inference algorithm can be used. The
              proposed FCCRF model is expressed on the complete set of pixels in
              both the observed multitemporal images, which can incorporate long
              range contextual information of remotesensing images and enable
              greatly reﬁned change-detection results. Experimental results
              demonstrate that the proposed approach leads to more accurate
              pixel-level change-detection performance and is more robust against
              noise than traditional algorithms.},
  language = {en},
  number = {5},
  urldate = {2024-07-11},
  journal = {International Journal of Remote Sensing},
  author = {Cao, Guo and Zhou, Licun and Li, Yupeng},
  month = mar,
  year = {2016},
  pages = {1173--1189},
  file = {Cao et al. - 2016 - A new change-detection method in high-resolution
          r.pdf:/home/jmei/Zotero/storage/T9VCHIEK/Cao et al. - 2016 - A new
          change-detection method in high-resolution r.pdf:application/pdf},
}

@article{genton_cross-covariance_2015,
  title = {Cross-{Covariance} {Functions} for {Multivariate} {Geostatistics}},
  volume = {30},
  issn = {0883-4237},
  url = {
         https://projecteuclid.org/journals/statistical-science/volume-30/issue-2/Cross-Covariance-Functions-for-Multivariate-Geostatistics/10.1214/14-STS487.full
         },
  doi = {10.1214/14-STS487},
  abstract = {Continuously indexed datasets with multiple variables have become
              ubiquitous in the geophysical, ecological, environmental and
              climate sciences, and pose substantial analysis challenges to
              scientists and statisticians. For many years, scientists developed
              models that aimed at capturing the spatial behavior for an
              individual process; only within the last few decades has it become
              commonplace to model multiple processes jointly. The key difﬁculty
              is in specifying the cross-covariance function, that is, the
              function responsible for the relationship between distinct
              variables. Indeed, these cross-covariance functions must be chosen
              to be consistent with marginal covariance functions in such a way
              that the second-order structure always yields a nonnegative deﬁnite
              covariance matrix. We review the main approaches to building
              cross-covariance models, including the linear model of
              coregionalization, convolution methods, the multivariate Matérn and
              nonstationary and space–time extensions of these among others. We
              additionally cover specialized constructions, including those
              designed for asymmetry, compact support and spherical domains, with
              a review of physics-constrained models. We illustrate select models
              on a bivariate regional climate model output example for
              temperature and pressure, along with a bivariate minimum and
              maximum temperature observational dataset; we compare models by
              likelihood value as well as via cross-validation co-kriging
              studies. The article closes with a discussion of unsolved problems.
              },
  language = {en},
  number = {2},
  urldate = {2024-07-10},
  journal = {Statistical Science},
  author = {Genton, Marc G. and Kleiber, William},
  month = may,
  year = {2015},
  file = {Genton and Kleiber - 2015 - Cross-Covariance Functions for
          Multivariate Geosta.pdf:/home/jmei/Zotero/storage/ULRTZEUA/Genton and
          Kleiber - 2015 - Cross-Covariance Functions for Multivariate
          Geosta.pdf:application/pdf},
}

@article{james_geomorphic_2012,
  title = {Geomorphic change detection using historic maps and {DEM}
           differencing: {The} temporal dimension of geospatial analysis},
  volume = {137},
  issn = {0169555X},
  shorttitle = {Geomorphic change detection using historic maps and {DEM}
                differencing},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0169555X11002923},
  doi = {10.1016/j.geomorph.2010.10.039},
  abstract = {The ability to develop spatially distributed models of topographic
              change is presenting new capabilities in geomorphic research. High
              resolution maps of elevation change indicate locations, processes,
              and rates of geomorphic change, and provide a means of calibrating
              temporal simulation models. Methods of geomorphic change detection
              (GCD), based on gridded models, may be applied to a wide range of
              time periods by utilizing cartometric, remote sensing, or
              ground-based topographic survey data to measure volumetric change.
              Advantages and limitations of historical DEM reconstruction methods
              are reviewed with a focus on coupling them with subsequent DEMs to
              construct DEMs of difference (DoD), which can be created by
              subtracting one elevation model from another, to map erosion,
              deposition, and volumetric change. The period of DoD analysis can
              be extended to several decades if accurate historical DEMs can be
              generated by extracting topographic data from historical data and
              selecting areas where geomorphic change has been substantial. The
              challenge is to recognize and minimize uncertainties in data that
              are particularly elusive with early topographic data. This paper
              reviews potential sources of error in digitized topographic maps
              and DEMs. Although the paper is primarily a review of methods,
              three brief examples are presented at the end to demonstrate GCD
              using DoDs constructed from data extending over periods ranging
              from 70 to 90 years.},
  language = {en},
  number = {1},
  urldate = {2024-07-10},
  journal = {Geomorphology},
  author = {James, L. Allan and Hodgson, Michael E. and Ghoshal, Subhajit and
            Latiolais, Mary Megison},
  month = jan,
  year = {2012},
  pages = {181--198},
  file = {James et al. - 2012 - Geomorphic change detection using historic maps
          an.pdf:/home/jmei/Zotero/storage/HI9KYPN6/James et al. - 2012 -
          Geomorphic change detection using historic maps an.pdf:application/pdf},
}

@article{xiong_framework_2015,
  title = {A framework of change-point detection for multivariate hydrological
           series},
  volume = {51},
  issn = {1944-7973},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/2015WR017677},
  doi = {10.1002/2015WR017677},
  abstract = {Under changing environments, not only univariate but also
              multivariate hydrological series might become nonstationary.
              Nonstationarity, in forms of change-point or trend, has been widely
              studied for univariate hydrological series, while it attracts
              attention only recently for multivariate hydrological series. For
              multivariate series, two types of change-point need to be
              distinguished, i.e., change-point in marginal distributions and
              change-point in the dependence structure among individual
              variables. In this paper, a three-step framework is proposed to
              separately detect two types of change-point in multivariate
              hydrological series, i.e., change-point detection for individual
              univariate series, estimation of marginal distributions, and
              change-point detection for dependence structure. The last step is
              implemented using both the Cramér-von Mises statistic (CvM) method
              and the copula-based likelihood-ratio test (CLR) method. For CLR,
              three kinds of copula model (symmetric, asymmetric, and
              pair-copula) are employed to construct the dependence structure of
              multivariate series. Monte Carlo experiments indicate that CLR is
              far more powerful than CvM in detecting the change-point of
              dependence structure. This framework is applied to the trivariate
              flood series composed of annual maxima daily discharge (AMDD),
              annual maxima 3 day flood volume, and annual maxima 15 day flood
              volume of the Upper Hanjiang River, China. It is found that each
              individual univariate flood series has a significant change-point;
              and the trivariate series presents a significant change-point in
              dependence structure due to the abrupt change in the dependence
              structure between AMDD and annual maxima 3 day flood volume. All
              these changes are caused by the construction of the Ankang
              Reservoir.},
  language = {en},
  number = {10},
  urldate = {2024-07-07},
  journal = {Water Resources Research},
  author = {Xiong, Lihua and Jiang, Cong and Xu, Chong-Yu and Yu, Kun-xia and
            Guo, Shenglian},
  year = {2015},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/2015WR017677
          },
  keywords = {change-point detection, copula-based likelihood-ratio, Cramér-von
              Mises statistic, dependence structure, multivariate hydrological
              series, reservoir impact},
  pages = {8198--8217},
  file = {Full Text PDF:/home/jmei/Zotero/storage/LRSVXJFT/Xiong et al. - 2015 -
          A framework of change-point detection for
          multivar.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/IM5QF8MP/2015WR017677.html:text/html;Water
          Resources Research - 2015 - Xiong - A framework of change‐point
          detection for multivariate hydrological
          series.pdf:/home/jmei/Zotero/storage/KGK4ZNFA/Water Resources Research
          - 2015 - Xiong - A framework of change‐point detection for multivariate
          hydrological series.pdf:application/pdf},
}

@inproceedings{phan_smartwatch_2015,
  address = {Beijing, China},
  title = {Smartwatch: {Performance} evaluation for long-term heart rate
           monitoring},
  isbn = {978-1-4673-6609-0},
  shorttitle = {Smartwatch},
  url = {http://ieeexplore.ieee.org/document/7344944/},
  doi = {10.1109/ISBB.2015.7344944},
  abstract = {Recent advancement in wearable technologies, particularly smart
              watches embedded with powerful processors, memory subsystems with
              various built-in sensors such as accelerometer, gyroscope and
              optical sensor in one single package has opened a whole new
              application space. One of the main applications of interest is the
              monitoring of movement patterns, heart rate, ECG and PPG
              particularly for longer duration’s in natural environments. In this
              study, we conducted a performance evaluation on the optical heart
              rate sensor of the smartwatch with respect to the commonly used ECG
              and PPG devices. Results have shown that the heart rate acquired
              from the smartwatch is reasonably accurate with a high degree of
              correlation. Further, we conducted a preliminary exerise to
              evaluate sleep quality using the heart rate readings and
              accelerometer readings captured from the smartwatch and compared
              with a commercially available and clinically used non-contact sleep
              sensor, RESMED S+.},
  language = {en},
  urldate = {2024-07-03},
  booktitle = {2015 {International} {Symposium} on {Bioelectronics} and {
               Bioinformatics} ({ISBB})},
  publisher = {IEEE},
  author = {Phan, Dung and Siong, Lee Yee and Pathirana, Pubudu N. and
            Seneviratne, Aruna},
  month = oct,
  year = {2015},
  pages = {144--147},
  file = {Phan et al. - 2015 - Smartwatch Performance evaluation for long-term
          h.pdf:/home/jmei/Zotero/storage/FMQJATRG/Phan et al. - 2015 -
          Smartwatch Performance evaluation for long-term h.pdf:application/pdf},
}

@article{asgari_mehrabadi_sleep_2020,
  title = {Sleep {Tracking} of a {Commercially} {Available} {Smart} {Ring} and {
           Smartwatch} {Against} {Medical}-{Grade} {Actigraphy} in {Everyday} {
           Settings}: {Instrument} {Validation} {Study}},
  volume = {8},
  issn = {2291-5222},
  shorttitle = {Sleep {Tracking} of a {Commercially} {Available} {Smart} {Ring}
                and {Smartwatch} {Against} {Medical}-{Grade} {Actigraphy} in {
                Everyday} {Settings}},
  url = {http://mhealth.jmir.org/2020/10/e20465/},
  doi = {10.2196/20465},
  abstract = {Background: Assessment of sleep quality is essential to address
              poor sleep quality and understand changes. Owing to the advances in
              the Internet of Things and wearable technologies, sleep monitoring
              under free-living conditions has become feasible and practicable.
              Smart rings and smartwatches can be employed to perform mid- or
              long-term home-based sleep monitoring. However, the validity of
              such wearables should be investigated in terms of sleep parameters.
              Sleep validation studies are mostly limited to short-term
              laboratory tests; there is a need for a study to assess the sleep
              attributes of wearables in everyday settings, where users engage in
              their daily routines.},
  language = {en},
  number = {10},
  urldate = {2024-07-02},
  journal = {JMIR mHealth and uHealth},
  author = {Asgari Mehrabadi, Milad and Azimi, Iman and Sarhaddi, Fatemeh and
            Axelin, Anna and Niela-Vilén, Hannakaisa and Myllyntausta, Saana and
            Stenholm, Sari and Dutt, Nikil and Liljeberg, Pasi and Rahmani, Amir
            M},
  month = nov,
  year = {2020},
  pages = {e20465},
  file = {Asgari Mehrabadi et al. - 2020 - Sleep Tracking of a Commercially
          Available Smart R.pdf:/home/jmei/Zotero/storage/KS2KVCIY/Asgari
          Mehrabadi et al. - 2020 - Sleep Tracking of a Commercially Available
          Smart R.pdf:application/pdf},
}

@inproceedings{stankovic_two-component_2017,
  address = {London},
  title = {Two-component bivariate signal decomposition based on time-frequency
           analysis},
  isbn = {978-1-5386-1895-0},
  url = {http://ieeexplore.ieee.org/document/8096048/},
  doi = {10.1109/ICDSP.2017.8096048},
  abstract = {A time-frequency analysis based approach for the decomposition of
              bivariate signals is presented. In particular, the well-known
              problem of two components overlapping in the time-frequency plane
              while having non-linear instantaneous frequencies is considered.
              The bivariate form of data leads to a signiﬁcant modiﬁcation of the
              Wigner distribution cross-terms. Therefore, the eigenvalue
              decomposition of Wigner distribution based signal autocorrelation
              matrix produces two signiﬁcant eigenvalues instead of one in the
              common Wigner distribution. It is shown that the two corresponding
              eigenvectors can be linearly combined in order to produce fully
              separated signal components. The unknown coefﬁcients are found by
              minimizing the timefrequency concentration measure of these
              particular eigenvectors linear combination. The presented approach
              is illustrated on the decomposition of a fast-varying real-valued
              signal with small instantaneous frequencies, so that its positive
              and negative frequency parts are so close that they degrade the
              analytical signal representation.},
  language = {en},
  urldate = {2024-06-30},
  booktitle = {2017 22nd {International} {Conference} on {Digital} {Signal} {
               Processing} ({DSP})},
  publisher = {IEEE},
  author = {Stankovic, Ljubisa and Brajovic, Milos and Dakovic, Milos and Mandic
            , Danilo},
  month = aug,
  year = {2017},
  pages = {1--5},
  file = {Stankovic et al. - 2017 - Two-component bivariate signal decomposition
          based.pdf:/home/jmei/Zotero/storage/X7LSLPF9/Stankovic et al. - 2017 -
          Two-component bivariate signal decomposition based.pdf:application/pdf},
}

@article{inui_use_2020,
  title = {Use of a {Smart} {Watch} for {Early} {Detection} of {Paroxysmal} {
           Atrial} {Fibrillation}: {Validation} {Study}},
  volume = {4},
  issn = {2561-1011},
  shorttitle = {Use of a {Smart} {Watch} for {Early} {Detection} of {Paroxysmal}
                {Atrial} {Fibrillation}},
  url = {http://cardio.jmir.org/2020/1/e14857/},
  doi = {10.2196/14857},
  abstract = {Background: Wearable devices with photoplethysmography (PPG)
              technology can be useful for detecting paroxysmal atrial
              fibrillation (AF), which often goes uncaptured despite being a
              leading cause of stroke. Objective: This study is the first part of
              a 2-phase study that aimed at developing a method for immediate
              detection of paroxysmal AF using PPG-integrated wearable devices.
              In this study, the diagnostic performance of 2 major smart watches,
              Apple Watch Series 3 and Fitbit (FBT) Charge HR Wireless Activity
              Wristband, each equipped with a PPG sensor, was compared, and the
              pulse rate data outputted from those devices were analyzed for
              precision and accuracy in reference to the heart rate data from
              electrocardiography (ECG) during AF. Methods: A total of 40
              subjects from patients who underwent cardiac surgery at a single
              center between September 2017 and March 2018 were monitored for
              postoperative AF using telemetric ECG and PPG devices. AF was
              diagnosed using a 12-lead ECG by qualified physicians. Each subject
              was given a pair of smart watches, Apple Watch and FBT, for
              simultaneous pulse rate monitoring. The heart rate of all subjects
              was also recorded on the telemetry system. Time series pulse rate
              trends and heart rate trends were created and analyzed for trend
              pattern similarities. Those trend data were then used to determine
              the accuracy of PPG-based pulse rate measurements in reference to
              ECG-based heart rate measurements during AF. Results: Of the 20 AF
              events in group FBT, 6 (30\%) showed a moderate or higher
              correlation (cross-correlation function{\textgreater}0.40) between
              pulse rate trend patterns and heart rate trend patterns. Of the 16
              AF events in group Apple Watch (workout [W] mode), 12 (75\%) showed
              a moderate or higher correlation between the 2 trend patterns.
              Linear regression analyses also showed a significant correlation
              between the pulse rates and the heart rates during AF in the
              subjects with Apple Watch. This correlation was not observed with
              FBT. The regression formula for Apple Watch W mode and FBT was
              X=14.203 + 0.841Y and X=58.225 + 0.228Y, respectively (where X
              denotes the mean of all average pulse rates during AF and Y denotes
              the mean of all corresponding average heart rates during AF), and
              the coefficient of determination (R2) was 0.685 and 0.057,
              respectively (P{\textless}.001 and .29, respectively). Conclusions:
              In this validation study, the detection precision of AF and
              measurement accuracy during AF were both better with Apple Watch W
              mode than with FBT.},
  language = {en},
  number = {1},
  urldate = {2024-06-29},
  journal = {JMIR Cardio},
  author = {Inui, Tomohiko and Kohno, Hiroki and Kawasaki, Yohei and Matsuura,
            Kaoru and Ueda, Hideki and Tamura, Yusaku and Watanabe, Michiko and
            Inage, Yuichi and Yakita, Yasunori and Wakabayashi, Yutaka and
            Matsumiya, Goro},
  month = jan,
  year = {2020},
  pages = {e14857},
  file = {Inui et al. - 2020 - Use of a Smart Watch for Early Detection of
          Paroxy.pdf:/home/jmei/Zotero/storage/V5VZNZ8Q/Inui et al. - 2020 - Use
          of a Smart Watch for Early Detection of Paroxy.pdf:application/pdf},
}

@article{reeder_health_2016,
  title = {Health at hand: {A} systematic review of smart watch uses for health
           and wellness},
  volume = {63},
  issn = {15320464},
  shorttitle = {Health at hand},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046416301137},
  doi = {10.1016/j.jbi.2016.09.001},
  abstract = {Introduction: Smart watches have the potential to support health
              in everyday living by: enabling self-monitoring of personal
              activity; obtaining feedback based on activity measures; allowing
              for in-situ surveys to identify patterns of behavior; and
              supporting bi-directional communication with health care providers
              and family members. However, smart watches are an emerging
              technology and research with these devices is at a nascent stage.
              Methods: We conducted a systematic review of smart watch studies
              that engaged people in their use by searching PubMed, Embase, IEEE
              XPlore and ACM Digital libraries. Participant demographics, device
              features, watch applications and methods, and technical challenges
              were abstracted from included studies. Results: Seventy-three
              studies were returned in the search. Seventeen studies published
              were included. Included studies were published from 2014 to 2016,
              with the exception of one published in 2011. Most studies employed
              the use of consumer-grade smart watches (14/17, 82\%).
              Patient-related studies focused on activity monitoring, heart rate
              monitoring, speech therapy adherence, diabetes self-management, and
              detection of seizures, tremors, scratching, eating, and
              medication-taking behaviors. Most patient-related studies enrolled
              participants with few exclusion criteria to validate smart watch
              function (10/17, 58\%). Only studies that focused on Parkinson’s
              disease, epilepsy, and diabetes management enrolled persons living
              with targeted conditions. One study focused on nursing work in the
              ICU and one focused on CPR training for laypeople. Conclusion:
              Consumer-grade smart watches have penetrated the health research
              space rapidly since 2014. Smart watch technical function,
              acceptability, and effectiveness in supporting health must be
              validated in larger ﬁeld studies that enroll actual participants
              living with the conditions these devices target.},
  language = {en},
  urldate = {2024-06-29},
  journal = {Journal of Biomedical Informatics},
  author = {Reeder, Blaine and David, Alexandria},
  month = oct,
  year = {2016},
  pages = {269--276},
  file = {Reeder and David - 2016 - Health at hand A systematic review of smart
          watch.pdf:/home/jmei/Zotero/storage/H9S79PEQ/Reeder and David - 2016 -
          Health at hand A systematic review of smart watch.pdf:application/pdf},
}

@article{hamaker_curious_2023,
  title = {The {Curious} {Case} of the {Cross}-{Sectional} {Correlation}},
  issn = {0027-3171, 1532-7906},
  url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2022.2155930},
  doi = {10.1080/00273171.2022.2155930},
  abstract = {The cross-sectional correlation is frequently used to summarize
              psychological data, and can be considered the basis for many
              statistical techniques. However, the work of Peter Molenaar on
              ergodicity has raised concerns about the meaning and utility of
              this measure, especially when the interest is in discovering
              general laws that apply to (all) individuals. Through using
              Cattell’s databox and adopting a multilevel perspective, this paper
              provides a closer look at the cross-sectional correlation, with the
              goal to better understand its meaning when ergodicity is absent. An
              analytical expression is presented that shows the crosssectional
              correlation is a function of the between-person correlation (based
              on person-specific means), and the within-person correlation (based
              on individuals’ temporal deviations from their person-specific
              means). Two curiosities related to this expression of the
              cross-sectional correlation are elaborated on, that is: a) the
              difference between the within-person correlation and the (average)
              person-specific correlation; and b) the unexpected scenarios that
              can arise because the cross-sectional correlation is a weighted sum
              rather than a weighted average of the between-person and
              within-person correlations. Seven specific examples are presented
              to illustrate various ways in which these two curiosities may
              combine; R code is provided, which allows researchers to
              investigate additional scenarios.},
  language = {en},
  urldate = {2024-06-06},
  journal = {Multivariate Behavioral Research},
  author = {Hamaker, E. L.},
  month = jan,
  year = {2023},
  pages = {1--12},
  file = {Hamaker - 2023 - The Curious Case of the Cross-Sectional
          Correlatio.pdf:/home/jmei/Zotero/storage/RJ9QQLQ7/Hamaker - 2023 - The
          Curious Case of the Cross-Sectional Correlatio.pdf:application/pdf},
}

@misc{ma_power_2018,
  title = {The {Power} of {Interpolation}: {Understanding} the {Effectiveness}
           of {SGD} in {Modern} {Over}-parametrized {Learning}},
  shorttitle = {The {Power} of {Interpolation}},
  url = {http://arxiv.org/abs/1712.06559},
  abstract = {Stochastic Gradient Descent (SGD) with small mini-batch is a key
              component in modern large-scale machine learning. However, its
              efﬁciency has not been easy to analyze as most theoretical results
              require adaptive rates and show convergence rates far slower than
              that for gradient descent, making computational comparisons
              difﬁcult.},
  language = {en},
  urldate = {2024-05-01},
  publisher = {arXiv},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  month = jun,
  year = {2018},
  note = {arXiv:1712.06559 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              },
  file = {Ma et al. - 2018 - The Power of Interpolation Understanding the
          Effe.pdf:/home/jmei/Zotero/storage/9BVGV7CI/Ma et al. - 2018 - The
          Power of Interpolation Understanding the Effe.pdf:application/pdf},
}

@misc{goodfellow_generative_2014,
  title = {Generative {Adversarial} {Networks}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an
              adversarial process, in which we simultaneously train two models: a
              generative model G that captures the data distribution, and a
              discriminative model D that estimates the probability that a sample
              came from the training data rather than G. The training procedure
              for G is to maximize the probability of D making a mistake. This
              framework corresponds to a minimax two-player game. In the space of
              arbitrary functions G and D, a unique solution exists, with G
              recovering the training data distribution and D equal to 1/2
              everywhere. In the case where G and D are defined by multilayer
              perceptrons, the entire system can be trained with backpropagation.
              There is no need for any Markov chains or unrolled approximate
              inference networks during either training or generation of samples.
              Experiments demonstrate the potential of the framework through
              qualitative and quantitative evaluation of the generated samples.},
  language = {en},
  urldate = {2024-05-01},
  publisher = {arXiv},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu,
            Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron
            and Bengio, Yoshua},
  month = jun,
  year = {2014},
  note = {arXiv:1406.2661 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              },
  file = {Goodfellow et al. - 2014 - Generative Adversarial
          Networks.pdf:/home/jmei/Zotero/storage/SIVNP72A/Goodfellow et al. -
          2014 - Generative Adversarial Networks.pdf:application/pdf},
}

@misc{mnih_playing_2013,
  title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the ﬁrst deep learning model to successfully learn
              control policies directly from high-dimensional sensory input using
              reinforcement learning. The model is a convolutional neural network
              , trained with a variant of Q-learning, whose input is raw pixels
              and whose output is a value function estimating future rewards. We
              apply our method to seven Atari 2600 games from the Arcade Learning
              Environment, with no adjustment of the architecture or learning
              algorithm. We ﬁnd that it outperforms all previous approaches on
              six of the games and surpasses a human expert on three of them.},
  language = {en},
  urldate = {2024-05-01},
  publisher = {arXiv},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves,
            Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller,
            Martin},
  month = dec,
  year = {2013},
  note = {arXiv:1312.5602 [cs]},
  keywords = {Computer Science - Machine Learning},
  file = {Mnih et al. - 2013 - Playing Atari with Deep Reinforcement
          Learning.pdf:/home/jmei/Zotero/storage/3QI8KEST/Mnih et al. - 2013 -
          Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{zagoruyko_learning_2015,
  address = {Boston, MA, USA},
  title = {Learning to compare image patches via convolutional neural networks},
  isbn = {978-1-4673-6964-0},
  url = {http://ieeexplore.ieee.org/document/7299064/},
  doi = {10.1109/CVPR.2015.7299064},
  abstract = {In this paper we show how to learn directly from image data (i.e.,
              without resorting to manually-designed features) a general
              similarity function for comparing image patches, which is a task of
              fundamental importance for many computer vision problems. To encode
              such a function, we opt for a CNN-based model that is trained to
              account for a wide variety of changes in image appearance. To that
              end, we explore and study multiple neural network architectures,
              which are speciﬁcally adapted to this task. We show that such an
              approach can signiﬁcantly outperform the state-ofthe-art on several
              problems and benchmark datasets.},
  language = {en},
  urldate = {2024-05-01},
  booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {
               Recognition} ({CVPR})},
  publisher = {IEEE},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  month = jun,
  year = {2015},
  pages = {4353--4361},
  file = {Zagoruyko and Komodakis - 2015 - Learning to compare image patches via
          convolutiona.pdf:/home/jmei/Zotero/storage/DGFYMSAW/Zagoruyko and
          Komodakis - 2015 - Learning to compare image patches via
          convolutiona.pdf:application/pdf},
}

@article{maidstone_optimal_2017,
  title = {On optimal multiple changepoint algorithms for large data},
  volume = {27},
  doi = {10.1007/s11222-016-9636-3},
  abstract = {Many common approaches to detecting change-points, for example
              based on statistical criteria such as penalised likelihood or
              minimum description length, can be formulated in terms of
              minimising a cost over segmentations. We focus on a class of
              dynamic programming algorithms that can solve the resulting
              minimisation problem exactly, and thus find the optimal
              segmentation under the given statistical criteria. The standard
              implementation of these dynamic programming methods have a
              computational cost that scales at least quadratically in the length
              of the time-series. Recently pruning ideas have been suggested that
              can speed up the dynamic programming algorithms, whilst still being
              guaranteed to be optimal, in that they find the true minimum of the
              cost function. Here we extend these pruning methods, and introduce
              two new algorithms for segmenting data: FPOP and SNIP. Empirical
              results show that FPOP is substantially faster than existing
              dynamic programming methods, and unlike the existing methods its
              computational efficiency is robust to the number of changepoints in
              the data. We evaluate the method Electronic supplementary material
              The online version of this article (for detecting copy number
              variations and observe that FPOP has a computational cost that is
              even competitive with that of binary segmentation, but can give
              much more accurate segmentations.},
  urldate = {2022-09-14},
  journal = {Statistics and Computing},
  author = {Maidstone, Robert and Hocking, Toby and Rigaill, Guillem and
            Fearnhead, Paul},
  year = {2017},
  keywords = {Breakpoints, Dynamic Programming, FPOP, Optimal partitioning, pDPA
              , PELT, Segment neighbourhood, SNIP},
  pages = {519--533},
  file = {PDF:/home/jmei/Zotero/storage/8AC2U5CY/full-text.pdf:application/pdf},
}

@article{hocking_learning_2013,
  title = {Learning smoothing models of copy number profiles using breakpoint
           annotations},
  volume = {14},
  copyright = {http://creativecommons.org/licenses/by/2.0},
  issn = {1471-2105},
  url = {
         https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-164
         },
  doi = {10.1186/1471-2105-14-164},
  abstract = {Background: Many models have been proposed to detect copy number
              alterations in chromosomal copy number profiles, but it is usually
              not obvious to decide which is most effective for a given data set.
              Furthermore, most methods have a smoothing parameter that
              determines the number of breakpoints and must be chosen using
              various heuristics. Results: We present three contributions for
              copy number profile smoothing model selection. First, we propose to
              select the model and degree of smoothness that maximizes agreement
              with visual breakpoint region annotations. Second, we develop
              cross-validation procedures to estimate the error of the trained
              models. Third, we apply these methods to compare 17 smoothing
              models on a new database of 575 annotated neuroblastoma copy number
              profiles, which we make available as a public benchmark for testing
              new algorithms. Conclusions: Whereas previous studies have been
              qualitative or limited to simulated data, our annotation-guided
              approach is quantitative and suggests which algorithms are fastest
              and most accurate in practice on real data. In the neuroblastoma
              data, the equivalent pelt.n and cghseg.k methods were the best
              breakpoint detectors, and exhibited reasonable computation times.},
  language = {en},
  number = {1},
  urldate = {2024-04-28},
  journal = {BMC Bioinformatics},
  author = {Hocking, Toby Dylan and Schleiermacher, Gudrun and Janoueix-Lerosey,
            Isabelle and Boeva, Valentina and Cappo, Julie and Delattre, Olivier
            and Bach, Francis and Vert, Jean-Philippe},
  month = dec,
  year = {2013},
  pages = {164},
  file = {Hocking et al. - 2013 - Learning smoothing models of copy number
          profiles .pdf:/home/jmei/Zotero/storage/2HLK79YQ/Hocking et al. - 2013
          - Learning smoothing models of copy number profiles
          .pdf:application/pdf},
}

@article{ma_power_nodate,
  title = {The {Power} of {Interpolation}: {Understanding} the {Effectiveness}
           of {SGD} in {Modern} {Over}-parametrized {Learning}},
  abstract = {In this paper we aim to formally explain the phenomenon of fast
              convergence of Stochastic Gradient Descent (SGD) observed in modern
              machine learning. The key observation is that most modern learning
              architectures are over-parametrized and are trained to interpolate
              the data by driving the empirical loss (classiﬁcation and
              regression) close to zero. While it is still unclear why these
              interpolated solutions perform well on test data, we show that
              these regimes allow for fast convergence of SGD, comparable in
              number of iterations to full gradient descent. For convex loss
              functions we obtain an exponential convergence bound for mini-batch
              SGD parallel to that for full gradient descent. We show that there
              is a critical batch size m∗ such that: (a) SGD iteration with
              mini-batch size m ≤ m∗ is nearly equivalent to m iterations of
              mini-batch size 1 (linear scaling regime). (b) SGD iteration with
              mini-batch m {\textgreater} m∗ is nearly equivalent to a full
              gradient descent iteration (saturation regime). Moreover, for the
              quadratic loss, we derive explicit expressions for the optimal
              mini-batch and step size and explicitly characterize the two
              regimes above. The critical mini-batch size can be viewed as the
              limit for effective mini-batch parallelization. It is also nearly
              independent of the data size, implying O(n) acceleration over GD
              per unit of computation. We give experimental evidence on real data
              which closely follows our theoretical analyses. Finally, we show
              how our results ﬁt in the recent developments in training deep
              neural networks and discuss connections to adaptive rates for SGD
              and variance reduction.},
  language = {en},
  author = {Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  file = {Ma et al. - The Power of Interpolation Understanding the
          Effe.pdf:/home/jmei/Zotero/storage/PUAAL4IJ/Ma et al. - The Power of
          Interpolation Understanding the Effe.pdf:application/pdf},
}

@article{freeman_copy_2006,
  title = {Copy number variation: {New} insights in genome diversity},
  volume = {16},
  issn = {1088-9051},
  shorttitle = {Copy number variation},
  url = {http://genome.cshlp.org/lookup/doi/10.1101/gr.3677206},
  doi = {10.1101/gr.3677206},
  abstract = {DNA copy number variation has long been associated with specific
              chromosomal rearrangements and genomic disorders, but its ubiquity
              in mammalian genomes was not fully realized until recently.
              Although our understanding of the extent of this variation is still
              developing, it seems likely that, at least in humans, copy number
              variants (CNVs) account for a substantial amount of genetic
              variation. Since many CNVs include genes that result in
              differential levels of gene expression, CNVs may account for a
              significant proportion of normal phenotypic variation. Current
              efforts are directed toward a more comprehensive cataloging and
              characterization of CNVs that will provide the basis for
              determining how genomic diversity impacts biological function,
              evolution, and common human diseases.},
  language = {en},
  number = {8},
  urldate = {2024-04-19},
  journal = {Genome Research},
  author = {Freeman, Jennifer L. and Perry, George H. and Feuk, Lars and Redon,
            Richard and McCarroll, Steven A. and Altshuler, David M. and
            Aburatani, Hiroyuki and Jones, Keith W. and Tyler-Smith, Chris and
            Hurles, Matthew E. and Carter, Nigel P. and Scherer, Stephen W. and
            Lee, Charles},
  month = aug,
  year = {2006},
  pages = {949--961},
  file = {Freeman et al. - 2006 - Copy number variation New insights in genome
          dive.pdf:/home/jmei/Zotero/storage/WYACX8FL/Freeman et al. - 2006 -
          Copy number variation New insights in genome dive.pdf:application/pdf},
}

@article{jackson_algorithm_2005,
  title = {An algorithm for optimal partitioning of data on an interval},
  volume = {12},
  copyright = {
               https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html
               },
  issn = {1070-9908},
  url = {http://ieeexplore.ieee.org/document/1381461/},
  doi = {10.1109/LSP.2001.838216},
  abstract = {Many signal processing problems can be solved by maximizing the
              ﬁtness of a segmented model over all possible partitions of the
              data interval. This letter describes a simple but powerful
              algorithm that searches the exponentially large space of partitions
              of data points in time ( 2). The algorithm is guaranteed to ﬁnd the
              exact global optimum, automatically determines the model order (the
              number of segments), has a convenient realtime mode, can be
              extended to higher dimensional data spaces, and solves a surprising
              variety of problems in signal detection and characterization,
              density estimation, cluster analysis, and classiﬁcation.},
  language = {en},
  number = {2},
  urldate = {2024-04-04},
  journal = {IEEE Signal Processing Letters},
  author = {Jackson, B. and Scargle, J.D. and Barnes, D. and Arabhi, S. and Alt,
            A. and Gioumousis, P. and Gwin, E. and San, P. and Tan, L. and {Tun
            Tao Tsai}},
  month = feb,
  year = {2005},
  pages = {105--108},
  file = {Jackson et al. - 2005 - An algorithm for optimal partitioning of data
          on a.pdf:/home/jmei/Zotero/storage/K2PXS79J/Jackson et al. - 2005 - An
          algorithm for optimal partitioning of data on a.pdf:application/pdf},
}

@article{scott_cluster_1974,
  title = {A {Cluster} {Analysis} {Method} for {Grouping} {Means} in the {
           Analysis} of {Variance}},
  volume = {30},
  issn = {0006341X},
  url = {https://www.jstor.org/stable/2529204?origin=crossref},
  doi = {10.2307/2529204},
  abstract = {It is sometimes useful in an analysis of variance to split the
              treatments into reasonably homogeneous groups. Multiple comparison
              procedures are often used for this purpose, but a more direct
              method is to use the techniques of cluster analysis. This approach
              is illustrated for several sets of data, and a likelihood ratio
              test is developed for judging the significance of differences among
              the resulting groups.},
  language = {en},
  number = {3},
  urldate = {2024-04-04},
  journal = {Biometrics},
  author = {Scott, A. J. and Knott, M.},
  month = sep,
  year = {1974},
  pages = {507},
  file = {Scott and Knott - 1974 - A Cluster Analysis Method for Grouping Means
          in th.pdf:/home/jmei/Zotero/storage/MBCHVW3I/Scott and Knott - 1974 - A
          Cluster Analysis Method for Grouping Means in th.pdf:application/pdf},
}

@article{killick_optimal_2012,
  title = {Optimal {Detection} of {Changepoints} {With} a {Linear} {
           Computational} {Cost}},
  volume = {107},
  issn = {0162-1459, 1537-274X},
  url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2012.737745},
  doi = {10.1080/01621459.2012.737745},
  language = {en},
  number = {500},
  urldate = {2024-04-04},
  journal = {Journal of the American Statistical Association},
  author = {Killick, R. and Fearnhead, P. and Eckley, I. A.},
  month = dec,
  year = {2012},
  pages = {1590--1598},
  file = {Killick et al. - 2012 - Optimal Detection of Changepoints With a
          Linear Co.pdf:/home/jmei/Zotero/storage/CVMYLHWH/Killick et al. - 2012
          - Optimal Detection of Changepoints With a Linear
          Co.pdf:application/pdf},
}

@article{hansen_new_2001,
  title = {The {New} {Econometrics} of {Structural} {Change}: {Dating} {Breaks}
           in {U}.{S}. {Labor} {Productivity}},
  volume = {15},
  issn = {0895-3309},
  shorttitle = {The {New} {Econometrics} of {Structural} {Change}},
  url = {https://pubs.aeaweb.org/doi/10.1257/jep.15.4.117},
  doi = {10.1257/jep.15.4.117},
  abstract = {We have seen the emergence of three major innovations in the
              econometrics of structural change in the past fifteen years: (1)
              tests for a structural break of unknown timing; (2) estimation of
              the timing of a structural break; and (3) tests to distinguish unit
              roots from broken time trends. These three innovations have
              dramatically altered the face of applied time series econometrics.
              In this paper, we review these three innovations, and illustrate
              their application through an empirical assessment of U.S. labor
              productivity in the manufacturing/durables sector.},
  language = {en},
  number = {4},
  urldate = {2024-04-02},
  journal = {Journal of Economic Perspectives},
  author = {Hansen, Bruce E},
  month = nov,
  year = {2001},
  pages = {117--128},
  file = {Hansen - 2001 - The New Econometrics of Structural Change Dating
          .pdf:/home/jmei/Zotero/storage/IEHJIMUX/Hansen - 2001 - The New
          Econometrics of Structural Change Dating .pdf:application/pdf},
}

@article{boettiger_introduction_2017,
  title = {An {Introduction} to {Rocker}: {Docker} {Containers} for {R}},
  volume = {9},
  issn = {2073-4859},
  shorttitle = {An {Introduction} to {Rocker}},
  url = {https://journal.r-project.org/archive/2017/RJ-2017-065/index.html},
  doi = {10.32614/RJ-2017-065},
  abstract = {We describe the Rocker project, which provides a widely-used suite
              of Docker images with customized R environments for particular
              tasks. We discuss how this suite is organized, and how these tools
              can increase portability, scaling, reproducibility, and convenience
              of R users and developers.},
  language = {en},
  number = {2},
  urldate = {2024-03-29},
  journal = {The R Journal},
  author = {Boettiger, Carl and Eddelbuettel, Dirk},
  year = {2017},
  pages = {527},
  file = {Boettiger and Eddelbuettel - 2017 - An Introduction to Rocker Docker
          Containers for R.pdf:/home/jmei/Zotero/storage/82I9BK6S/Boettiger and
          Eddelbuettel - 2017 - An Introduction to Rocker Docker Containers for
          R.pdf:application/pdf},
}

@article{nust_ten_2020,
  title = {Ten simple rules for writing {Dockerfiles} for reproducible data
           science},
  volume = {16},
  issn = {1553-7358},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1008316},
  doi = {10.1371/journal.pcbi.1008316},
  abstract = {Computational science has been greatly improved by the use of
              containers for packaging software and data dependencies. In a
              scholarly context, the main drivers for using these containers are
              transparency and support of reproducibility; in turn, a workflow’s
              reproducibility can be greatly affected by the choices that are
              made with respect to building containers. In many cases, the build
              process for the container’s image is created from instructions
              provided in a Dockerfile format. In support of this approach, we
              present a set of rules to help researchers write understandable
              Dockerfiles for typical data science workflows. By following the
              rules in this article, researchers can create containers suitable
              for sharing with fellow scientists, for including in scholarly
              communication such as education or scientific papers, and for
              effective and sustainable personal workflows.},
  language = {en},
  number = {11},
  urldate = {2024-03-29},
  journal = {PLOS Computational Biology},
  author = {Nüst, Daniel and Sochat, Vanessa and Marwick, Ben and Eglen, Stephen
            J. and Head, Tim and Hirst, Tony and Evans, Benjamin D.},
  editor = {Markel, Scott},
  month = nov,
  year = {2020},
  pages = {e1008316},
  file = {Nüst et al. - 2020 - Ten simple rules for writing Dockerfiles for
          repro.pdf:/home/jmei/Zotero/storage/VAU5WXWN/Nüst et al. - 2020 - Ten
          simple rules for writing Dockerfiles for repro.pdf:application/pdf},
}

@article{kristoufek_measuring_2014,
  title = {Measuring correlations between non-stationary series with {DCCA}
           coefficient},
  volume = {402},
  issn = {03784371},
  url = {http://arxiv.org/abs/1310.3984},
  doi = {10.1016/j.physa.2014.01.058},
  abstract = {In this short report, we investigate the ability of the DCCA
              coeﬃcient to measure correlation level between non-stationary
              series. Based on a wide Monte Carlo simulation study, we show that
              the DCCA coeﬃcient can estimate the correlation coeﬃcient
              accurately regardless the strength of non-stationarity (measured by
              the fractional diﬀerencing parameter d). For a comparison, we also
              report the results for the standard Pearson’s correlation
              coeﬃcient. The DCCA coeﬃcient dominates the Pearson’s coeﬃcient for
              non-stationary series.},
  language = {en},
  urldate = {2024-03-24},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Kristoufek, Ladislav},
  month = may,
  year = {2014},
  note = {arXiv:1310.3984 [q-fin]},
  keywords = {Quantitative Finance - Statistical Finance},
  pages = {291--298},
  file = {Kristoufek - 2014 - Measuring correlations between non-stationary
          seri.pdf:/home/jmei/Zotero/storage/AH9RT35S/Kristoufek - 2014 -
          Measuring correlations between non-stationary seri.pdf:application/pdf},
}

@article{zebende_dcca_2011,
  title = {{DCCA} cross-correlation coefficient: {Quantifying} level of
           cross-correlation},
  volume = {390},
  issn = {03784371},
  shorttitle = {{DCCA} cross-correlation coefficient},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437110008800},
  doi = {10.1016/j.physa.2010.10.022},
  abstract = {In this paper, a new coefficient is proposed with the objective of
              quantifying the level of cross-correlation between nonstationary
              time series. This cross-correlation coefficient is defined in terms
              of the DFA method and the DCCA method. The implementation of this
              cross-correlation coefficient will be illustrated with selected
              time series.},
  language = {en},
  number = {4},
  urldate = {2024-03-24},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Zebende, G.F.},
  month = feb,
  year = {2011},
  pages = {614--618},
  file = {Zebende - 2011 - DCCA cross-correlation coefficient Quantifying
          le.pdf:/home/jmei/Zotero/storage/L3LMZ8B4/Zebende - 2011 - DCCA
          cross-correlation coefficient Quantifying le.pdf:application/pdf},
}

@article{podobnik_detrended_2008,
  title = {Detrended {Cross}-{Correlation} {Analysis}: {A} {New} {Method} for {
           Analyzing} {Two} {Nonstationary} {Time} {Series}},
  volume = {100},
  issn = {0031-9007, 1079-7114},
  shorttitle = {Detrended {Cross}-{Correlation} {Analysis}},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.100.084102},
  doi = {10.1103/PhysRevLett.100.084102},
  language = {en},
  number = {8},
  urldate = {2024-03-24},
  journal = {Physical Review Letters},
  author = {Podobnik, Boris and Stanley, H. Eugene},
  month = feb,
  year = {2008},
  pages = {084102},
  file = {Podobnik and Stanley - 2008 - Detrended Cross-Correlation Analysis A
          New Method.pdf:/home/jmei/Zotero/storage/R6IVCV7G/Podobnik and Stanley
          - 2008 - Detrended Cross-Correlation Analysis A New
          Method.pdf:application/pdf},
}

@article{zebende_dcca_2013,
  title = {{DCCA} cross-correlation coefficient differentiation: {Theoretical}
           and practical approaches},
  volume = {392},
  issn = {03784371},
  shorttitle = {{DCCA} cross-correlation coefficient differentiation},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437113000253},
  doi = {10.1016/j.physa.2013.01.011},
  abstract = {We propose in this paper to establish a well-defined relationship
              between αDFA (the long range auto-correlation exponent) and λDCCA
              (the long range cross-correlation exponent), respectively described
              by the DFA and DCCA methods. This relationship will be accomplished
              theoretically by differentiating the DCCA cross-correlation
              coefficient, ρDCCA(n). Also, for some specific times series, we
              apply this theory in order to establish its validity.},
  language = {en},
  number = {8},
  urldate = {2024-03-24},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Zebende, G.F. and da Silva, M.F. and Machado Filho, A.},
  month = apr,
  year = {2013},
  pages = {1756--1761},
  file = {Zebende et al. - 2013 - DCCA cross-correlation coefficient
          differentiation.pdf:/home/jmei/Zotero/storage/ZZZIDD23/Zebende et al. -
          2013 - DCCA cross-correlation coefficient
          differentiation.pdf:application/pdf},
}

@article{niu_multiple_2016-1,
  title = {Multiple {Change}-{Point} {Detection}: {A} {Selective} {Overview}},
  volume = {31},
  issn = {0883-4237},
  shorttitle = {Multiple {Change}-{Point} {Detection}},
  url = {
         https://projecteuclid.org/journals/statistical-science/volume-31/issue-4/Multiple-Change-Point-Detection-A-Selective-Overview/10.1214/16-STS587.full
         },
  doi = {10.1214/16-STS587},
  abstract = {Very long and noisy sequence data arise from biological sciences
              to social science including high throughput data in genomics and
              stock prices in econometrics. Often such data are collected in
              order to identify and understand shifts in trends, for example,
              from a bull market to a bear market in ﬁnance or from a normal
              number of chromosome copies to an excessive number of chromosome
              copies in genetics. Thus, identifying multiple change points in a
              long, possibly very long, sequence is an important problem. In this
              article, we review both classical and new multiple change-point
              detection strategies. Considering the long history and the
              extensive literature on the change-point detection, we provide an
              in-depth discussion on a normal mean change-point model from
              aspects of regression analysis, hypothesis testing, consistency and
              inference. In particular, we present a strategy to gather and
              aggregate local information for change-point detection that has
              become the cornerstone of several emerging methods because of its
              attractiveness in both computational and theoretical properties.},
  language = {en},
  number = {4},
  urldate = {2024-03-19},
  journal = {Statistical Science},
  author = {Niu, Yue S. and Hao, Ning and Zhang, Heping},
  month = nov,
  year = {2016},
  file = {Niu et al. - 2016 - Multiple Change-Point Detection A Selective
          Overv.pdf:/home/jmei/Zotero/storage/4GDFS94A/Niu et al. - 2016 -
          Multiple Change-Point Detection A Selective Overv.pdf:application/pdf},
}

@article{truong_selective_2020,
  title = {Selective review of offline change point detection methods},
  volume = {167},
  issn = {01651684},
  url = {http://arxiv.org/abs/1801.00718},
  doi = {10.1016/j.sigpro.2019.107299},
  abstract = {This article presents a selective survey of algorithms for the
              oﬄine detection of multiple change points in multivariate time
              series. A general yet structuring methodological strategy is
              adopted to organize this vast body of work. More precisely,
              detection algorithms considered in this review are characterized by
              three elements: a cost function, a search method and a constraint
              on the number of changes. Each of those elements is described,
              reviewed and discussed separately. Implementations of the main
              algorithms described in this article are provided within a Python
              package called ruptures.},
  language = {en},
  urldate = {2024-03-19},
  journal = {Signal Processing},
  author = {Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  month = feb,
  year = {2020},
  note = {arXiv:1801.00718 [cs, stat]},
  keywords = {Computer Science - Computational Engineering, Finance, and Science
              , Statistics - Computation, Statistics - Methodology},
  pages = {107299},
  file = {Truong et al. - 2020 - Selective review of offline change point
          detection.pdf:/home/jmei/Zotero/storage/6JV9ZM4R/Truong et al. - 2020 -
          Selective review of offline change point detection.pdf:application/pdf},
}

@misc{rigaill_pruned_2015,
  title = {A pruned dynamic programming algorithm to recover the best
           segmentations with \$1\$ to \${K}\_\{max\}\$ change-points},
  url = {http://arxiv.org/abs/1004.0887},
  abstract = {Multiple change-point detection models assume that the observed
              data is a realization of an independent random process aﬀected by K
              − 1 abrupt changes, called change-points, at some unknown
              positions. For oﬀ-line detection a dynamic programming (DP)
              algorithm retrieves the K − 1 change-points minimizing the
              quadratic loss and reduces the complexity from Θ(nK) to Θ(Kn2)
              where n is the number of observations. The quadratic complexity in
              n still restricts the use of such an algorithm to small or
              intermediate values of n. We propose a pruned DP algorithm that
              recovers the optimal solution. We demonstrate that at worst the
              complexity is in O(Kn2) time and O(Kn) space and is therefore at
              worst equivalent to the classical DP. We show empirically that the
              run-time of our proposed algorithm is drastically reduced compared
              to the classical DP algorithm. More precisely, our algorithm is
              able to process a million points in a matter of minutes compared to
              several days with the classical DP algorithm. Moreover, the
              principle of the proposed algorithm can be extended to other convex
              losses (for example the Poisson loss) and as the algorithm process
              one observation after the other it could be adapted for on-line
              problems.},
  language = {en},
  urldate = {2024-03-19},
  publisher = {arXiv},
  author = {Rigaill, Guillem},
  month = aug,
  year = {2015},
  note = {arXiv:1004.0887 [stat]},
  keywords = {Statistics - Computation},
  file = {Rigaill - 2015 - A pruned dynamic programming algorithm to recover
          .pdf:/home/jmei/Zotero/storage/HAIRBPXQ/Rigaill - 2015 - A pruned
          dynamic programming algorithm to recover .pdf:application/pdf},
}

@misc{bonanno_high-frequency_2000,
  title = {High-frequency {Cross}-correlation in a {Set} of {Stocks}},
  url = {http://arxiv.org/abs/cond-mat/0009350},
  abstract = {The high-frequency cross-correlation existing between pairs of
              stocks traded in a financial market are investigated in a set of
              100 stocks traded in US equity markets. A hierarchical organization
              of the investigated stocks is obtained by determining a metric
              distance between stocks and by investigating the properties of the
              subdominant ultrametric associated with it. A clear modification of
              the hierarchical organization of the set of stocks investigated is
              detected when the time horizon used to determine stock returns is
              changed. The hierarchical location of stocks of the energy sector
              is investigated as a function of the time horizon.},
  language = {en},
  urldate = {2024-03-17},
  publisher = {arXiv},
  author = {Bonanno, Giovanni and Lillo, Fabrizio and Mantegna, Rosario N.},
  month = nov,
  year = {2000},
  note = {arXiv:cond-mat/0009350},
  keywords = {Quantitative Finance - Statistical Finance, cross-correlation},
  file = {Bonanno et al. - 2000 - High-frequency Cross-correlation in a Set of
          Stock.pdf:/home/jmei/Zotero/storage/8D6RNV3J/Bonanno et al. - 2000 -
          High-frequency Cross-correlation in a Set of Stock.pdf:application/pdf},
}

@article{precup_comparison_2004,
  title = {A comparison of high-frequency cross-correlation measures},
  volume = {344},
  issn = {03784371},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437104009446},
  doi = {10.1016/j.physa.2004.06.127},
  abstract = {On a high-frequency scale the time series are not homogeneous,
              therefore standard correlation measures cannot be directly applied
              to the raw data. There are two ways to deal with this problem. The
              time series can be homogenised through an interpolation method (An
              Introduction to High-Frequency Finance, Academic Press, NY, 2001)
              (linear or previous tick) and then the Pearson correlation
              statistic computed. Recently, methods that can handle raw
              non-synchronous time series have been developed (Int. J. Theor.
              Appl. Finance 6(1) (2003) 87; J. Empirical Finance 4 (1997) 259).
              This paper compares two traditional methods that use interpolation
              with an alternative method applied directly to the actual time
              series.},
  language = {en},
  number = {1-2},
  urldate = {2024-03-17},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Precup, Ovidiu V. and Iori, Giulia},
  month = dec,
  year = {2004},
  pages = {252--256},
  file = {Precup and Iori - 2004 - A comparison of high-frequency
          cross-correlation m.pdf:/home/jmei/Zotero/storage/RLNLJ5XU/Precup and
          Iori - 2004 - A comparison of high-frequency cross-correlation
          m.pdf:application/pdf},
}

@article{elliott_pairs_2005,
  title = {Pairs trading},
  volume = {5},
  issn = {1469-7688, 1469-7696},
  url = {http://www.tandfonline.com/doi/abs/10.1080/14697680500149370},
  doi = {10.1080/14697680500149370},
  language = {en},
  number = {3},
  urldate = {2024-03-15},
  journal = {Quantitative Finance},
  author = {Elliott, Robert J. and Van Der Hoek *, John and Malcolm, William P.},
  month = jun,
  year = {2005},
  pages = {271--276},
  file = {Elliott et al. - 2005 - Pairs
          trading.pdf:/home/jmei/Zotero/storage/392F2E3Q/Elliott et al. - 2005 -
          Pairs trading.pdf:application/pdf},
}

@article{podobnik_detrended_2008-1,
  title = {Detrended {Cross}-{Correlation} {Analysis}: {A} {New} {Method} for {
           Analyzing} {Two} {Non}-stationary {Time} {Series}},
  volume = {100},
  issn = {0031-9007, 1079-7114},
  shorttitle = {Detrended {Cross}-{Correlation} {Analysis}},
  url = {http://arxiv.org/abs/0709.0281},
  doi = {10.1103/PhysRevLett.100.084102},
  abstract = {Here we propose a method, based on detrended covariance which we
              call detrended cross-correlation analysis (DXA), to investigate
              power-law cross-correlations between diﬀerent
              simultaneously-recorded time series in the presence of
              non-stationarity. We illustrate the method by selected examples
              from physics, physiology, and ﬁnance.},
  language = {en},
  number = {8},
  urldate = {2024-03-05},
  journal = {Physical Review Letters},
  author = {Podobnik, Boris and Stanley, H. Eugene},
  month = feb,
  year = {2008},
  note = {arXiv:0709.0281 [cond-mat, q-fin]},
  keywords = {Quantitative Finance - Statistical Finance, Condensed Matter -
              Statistical Mechanics},
  pages = {084102},
  file = {Podobnik and Stanley - 2008 - Detrended Cross-Correlation Analysis A
          New Method.pdf:/home/jmei/Zotero/storage/L25LYC3X/Podobnik and Stanley
          - 2008 - Detrended Cross-Correlation Analysis A New
          Method.pdf:application/pdf},
}

@article{el-gohary_establishing_2007,
  title = {Establishing {Causality} {With} {Whitened} {Cross}-{Correlation} {
           Analysis}},
  volume = {54},
  issn = {0018-9294},
  url = {http://ieeexplore.ieee.org/document/4376253/},
  doi = {10.1109/TBME.2007.906519},
  abstract = {In many biomedical applications, it is important to determine
              whether two recorded signals have a causal relationship and, if so,
              what the nature of that relationship is. Many advanced techniques
              have been proposed to characterize this relationship, but in
              practice simple techniques such as cross-correlation analysis are
              used. Unfortunately, the traditional cross-correlation analysis is
              inﬂuenced by the autocorrelation of the signals as much as it is by
              the relationship between the signals. Practically, this results in
              the cross correlation suggesting that the signals are correlated
              over a broad range of lags. Prewhitening the signals overcomes this
              limitation and reveals the essentially causal relationship between
              the signals. This is a simple method that can also easily
              generalize cross-correlation analysis to nonstationary signals,
              which are frequently encountered in biomedical applications. This
              technique has been used in other ﬁelds, but remains mostly
              undiscovered in biomedical research. In the case of a purely causal
              relationship, we show that whitened cross-correlation analysis is
              equivalent to directly estimating the all-pass component of the
              transfer function that relates the signals. We give examples of
              this type of analysis applied to several biomedical applications to
              demonstrate some of the new insights and information that can be
              produced by this type of analysis.},
  language = {en},
  number = {12},
  urldate = {2024-02-22},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {El-Gohary, M. and McNames, J.},
  month = dec,
  year = {2007},
  pages = {2214--2222},
  file = {El-Gohary and McNames - 2007 - Establishing Causality With Whitened
          Cross-Correla.pdf:/home/jmei/Zotero/storage/RM97C3DK/El-Gohary and
          McNames - 2007 - Establishing Causality With Whitened
          Cross-Correla.pdf:application/pdf},
}

@misc{noauthor_httpsieeexploreieeeorgstampstampjsparnumber4376253_nodate,
  title = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376253},
  url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376253},
  urldate = {2024-02-22},
  file = {https\:
          //ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4376253:/home/jmei/Zotero/storage/36JP3ZI8/stamp.html:text/html
          },
}

@article{erdem_new_2014,
  title = {A new correlation coefficient for bivariate time-series data},
  volume = {414},
  issn = {03784371},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0378437114006384},
  doi = {10.1016/j.physa.2014.07.054},
  abstract = {The correlation in time series has received considerable attention
              in the literature. Its use has attained an important role in the
              social sciences and finance. For example, pair trading in finance
              is concerned with the correlation between stock prices, returns,
              etc. In general, Pearson’s correlation coefficient is employed in
              these areas although it has many underlying assumptions which
              restrict its use. Here, we introduce a new correlation coefficient
              which takes into account the lag difference of data points. We
              investigate the properties of this new correlation coefficient. We
              demonstrate that it is more appropriate for showing the direction
              of the covariation of the two variables over time. We also compare
              the performance of the new correlation coefficient with Pearson’s
              correlation coefficient and Detrended Cross-Correlation Analysis
              (DCCA) via simulated examples.},
  language = {en},
  urldate = {2024-02-21},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Erdem, Orhan and Ceyhan, Elvan and Varli, Yusuf},
  month = nov,
  year = {2014},
  pages = {274--284},
  file = {Erdem et al. - 2014 - A new correlation coefficient for bivariate
          time-s.pdf:/home/jmei/Zotero/storage/EMGEW6FE/Erdem et al. - 2014 - A
          new correlation coefficient for bivariate time-s.pdf:application/pdf},
}

@article{rice_bandwidth_1984,
  title = {Bandwidth {Choice} for {Nonparametric} {Regression}},
  volume = {12},
  issn = {0090-5364},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-4/Bandwidth-Choice-for-Nonparametric-Regression/10.1214/aos/1176346788.full
         },
  doi = {10.1214/aos/1176346788},
  language = {en},
  number = {4},
  urldate = {2024-02-21},
  journal = {The Annals of Statistics},
  author = {Rice, John},
  month = dec,
  year = {1984},
  file = {Rice - 1984 - Bandwidth Choice for Nonparametric
          Regression.pdf:/home/jmei/Zotero/storage/A596HBLJ/Rice - 1984 -
          Bandwidth Choice for Nonparametric Regression.pdf:application/pdf},
}

@article{lindquist_evaluating_2014,
  title = {Evaluating dynamic bivariate correlations in resting-state {fMRI}: {A
           } comparison study and a new approach},
  volume = {101},
  issn = {10538119},
  shorttitle = {Evaluating dynamic bivariate correlations in resting-state {fMRI
                }},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811914005291},
  doi = {10.1016/j.neuroimage.2014.06.052},
  abstract = {To date, most functional Magnetic Resonance Imaging (fMRI) studies
              have assumed that the functional connectivity (FC) between time
              series from distinct brain regions is constant across time. However
              , recently, there has been an increased interest in quantifying
              possible dynamic changes in FC during fMRI experiments, as it is
              thought that this may provide insight into the fundamental workings
              of brain networks. In this work we focus on the speciﬁc problem of
              estimating the dynamic behavior of pair-wise correlations between
              time courses extracted from two different regions of the brain. We
              critique the commonly used sliding-window technique, and discuss
              some alternative methods used to model volatility in the ﬁnance
              literature that could also prove to be useful in the neuroimaging
              setting. In particular, we focus on the Dynamic Conditional
              Correlation (DCC) model, which provides a model-based approach
              towards estimating dynamic correlations. We investigate the
              properties of several techniques in a series of simulation studies
              and ﬁnd that DCC achieves the best overall balance between
              sensitivity and speciﬁcity in detecting dynamic changes in
              correlations. We also investigate its scalability beyond the
              bivariate case to demonstrate its utility for studying dynamic
              correlations between more than two brain regions. Finally, we
              illustrate its performance in an application to test–retest resting
              state fMRI data.},
  language = {en},
  urldate = {2024-02-21},
  journal = {NeuroImage},
  author = {Lindquist, Martin A. and Xu, Yuting and Nebel, Mary Beth and Caffo,
            Brain S.},
  month = nov,
  year = {2014},
  pages = {531--546},
  file = {Lindquist et al. - 2014 - Evaluating dynamic bivariate correlations in
          resti.pdf:/home/jmei/Zotero/storage/PUCZZNAL/Lindquist et al. - 2014 -
          Evaluating dynamic bivariate correlations in resti.pdf:application/pdf},
}

@article{muller_variance_1993,
  title = {On variance function estimation with quadratic forms},
  volume = {35},
  issn = {03783758},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0378375893900469},
  doi = {10.1016/0378-3758(93)90046-9},
  language = {en},
  number = {2},
  urldate = {2024-02-21},
  journal = {Journal of Statistical Planning and Inference},
  author = {Müller, Hans-Georg and Stadtmüller, Ulrich},
  month = may,
  year = {1993},
  pages = {213--231},
  file = {Müller and Stadtmüller - 1993 - On variance function estimation with
          quadratic for.pdf:/home/jmei/Zotero/storage/J8ZJYQ87/Müller and
          Stadtmüller - 1993 - On variance function estimation with quadratic
          for.pdf:application/pdf},
}

@article{parks_efficient_1967,
  title = {Efficient {Estimation} of a {System} of {Regression} {Equations} when
           {Disturbances} are {Both} {Serially} and {Contemporaneously} {
           Correlated}},
  volume = {62},
  issn = {0162-1459, 1537-274X},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10482923},
  doi = {10.1080/01621459.1967.10482923},
  language = {en},
  number = {318},
  urldate = {2024-02-21},
  journal = {Journal of the American Statistical Association},
  author = {Parks, Richard W.},
  month = jun,
  year = {1967},
  pages = {500--509},
  file = {Parks - 1967 - Efficient Estimation of a System of Regression
          Equ.pdf:/home/jmei/Zotero/storage/FYYD9KAT/Parks - 1967 - Efficient
          Estimation of a System of Regression Equ.pdf:application/pdf},
}

@article{robinson_correlation_2008,
  title = {Correlation testing in time series, spatial and cross-sectional data},
  volume = {147},
  issn = {03044076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407608001206},
  doi = {10.1016/j.jeconom.2008.09.001},
  abstract = {We provide a general class of tests for correlation in time series
              , spatial, spatio-temporal and crosssectional data. We motivate our
              focus by reviewing how computational and theoretical difficulties
              of point estimation mount, as one moves from regularly-spaced time
              series data, through forms of irregular spacing, and to spatial
              data of various kinds. A broad class of computationally simple
              tests is justified. These specialize to Lagrange multiplier tests
              against parametric departures of various kinds. Their forms are
              illustrated in case of several models for describing correlation in
              various kinds of data. The initial focus assumes homoscedasticity,
              but we also robustify the tests to nonparametric
              heteroscedasticity.},
  language = {en},
  number = {1},
  urldate = {2024-02-20},
  journal = {Journal of Econometrics},
  author = {Robinson, P.M.},
  month = nov,
  year = {2008},
  pages = {5--16},
  file = {Robinson - 2008 - Correlation testing in time series, spatial and
          cr.pdf:/home/jmei/Zotero/storage/6WGI6FP7/Robinson - 2008 - Correlation
          testing in time series, spatial and cr.pdf:application/pdf},
}

@article{beck_what_1995,
  title = {What {To} {Do} (and {Not} to {Do}) with {Time}-{Series} {Cross}-{
           Section} {Data}},
  volume = {89},
  issn = {0003-0554, 1537-5943},
  url = {
         https://www.cambridge.org/core/product/identifier/S0003055400000083/type/journal_article
         },
  doi = {10.2307/2082979},
  abstract = {We examine some issues in the estimation of time-series
              cross-section models, calling into question the conclusions of many
              published studies, particularly in the field of comparative
              political economy. We show that the generalized least squares
              approach of Parks produces standard errors that lead to extreme
              overconfidence, often underestimating variability by 50\% or more.
              We also provide an alternative estimator of the standard errors
              that is correct when the error structures show complications found
              in this type of model. Monte Carlo analysis shows that these
              “panel-corrected standard errors” perform well. The utility of our
              approach is demonstrated via a reanalysis of one “social democratic
              corporatist” model.},
  language = {en},
  number = {3},
  urldate = {2024-02-19},
  journal = {American Political Science Review},
  author = {Beck, Nathaniel and Katz, Jonathan N.},
  month = sep,
  year = {1995},
  pages = {634--647},
  file = {Beck and Katz - 1995 - What To Do (and Not to Do) with Time-Series
          Cross-.pdf:/home/jmei/Zotero/storage/K533F5S4/Beck and Katz - 1995 -
          What To Do (and Not to Do) with Time-Series Cross-.pdf:application/pdf},
}

@incollection{box-steffensmeier_timeseries_2009,
  edition = {1},
  title = {Time‐{Series} {Cross}‐{Section} {Methods}},
  isbn = {978-0-19-928654-6 978-0-19-157730-7},
  url = {https://academic.oup.com/edited-volume/28340/chapter/215157430},
  abstract = {Abstract This article outlines the literature on time-series
              cross-sectional (TSCS) methods. First, it addresses time-series
              properties including issues of nonstationarity. It moves to
              cross-sectional issues including heteroskedasticity and spatial
              autocorrelation. The ways that TSCS methods deal with heterogeneous
              units through fixed effects and random coefficient models are
              shown. In addition, a discussion of binary variables and their
              relationship to event history models is provided. The best way to
              think about modeling single time series is to think about modeling
              the time-series component of TSCS data. On the cross-sectional side
              , the best approach is one based on thinking about cross-sectional
              issues like a spatial econometrician. In general, the critical
              insight is that TSCS and binary TSCS data present a series of
              interesting issues that must be carefully considered, and not a
              standard set of nuisances that can be dealt with by a command in
              some statistical package.},
  language = {en},
  urldate = {2024-02-19},
  booktitle = {The {Oxford} {Handbook} of {Political} {Methodology}},
  publisher = {Oxford University Press},
  author = {Beck, Nathaniel},
  editor = {Box-Steffensmeier, Janet M. and Brady, Henry E. and Collier, David},
  month = sep,
  year = {2009},
  doi = {10.1093/oxfordhb/9780199286546.003.0020},
  pages = {475--493},
  file = {Beck - 2009 - Time‐Series Cross‐Section
          Methods.pdf:/home/jmei/Zotero/storage/9EI74XDK/Beck - 2009 -
          Time‐Series Cross‐Section Methods.pdf:application/pdf},
}

@article{baltagi_estimation_2016,
  title = {Estimation of heterogeneous panels with structural breaks},
  volume = {191},
  issn = {03044076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407615002353},
  doi = {10.1016/j.jeconom.2015.03.048},
  abstract = {This paper extends Pesaran’s (2006) work on common correlated
              effects (CCE) estimators for large heterogeneous panels with a
              general multifactor error structure by allowing for unknown common
              structural breaks. Structural breaks due to new policy
              implementation or major technological shocks, are more likely to
              occur over a longer time span. Consequently, ignoring structural
              breaks may lead to inconsistent estimation and invalid inference.
              We propose a general framework that includes heterogeneous panel
              data models and structural break models as special cases. The least
              squares method proposed by Bai (1997a, 2010) is applied to estimate
              the common change points, and the consistency of the estimated
              change points is established. We find that the CCE estimator have
              the same asymptotic distribution as if the true change points were
              known. Additionally, Monte Carlo simulations are used to verify the
              main results of this paper.},
  language = {en},
  number = {1},
  urldate = {2024-02-19},
  journal = {Journal of Econometrics},
  author = {Baltagi, Badi H. and Feng, Qu and Kao, Chihwa},
  month = mar,
  year = {2016},
  pages = {176--195},
  file = {Baltagi et al. - 2016 - Estimation of heterogeneous panels with
          structural.pdf:/home/jmei/Zotero/storage/WS2CH8BR/Baltagi et al. - 2016
          - Estimation of heterogeneous panels with
          structural.pdf:application/pdf},
}

@article{pesaran_estimation_2006,
  title = {Estimation and {Inference} in {Large} {Heterogeneous} {Panels} with a
           {Multifactor} {Error} {Structure}},
  volume = {74},
  issn = {1468-0262},
  url = {
         https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00692.x
         },
  doi = {10.1111/j.1468-0262.2006.00692.x},
  abstract = {This paper presents a new approach to estimation and inference in
              panel data models with a general multifactor error structure. The
              unobserved factors and the individual-specific errors are allowed
              to follow arbitrary stationary processes, and the number of
              unobserved factors need not be estimated. The basic idea is to
              filter the individual-specific regressors by means of cross-section
              averages such that asymptotically as the cross-section dimension
              (N) tends to infinity, the differential effects of unobserved
              common factors are eliminated. The estimation procedure has the
              advantage that it can be computed by least squares applied to
              auxiliary regressions where the observed regressors are augmented
              with cross-sectional averages of the dependent variable and the
              individual-specific regressors. A number of estimators (referred to
              as common correlated effects (CCE) estimators) are proposed and
              their asymptotic distributions are derived. The small sample
              properties of mean group and pooled CCE estimators are investigated
              by Monte Carlo experiments, showing that the CCE estimators have
              satisfactory small sample properties even under a substantial
              degree of heterogeneity and dynamics, and for relatively small
              values of N and T.},
  language = {en},
  number = {4},
  urldate = {2024-02-19},
  journal = {Econometrica},
  author = {Pesaran, M. Hashem},
  year = {2006},
  note = {\_eprint:
          https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2006.00692.x
          },
  keywords = {common correlated effects, Cross-section dependence, estimation
              and inference, heterogeneity, large panels, cross-sectional
              correlation, panel data},
  pages = {967--1012},
  file = {Full Text PDF:/home/jmei/Zotero/storage/GQAGD9GS/Pesaran - 2006 -
          Estimation and Inference in Large Heterogeneous
          Pa.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/JK5VGBBC/j.1468-0262.2006.00692.html:text/html
          },
}

@article{harchaoui_multiple_2010,
  title = {Multiple {Change}-{Point} {Estimation} {With} a {Total} {Variation} {
           Penalty}},
  volume = {105},
  issn = {0162-1459, 1537-274X},
  url = {https://www.tandfonline.com/doi/full/10.1198/jasa.2010.tm09181},
  doi = {10.1198/jasa.2010.tm09181},
  language = {en},
  number = {492},
  urldate = {2024-02-19},
  journal = {Journal of the American Statistical Association},
  author = {Harchaoui, Z. and Lévy-Leduc, C.},
  month = dec,
  year = {2010},
  pages = {1480--1493},
  file = {Harchaoui and Lévy-Leduc - 2010 - Multiple Change-Point Estimation
          With a Total Vari.pdf:/home/jmei/Zotero/storage/84M8PYZT/Harchaoui and
          Lévy-Leduc - 2010 - Multiple Change-Point Estimation With a Total
          Vari.pdf:application/pdf},
}

@article{zhu_new_2020,
  title = {A new structural break test for panels with common factors},
  volume = {23},
  issn = {1368-4221, 1368-423X},
  url = {https://academic.oup.com/ectj/article/23/1/137/5593950},
  doi = {10.1093/ectj/utz018},
  abstract = {This paper develops new tests against a structural break in panel
              data models with common factors when T is ﬁxed, where T denotes the
              number of observations over time. For this class of models, the
              available tests against a structural break are valid only under the
              assumption that T is ‘large’. However, this may be a stringent
              requirement—more commonly so in datasets with annual time frequency
              , in which case the sample may cover a relatively long period even
              if T is not large. The proposed approach builds upon existing
              generalized method of moments methodology and develops
              Distance-type and Lagrange Multiplier-type tests for detecting a
              structural break, both when the break point is known and when it is
              unknown. The proposed methodology permits weak exogeneity and/or
              endogeneity of the regressors. In a simulation study, the method
              performed well, in terms of size and power, as well as in terms of
              successfully locating the time of the structural break. The method
              is illustrated by testing the so-called ‘Gibrat’s Law’, using a
              dataset from 4,128 ﬁnancial institutions, each one observed for the
              period 2002–2014.},
  language = {en},
  number = {1},
  urldate = {2024-02-19},
  journal = {The Econometrics Journal},
  author = {Zhu, Huanjun and Sarafidis, Vasilis and Silvapulle, Mervyn J},
  month = jan,
  year = {2020},
  pages = {137--155},
  file = {Zhu et al. - 2020 - A new structural break test for panels with
          common.pdf:/home/jmei/Zotero/storage/I9RTRCU7/Zhu et al. - 2020 - A new
          structural break test for panels with common.pdf:application/pdf},
}

@article{perron_dealing_nodate,
  title = {Dealing with {Structural} {Breaks}},
  abstract = {This chapter is concerned with methodological issues related to
              estimation, testing and computation in the context of structural
              changes in the linear models. A central theme of the review is the
              interplay between structural change and unit root and on methods to
              distinguish between the two. The topics covered are: methods
              related to estimation and inference about break dates for single
              equations with or without restrictions, with extensions to
              multi-equations systems where allowance is also made for changes in
              the variability of the shocks; tests for structural changes
              including tests for a single or multiple changes and tests valid
              with unit root or trending regressors, and tests for changes in the
              trend function of a series that can be integrated or
              trendstationary; testing for a unit root versus trend-stationarity
              in the presence of structural changes in the trend function;
              testing for cointegration in the presence of structural changes;
              and issues related to long memory and level shifts. Our focus is on
              the conceptual issues about the frameworks adopted and the
              assumptions imposed as they relate to potential applicability. We
              also highlight the potential problems that can occur with methods
              that are commonly used and recent work that has been done to
              overcome them.},
  language = {en},
  author = {Perron, Pierre},
  file = {Perron - Dealing with Structural
          Breaks.pdf:/home/jmei/Zotero/storage/R6XFQWU3/Perron - Dealing with
          Structural Breaks.pdf:application/pdf},
}

@article{barigozzi_simultaneous_2018,
  title = {Simultaneous multiple change-point and factor analysis for
           high-dimensional time series},
  volume = {206},
  issn = {03044076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407618300915},
  doi = {10.1016/j.jeconom.2018.05.003},
  abstract = {We propose the first comprehensive treatment of high-dimensional
              time series factor models with multiple change-points in their
              second-order structure. We operate under the most flexible
              definition of piecewise stationarity, and estimate the number and
              locations of change-points consistently as well as identifying
              whether they originate in the common or idiosyncratic components.
              Through the use of wavelets, we transform the problem of
              change-point detection in the second-order structure of a
              high-dimensional time series, into the (relatively easier) problem
              of change-point detection in the means of high-dimensional panel
              data. Also, our methodology circumvents the difficult issue of the
              accurate estimation of the true number of factors in the presence
              of multiple change-points by adopting a screening procedure. We
              further show that consistent factor analysis is achieved over each
              segment defined by the change-points estimated by the proposed
              methodology. In extensive simulation studies, we observe that
              factor analysis prior to change-point detection improves the
              detectability of change-points, and identify and describe an
              interesting ‘spillover’ effect in which substantial breaks in the
              idiosyncratic components get, naturally enough, identified as
              change-points in the common components, which prompts us to regard
              the corresponding change-points as also acting as a form of
              ‘factors’. Our methodology is implemented in the R package
              factorcpt, available from CRAN.},
  language = {en},
  number = {1},
  urldate = {2024-02-16},
  journal = {Journal of Econometrics},
  author = {Barigozzi, Matteo and Cho, Haeran and Fryzlewicz, Piotr},
  month = sep,
  year = {2018},
  pages = {187--225},
  file = {Barigozzi et al. - 2018 - Simultaneous multiple change-point and
          factor anal.pdf:/home/jmei/Zotero/storage/3GEV35JZ/Barigozzi et al. -
          2018 - Simultaneous multiple change-point and factor
          anal.pdf:application/pdf},
}

@article{pagan_econometric_2008,
  title = {Econometric analysis of structural systems with permanent and
           transitory shocks},
  volume = {32},
  issn = {01651889},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0165188908000304},
  doi = {10.1016/j.jedc.2008.01.006},
  language = {en},
  number = {10},
  urldate = {2024-02-16},
  journal = {Journal of Economic Dynamics and Control},
  author = {Pagan, A.R. and Pesaran, M. Hashem},
  month = oct,
  year = {2008},
  pages = {3376--3395},
  file = {Pagan and Pesaran - 2008 - Econometric analysis of structural systems
          with pe.pdf:/home/jmei/Zotero/storage/JF9NZGJR/Pagan and Pesaran - 2008
          - Econometric analysis of structural systems with
          pe.pdf:application/pdf},
}

@article{andrews_cross-section_2005,
  title = {Cross-{Section} {Regression} with {Common} {Shocks}},
  volume = {73},
  issn = {0012-9682, 1468-0262},
  url = {http://doi.wiley.com/10.1111/j.1468-0262.2005.00629.x},
  doi = {10.1111/j.1468-0262.2005.00629.x},
  language = {en},
  number = {5},
  urldate = {2024-02-16},
  journal = {Econometrica},
  author = {Andrews, Donald W. K.},
  month = sep,
  year = {2005},
  pages = {1551--1585},
  file = {Andrews - 2005 - Cross-Section Regression with Common
          Shocks.pdf:/home/jmei/Zotero/storage/Y5SM4YBY/Andrews - 2005 -
          Cross-Section Regression with Common Shocks.pdf:application/pdf},
}

@article{chudik_weak_2011,
  title = {Weak and strong cross‐section dependence and estimation of large
           panels},
  volume = {14},
  issn = {1368-4221, 1368-423X},
  url = {https://academic.oup.com/ectj/article/14/1/C45/5060343},
  doi = {10.1111/j.1368-423X.2010.00330.x},
  abstract = {This paper introduces the concepts of time-speciﬁc weak and strong
              crosssection dependence, and investigates how these notions are
              related to the concepts of weak, strong and semi-strong common
              factors, frequently used for modelling residual cross-section
              correlations in panel data models. It then focuses on the problems
              of estimating slope coefﬁcients in large panels, where
              cross-section units are subject to possibly a large number of
              unobserved common factors. It is established that the common
              correlated effects (CCE) estimator introduced by Pesaran remains
              asymptotically normal under certain conditions on factor loadings
              of an inﬁnite factor error structure, including cases where methods
              relying on principal components fail. The paper concludes with a
              set of Monte Carlo experiments where the small sample properties of
              estimators based on principal components and CCE estimators are
              investigated and compared under various assumptions on the nature
              of the unobserved common effects.},
  language = {en},
  number = {1},
  urldate = {2024-02-16},
  journal = {The Econometrics Journal},
  author = {Chudik, Alexander and Pesaran, M. Hashem and Tosetti, Elisa},
  month = feb,
  year = {2011},
  pages = {C45--C90},
  file = {Chudik et al. - 2011 - Weak and strong cross‐section dependence and
          estim.pdf:/home/jmei/Zotero/storage/5TCKB2QW/Chudik et al. - 2011 -
          Weak and strong cross‐section dependence and estim.pdf:application/pdf},
}

@article{hsiao_analysis_nodate,
  title = {Analysis of {Panel} {Data}},
  language = {en},
  author = {Hsiao, Cheng},
  file = {Hsiao - Analysis of Panel
          Data.pdf:/home/jmei/Zotero/storage/26JCNUFA/Hsiao - Analysis of Panel
          Data.pdf:application/pdf},
}

@book{hsiao_analysis_2007,
  address = {Cambridge},
  edition = {2. ed., 8. print},
  series = {Econometric {Society} monographs},
  title = {Analysis of panel data},
  isbn = {978-0-521-81855-1 978-0-521-52271-7},
  language = {en},
  number = {34},
  publisher = {Cambridge University Press},
  author = {Hsiao, Cheng},
  year = {2007},
  file = {Hsiao - 2007 - Analysis of panel
          data.pdf:/home/jmei/Zotero/storage/UBX6AD8M/Hsiao - 2007 - Analysis of
          panel data.pdf:application/pdf},
}

@article{lindquist_modeling_2007,
  title = {Modeling state-related {fMRI} activity using change-point theory},
  volume = {35},
  issn = {10538119},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811907000092},
  doi = {10.1016/j.neuroimage.2007.01.004},
  language = {en},
  number = {3},
  urldate = {2024-02-16},
  journal = {NeuroImage},
  author = {Lindquist, Martin A. and Waugh, Christian and Wager, Tor D.},
  month = apr,
  year = {2007},
  pages = {1125--1141},
  file = {Lindquist et al. - 2007 - Modeling state-related fMRI activity using
          change-.pdf:/home/jmei/Zotero/storage/ZXSQDAHE/Lindquist et al. - 2007
          - Modeling state-related fMRI activity using
          change-.pdf:application/pdf},
}

@article{gao_estimating_2017,
  title = {Estimating correlation between multivariate longitudinal data in the
           presence of heterogeneity},
  volume = {17},
  issn = {1471-2288},
  url = {
         http://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-017-0398-1
         },
  doi = {10.1186/s12874-017-0398-1},
  abstract = {Background: Estimating correlation coefficients among outcomes is
              one of the most important analytical tasks in epidemiological and
              clinical research. Availability of multivariate longitudinal data
              presents a unique opportunity to assess joint evolution of outcomes
              over time. Bivariate linear mixed model (BLMM) provides a versatile
              tool with regard to assessing correlation. However, BLMMs often
              assume that all individuals are drawn from a single homogenous
              population where the individual trajectories are distributed
              smoothly around population average. Methods: Using longitudinal
              mean deviation (MD) and visual acuity (VA) from the Ocular
              Hypertension Treatment Study (OHTS), we demonstrated strategies to
              better understand the correlation between multivariate longitudinal
              data in the presence of potential heterogeneity. Conditional
              correlation (i.e., marginal correlation given random effects) was
              calculated to describe how the association between longitudinal
              outcomes evolved over time within specific subpopulation. The
              impact of heterogeneity on correlation was also assessed by
              simulated data. Results: There was a significant positive
              correlation in both random intercepts (ρ = 0.278, 95\% CI:
              0.121–0.420) and random slopes (ρ = 0.579, 95\% CI: 0.349–0.810)
              between longitudinal MD and VA, and the strength of correlation
              constantly increased over time. However, conditional correlation
              and simulation studies revealed that the correlation was induced
              primarily by participants with rapid deteriorating MD who only
              accounted for a small fraction of total samples. Conclusion:
              Conditional correlation given random effects provides a robust
              estimate to describe the correlation between multivariate
              longitudinal data in the presence of unobserved heterogeneity
              (NCT00000125).},
  language = {en},
  number = {1},
  urldate = {2024-02-15},
  journal = {BMC Medical Research Methodology},
  author = {Gao, Feng and Philip Miller, J. and Xiong, Chengjie and Luo, Jingqin
            and Beiser, Julia A. and Chen, Ling and Gordon, Mae O.},
  month = dec,
  year = {2017},
  pages = {124},
  file = {Gao et al. - 2017 - Estimating correlation between multivariate
          longit.pdf:/home/jmei/Zotero/storage/U4FRSNHS/Gao et al. - 2017 -
          Estimating correlation between multivariate longit.pdf:application/pdf},
}

@article{rosenfield_change_2010,
  title = {Change point analysis for longitudinal physiological data: {Detection
           } of cardio-respiratory changes preceding panic attacks},
  volume = {84},
  issn = {03010511},
  shorttitle = {Change point analysis for longitudinal physiological data},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0301051110000396},
  doi = {10.1016/j.biopsycho.2010.01.020},
  abstract = {Statistical methods for detecting changes in longitudinal time
              series of psychophysiological data are limited. ANOVA and mixed
              models are not designed to detect the existence, timing, or
              duration of unknown changes in such data. Change point (CP)
              analysis was developed to detect distinct changes in time series
              data. Preliminary reports using CP analysis for fMRI data are
              promising. Here, we illustrate the application of CP analysis for
              detecting discrete changes in ambulatory, peripheral physiological
              data leading up to naturally occurring panic attacks (PAs). The CP
              method was successful in detecting cardiorespiratory changes that
              preceded the onset of reported PAs. Furthermore, the changes were
              unique to the pre-PA period, and were not detected in matched
              non-PA control periods. The efﬁcacy of our CP method was further
              validated by detecting patterns of change that were consistent with
              prominent respiratory theories of panic positing a relation between
              aberrant respiration and panic etiology.},
  language = {en},
  number = {1},
  urldate = {2024-02-15},
  journal = {Biological Psychology},
  author = {Rosenfield, David and Zhou, Enlu and Wilhelm, Frank H. and Conrad,
            Ansgar and Roth, Walton T. and Meuret, Alicia E.},
  month = apr,
  year = {2010},
  pages = {112--120},
  file = {Rosenfield et al. - 2010 - Change point analysis for longitudinal
          physiologic.pdf:/home/jmei/Zotero/storage/IMLTVWAL/Rosenfield et al. -
          2010 - Change point analysis for longitudinal
          physiologic.pdf:application/pdf},
}

@article{bai_common_2010,
  title = {Common breaks in means and variances for panel data},
  volume = {157},
  issn = {03044076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407609002735},
  doi = {10.1016/j.jeconom.2009.10.020},
  abstract = {This paper establishes the consistency of the estimated common
              break point in panel data. Consistency is obtainable even when a
              regime contains a single observation, making it possible to quickly
              identify the onset of a new regime. We also propose a new framework
              for developing the limiting distribution for the estimated break
              point, and show how to construct confidence intervals. The least
              squares method is used for estimating breaks in means and the
              quasi-maximum likelihood (QML) method is used to estimate breaks in
              means and in variances. QML is shown to be more efficient than the
              least squares even if there is no change in the variances.},
  language = {en},
  number = {1},
  urldate = {2024-02-14},
  journal = {Journal of Econometrics},
  author = {Bai, Jushan},
  month = jul,
  year = {2010},
  pages = {78--92},
  file = {Bai - 2010 - Common breaks in means and variances for panel
          dat.pdf:/home/jmei/Zotero/storage/XPFZH7IK/Bai - 2010 - Common breaks
          in means and variances for panel dat.pdf:application/pdf},
}

@article{breusch_lagrange_1980,
  title = {The {Lagrange} {Multiplier} {Test} and its {Applications} to {Model}
           {Specification} in {Econometrics}},
  volume = {47},
  issn = {00346527},
  url = {https://academic.oup.com/restud/article-lookup/doi/10.2307/2297111},
  doi = {10.2307/2297111},
  language = {en},
  number = {1},
  urldate = {2024-02-14},
  journal = {The Review of Economic Studies},
  author = {Breusch, T. S. and Pagan, A. R.},
  month = jan,
  year = {1980},
  pages = {239},
  file = {Breusch and Pagan - 1980 - The Lagrange Multiplier Test and its
          Applications .pdf:/home/jmei/Zotero/storage/W4W435D3/Breusch and Pagan
          - 1980 - The Lagrange Multiplier Test and its Applications
          .pdf:application/pdf},
}

@article{pesaran_testing_2015,
  title = {Testing {Weak} {Cross}-{Sectional} {Dependence} in {Large} {Panels}},
  volume = {34},
  issn = {0747-4938, 1532-4168},
  url = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2014.956623},
  doi = {10.1080/07474938.2014.956623},
  language = {en},
  number = {6-10},
  urldate = {2024-02-14},
  journal = {Econometric Reviews},
  author = {Pesaran, M. Hashem},
  month = may,
  year = {2015},
  keywords = {cross-sectional correlation, panel data},
  pages = {1089--1117},
  file = {Pesaran - 2015 - Testing Weak Cross-Sectional Dependence in Large
          P.pdf:/home/jmei/Zotero/storage/JVBXYP2U/Pesaran - 2015 - Testing Weak
          Cross-Sectional Dependence in Large P.pdf:application/pdf},
}

@article{horvath_change-point_2012,
  title = {Change-point detection in panel data},
  volume = {33},
  issn = {1467-9892},
  url = {
         https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9892.2012.00796.x
         },
  doi = {10.1111/j.1467-9892.2012.00796.x},
  abstract = {We consider N panels and each panel is based on T observations. We
              are interested to test if the means of the panels remain the same
              during the observation period against the alternative that the
              means change at an unknown time. We provide tests which are derived
              from a likelihood argument and they are based on the adaptation of
              the CUSUM method to panel data. Asymptotic distributions are
              derived under the no change null hypothesis and the consistency of
              the tests are proven under the alternative. The asymptotic results
              are shown to work in case of small and moderate sample sizes via
              Monte Carlo simulations.},
  language = {en},
  number = {4},
  urldate = {2024-02-14},
  journal = {Journal of Time Series Analysis},
  author = {Horváth, Lajos and Hušková, Marie},
  year = {2012},
  note = {\_eprint:
          https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.2012.00796.x
          },
  keywords = {weak convergence, 60F17, 62F05, 62M10, change in the mean, CUSUM
              process, linear processes, Panel date},
  pages = {631--648},
  file = {Full Text PDF:/home/jmei/Zotero/storage/CIMHENY5/Horváth and Hušková -
          2012 - Change-point detection in panel
          data.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/TVR8BW5N/j.1467-9892.2012.00796.html:text/html
          },
}

@article{frees_assessing_1995,
  title = {Assessing cross-sectional correlation in panel data},
  volume = {69},
  issn = {03044076},
  url = {https://linkinghub.elsevier.com/retrieve/pii/030440769401658M},
  doi = {10.1016/0304-4076(94)01658-M},
  abstract = {Consider drawing a sample of ‘n’ experimental units where each
              unit is observed over ‘T ’ time periods. Are the draws independent?
              A test statistic introduced by Breusch and Pagan (1980) is one
              measure used to quantify the amount of cross-sectional correlation.
              It is effective when T is large relative to n and, as shown by
              Breusch and Pagan, has desirable asymptotic (in T) properties. In
              this paper, we consider the case often encountered in the analysis
              of panel data, where n is large relative to T. Here, the Breusch
              and Pagan statistic does not enjoy the same desirable asymptotic
              (in n) properties. In fact, we show that the asymptotic
              distribution depends on the parent population even under the
              hypothesis of no cross-sectional correlations. Thus, we introduce a
              distribution-free statistic that does not have this drawback.
              Asymptotic properties of the new statistic are established, when
              the data are drawn from identical distributions and when the
              statistic is evaluated using residuals from a complex model. Both
              the Breusch and Pagan statistic and the new statistic are shown to
              be members of a more general family. By considering this
              generalization, finite-sample properties of members of the family
              are easily established. Further, it turns out that a statistic
              introduced by Friedman (1937) is another special case of this
              family.},
  language = {en},
  number = {2},
  urldate = {2024-02-14},
  journal = {Journal of Econometrics},
  author = {Frees, Edward W.},
  month = oct,
  year = {1995},
  pages = {393--414},
  file = {Frees - 1995 - Assessing cross-sectional correlation in panel
          dat.pdf:/home/jmei/Zotero/storage/928728WL/Frees - 1995 - Assessing
          cross-sectional correlation in panel dat.pdf:application/pdf},
}

@article{ng_testing_2006,
  title = {Testing {Cross}-{Section} {Correlation} in {Panel} {Data} {Using} {
           Spacings}},
  volume = {24},
  issn = {0735-0015, 1537-2707},
  url = {http://www.tandfonline.com/doi/abs/10.1198/073500105000000171},
  doi = {10.1198/073500105000000171},
  language = {en},
  number = {1},
  urldate = {2024-02-14},
  journal = {Journal of Business \& Economic Statistics},
  author = {Ng, Serena},
  month = jan,
  year = {2006},
  pages = {12--23},
  file = {Ng - 2006 - Testing Cross-Section Correlation in Panel Data
          Us.pdf:/home/jmei/Zotero/storage/PZM2ZIXQ/Ng - 2006 - Testing
          Cross-Section Correlation in Panel Data Us.pdf:application/pdf},
}

@incollection{wolpert_reflections_2018,
  edition = {1},
  title = {Reflections {After} {Refereeing} {Papers} for {NIPS}},
  isbn = {978-0-429-49252-5},
  url = {
         https://www.taylorfrancis.com/books/9780429961076/chapters/10.1201/9780429492525-2
         },
  language = {en},
  urldate = {2023-10-09},
  booktitle = {The {Mathematics} of {Generalization}},
  publisher = {CRC Press},
  author = {Breiman, Leo},
  collaborator = {Wolpert, David. H},
  month = mar,
  year = {2018},
  doi = {10.1201/9780429492525-2},
  pages = {11--15},
  file = {Breiman - 2018 - Reflections After Refereeing Papers for
          NIPS.pdf:/home/jmei/Zotero/storage/IT2A8XXK/Breiman - 2018 -
          Reflections After Refereeing Papers for NIPS.pdf:application/pdf},
}

@misc{zhang_understanding_2017,
  title = {Understanding deep learning requires rethinking generalization},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artiﬁcial neural
              networks can exhibit a remarkably small difference between training
              and test performance. Conventional wisdom attributes small
              generalization error either to properties of the model family, or
              to the regularization techniques used during training.},
  language = {en},
  urldate = {2023-10-06},
  publisher = {arXiv},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,
            Benjamin and Vinyals, Oriol},
  month = feb,
  year = {2017},
  note = {arXiv:1611.03530 [cs]},
  keywords = {Computer Science - Machine Learning},
  file = {Zhang et al. - 2017 - Understanding deep learning requires rethinking
          ge.pdf:/home/jmei/Zotero/storage/M3P8UMXA/Zhang et al. - 2017 -
          Understanding deep learning requires rethinking ge.pdf:application/pdf},
}

@misc{zhang_understanding_2017-1,
  title = {Understanding deep learning requires rethinking generalization},
  url = {http://arxiv.org/abs/1611.03530},
  abstract = {Despite their massive size, successful deep artiﬁcial neural
              networks can exhibit a remarkably small difference between training
              and test performance. Conventional wisdom attributes small
              generalization error either to properties of the model family, or
              to the regularization techniques used during training.},
  language = {en},
  urldate = {2023-10-05},
  publisher = {arXiv},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht,
            Benjamin and Vinyals, Oriol},
  month = feb,
  year = {2017},
  note = {arXiv:1611.03530 [cs]},
  keywords = {Computer Science - Machine Learning},
  file = {Zhang et al. - 2017 - Understanding deep learning requires rethinking
          ge.pdf:/home/jmei/Zotero/storage/6IW6GXCD/Zhang et al. - 2017 -
          Understanding deep learning requires rethinking ge.pdf:application/pdf},
}

@article{belkin_fit_2021,
  title = {Fit without fear: remarkable mathematical phenomena of deep learning
           through the prism of interpolation},
  volume = {30},
  issn = {0962-4929, 1474-0508},
  shorttitle = {Fit without fear},
  url = {
         https://www.cambridge.org/core/product/identifier/S0962492921000039/type/journal_article
         },
  doi = {10.1017/S0962492921000039},
  abstract = {In the past decade the mathematical theory of machine learning has
              lagged far behind the triumphs of deep neural networks on practical
              challenges. However, the gap between theory and practice is
              gradually starting to close. In this paper I will attempt to
              assemble some pieces of the remarkable and still incomplete
              mathematical mosaic emerging from the efforts to understand the
              foundations of deep learning. The two key themes will be
              interpolation and its sibling over-parametrization. Interpolation
              corresponds to fitting data, even noisy data, exactly.
              Over-parametrization enables interpolation and provides flexibility
              to select a suitable interpolating model. As we will see, just as a
              physical prism separates colours mixed within a ray of light, the
              figurative prism of interpolation helps to disentangle
              generalization and optimization properties within the complex
              picture of modern machine learning. This article is written in the
              belief and hope that clearer understanding of these issues will
              bring us a step closer towards a general theory of deep learning
              and machine learning.},
  language = {en},
  urldate = {2023-10-05},
  journal = {Acta Numerica},
  author = {Belkin, Mikhail},
  month = may,
  year = {2021},
  pages = {203--248},
  file = {Belkin - 2021 - Fit without fear remarkable mathematical
          phenomen.pdf:/home/jmei/Zotero/storage/P4S6BQK7/Belkin - 2021 - Fit
          without fear remarkable mathematical phenomen.pdf:application/pdf},
}

@article{cook_bi-variate_2023,
  title = {Bi-{Variate} k-{Statistics} and {Cumulants} of {Their} {Joint} {
           Sampling} {Distribution}},
  language = {en},
  author = {Cook, M B},
  year = {2023},
  file = {Cook - 2023 - Bi-Variate k-Statistics and Cumulants of Their
          Joi.pdf:/home/jmei/Zotero/storage/UAE2NAQ4/Cook - 2023 - Bi-Variate
          k-Statistics and Cumulants of Their Joi.pdf:application/pdf},
}

@article{dette_change_2019,
  title = {Change {Point} {Analysis} of {Correlation} in {Non}-stationary {Time}
           {Series}},
  issn = {10170405},
  url = {http://www3.stat.sinica.edu.tw/statistica/J29N2/J29N25/J29N25.html},
  doi = {10.5705/ss.202016.0493},
  abstract = {A restrictive assumption in change point analysis is “stationarity
              under the null hypothesis of no change-point”, which is crucial for
              asymptotic theory but not very realistic from a practical point of
              view. For example, if change point analysis for correlations is
              performed, it is not necessarily clear that the mean, marginal
              variance or higher order moments are constant, even if there is no
              change in the correlation. This paper develops change point
              analysis for the correlation structures under less restrictive
              assumptions. In contrast to previous work, our approach does not
              require that the mean, variance and fourth order joint cumulants
              are constant under the null hypothesis. Moreover, we also address
              the problem of detecting relevant change points.},
  language = {en},
  urldate = {2023-07-16},
  journal = {Statistica Sinica},
  author = {Dette, Holger and Wu, Weichi and Zhou, Zhou},
  year = {2019},
  file = {Dette et al. - 2019 - Change Point Analysis of Correlation in
          Non-statio.pdf:/home/jmei/Zotero/storage/CUCNR3Y3/Dette et al. - 2019 -
          Change Point Analysis of Correlation in Non-statio.pdf:application/pdf},
}

@article{kim_change-point_nodate,
  title = {{CHANGE}-{POINT} {DETECTION} {FOR} {CORRELATED} {OBSERVATIONS}},
  abstract = {This paper considers a likelihood ratio test for a change in mean
              when observations are not independent. First, the e ect of
              correlation on the performance of the likelihood ratio test derived
              under the assumption of no correlation is examined. Then, the
              likelihood ratio statistic for testing for a change in mean is
              obtained under a general structure of nonzero correlation. For
              general correlation and some serial correlations such as AR(p),
              distributional properties of the test statistic are examined and
              methods to compute approximate p-values are discussed. Finally, the
              power of the likelihood ratio test is compared with that of the
              test proposed by Henderson (1986).},
  language = {en},
  author = {Kim, Hyune-Ju},
  file = {Kim - CHANGE-POINT DETECTION FOR CORRELATED
          OBSERVATIONS.pdf:/home/jmei/Zotero/storage/EXP6LIIY/Kim - CHANGE-POINT
          DETECTION FOR CORRELATED OBSERVATIONS.pdf:application/pdf},
}

@article{liu_change-point_2013,
  title = {Change-{Point} {Detection} in {Time}-{Series} {Data} by {Relative} {
           Density}-{Ratio} {Estimation}},
  volume = {43},
  issn = {08936080},
  url = {http://arxiv.org/abs/1203.0453},
  doi = {10.1016/j.neunet.2013.01.012},
  abstract = {The objective of change-point detection is to discover abrupt
              property changes lying behind time-series data. In this paper, we
              present a novel statistical changepoint detection algorithm based
              on non-parametric divergence estimation between time-series samples
              from two retrospective segments. Our method uses the relative
              Pearson divergence as a divergence measure, and it is accurately
              and eﬃciently estimated by a method of direct density-ratio
              estimation. Through experiments on artiﬁcial and real-world
              datasets including human-activity sensing, speech, and Twitter
              messages, we demonstrate the usefulness of the proposed method.},
  language = {en},
  urldate = {2023-07-11},
  journal = {Neural Networks},
  author = {Liu, Song and Yamada, Makoto and Collier, Nigel and Sugiyama,
            Masashi},
  month = jul,
  year = {2013},
  note = {arXiv:1203.0453 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Statistics - Methodology},
  pages = {72--83},
  file = {Liu et al. - 2013 - Change-Point Detection in Time-Series Data by
          Rela.pdf:/home/jmei/Zotero/storage/PLP3TN6E/Liu et al. - 2013 -
          Change-Point Detection in Time-Series Data by Rela.pdf:application/pdf},
}

@misc{matteson_nonparametric_2013,
  title = {A {Nonparametric} {Approach} for {Multiple} {Change} {Point} {
           Analysis} of {Multivariate} {Data}},
  url = {http://arxiv.org/abs/1306.4933},
  abstract = {Change point analysis has applications in a wide variety of ﬁelds.
              The general problem concerns the inference of a change in
              distribution for a set of time-ordered observations. Sequential
              detection is an online version in which new data is continually
              arriving and is analyzed adaptively. We are concerned with the
              related, but distinct, oﬄine version, in which retrospective
              analysis of an entire sequence is performed. For a set of
              multivariate observations of arbitrary dimension, we consider
              nonparametric estimation of both the number of change points and
              the positions at which they occur. We do not make any assumptions
              regarding the nature of the change in distribution or any
              distribution assumptions beyond the existence of the αth absolute
              moment, for some α ∈ (0, 2). Estimation is based on hierarchical
              clustering and we propose both divisive and agglomerative
              algorithms. The divisive method is shown to provide consistent
              estimates of both the number and location of change points under
              standard regularity assumptions. We compare the proposed approach
              with competing methods in a simulation study. Methods from cluster
              analysis are applied to assess performance and to allow simple
              comparisons of location estimates, even when the estimated number
              diﬀers. We conclude with applications in genetics, ﬁnance and
              spatio-temporal analysis.},
  language = {en},
  urldate = {2023-07-11},
  publisher = {arXiv},
  author = {Matteson, David S. and James, Nicholas A.},
  month = oct,
  year = {2013},
  note = {arXiv:1306.4933 [stat]},
  keywords = {Statistics - Methodology},
  file = {Matteson and James - 2013 - A Nonparametric Approach for Multiple
          Change Point.pdf:/home/jmei/Zotero/storage/5I4GMUQW/Matteson and James
          - 2013 - A Nonparametric Approach for Multiple Change
          Point.pdf:application/pdf},
}

@article{talih_structural_2005,
  title = {Structural {Learning} with {Time}-{Varying} {Components}: {Tracking}
           the {Cross}-{Section} of {Financial} {Time} {Series}},
  volume = {67},
  issn = {1369-7412, 1467-9868},
  shorttitle = {Structural {Learning} with {Time}-{Varying} {Components}},
  url = {https://academic.oup.com/jrsssb/article/67/3/321/7109488},
  doi = {10.1111/j.1467-9868.2005.00504.x},
  abstract = {When modelling multivariate financial data, the problem of
              structural learning is compounded by the fact that the covariance
              structure changes with time. Previous work has focused on modelling
              those changes by using multivariate stochastic volatility models.
              We present an alternative to these models that focuses instead on
              the latent graphical structure that is related to the precision
              matrix. We develop a graphical model for sequences of Gaussian
              random vectors when changes in the underlying graph occur at random
              times, and a new block of data is created with the addition or
              deletion of an edge. We show how a Bayesian hierarchical model
              incorporates both the uncertainty about that graph and the time
              variation thereof.},
  language = {en},
  number = {3},
  urldate = {2023-07-11},
  journal = {Journal of the Royal Statistical Society Series B: Statistical
             Methodology},
  author = {Talih, Makram and Hengartner, Nicolas},
  month = jun,
  year = {2005},
  pages = {321--341},
  file = {
          1-s2.0-S0301051110000396-main.pdf:/home/jmei/Zotero/storage/6ATMYFTJ/1-s2.0-S0301051110000396-main.pdf:application/pdf;Talih
          and Hengartner - 2005 - Structural Learning with Time-Varying
          Components .pdf:/home/jmei/Zotero/storage/XSL98B82/Talih and Hengartner
          - 2005 - Structural Learning with Time-Varying Components
          .pdf:application/pdf},
}

@misc{belkin_understand_2018,
  title = {To understand deep learning we need to understand kernel learning},
  url = {http://arxiv.org/abs/1802.01396},
  abstract = {Generalization performance of classiﬁers in deep learning has
              recently become a subject of intense study. Deep models, which are
              typically heavily over-parametrized, tend to ﬁt the training data
              exactly. Despite this “overﬁtting", they perform well on test data,
              a phenomenon not yet fully understood.},
  language = {en},
  urldate = {2023-07-11},
  publisher = {arXiv},
  author = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  month = jun,
  year = {2018},
  note = {arXiv:1802.01396 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              },
  file = {Belkin et al. - 2018 - To understand deep learning we need to
          understand .pdf:/home/jmei/Zotero/storage/5KHXEEWE/Belkin et al. - 2018
          - To understand deep learning we need to understand
          .pdf:application/pdf},
}

@article{radhakrishnan_lecture_nodate,
  title = {Lecture 3: {Kernel} {Regression}},
  language = {en},
  author = {Radhakrishnan, Adityanarayanan and Luyten, Max Ruiz and Stefanakis,
            George and Cai, Cathy},
  file = {Radhakrishnan et al. - Lecture 3 Kernel
          Regression.pdf:/home/jmei/Zotero/storage/6GMB32L5/Radhakrishnan et al.
          - Lecture 3 Kernel Regression.pdf:application/pdf},
}

@article{lee_learning_1999,
  title = {Learning the parts of objects by non-negative matrix factorization},
  volume = {401},
  copyright = {1999 Macmillan Magazines Ltd.},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/44565},
  doi = {10.1038/44565},
  abstract = {Is perception of the whole based on perception of its parts? There
              is psychological1 and physiological2,3 evidence for parts-based
              representations in the brain, and certain computational theories of
              object recognition rely on such representations4,5. But little is
              known about how brains or computers might learn the parts of
              objects. Here we demonstrate an algorithm for non-negative matrix
              factorization that is able to learn parts of faces and semantic
              features of text. This is in contrast to other methods, such as
              principal components analysis and vector quantization, that learn
              holistic, not parts-based, representations. Non-negative matrix
              factorization is distinguished from the other methods by its use of
              non-negativity constraints. These constraints lead to a parts-based
              representation because they allow only additive, not subtractive,
              combinations. When non-negative matrix factorization is implemented
              as a neural network, parts-based representations emerge by virtue
              of two properties: the firing rates of neurons are never negative
              and synaptic strengths do not change sign.},
  language = {en},
  number = {6755},
  urldate = {2023-06-22},
  journal = {Nature},
  author = {Lee, Daniel D. and Seung, H. Sebastian},
  month = oct,
  year = {1999},
  note = {Number: 6755 Publisher: Nature Publishing Group},
  keywords = {Humanities and Social Sciences, multidisciplinary, Science},
  pages = {788--791},
  file = {Full Text PDF:/home/jmei/Zotero/storage/MBZGGFPP/Lee and Seung - 1999
          - Learning the parts of objects by non-negative
          matr.pdf:application/pdf},
}

@article{lee_algorithms_nodate,
  title = {Algorithms for {Non}-negative {Matrix} {Factorization}},
  abstract = {Non-negative matrix factorization (NMF) has previously been shown
              to be a useful decomposition for multivariate data. Two different
              multiplicative algorithms for NMF are analyzed. They differ only
              slightly in the multiplicative factor used in the update rules. One
              algorithm can be shown to minimize the conventional least squares
              error while the other minimizes the generalized Kullback-Leibler
              divergence. The monotonic convergence of both algorithms can be
              proven using an auxiliary function analogous to that used for
              proving convergence of the ExpectationMaximization algorithm. The
              algorithms can also be interpreted as diagonally rescaled gradient
              descent, where the rescaling factor is optimally chosen to ensure
              convergence.},
  language = {en},
  author = {Lee, Daniel D and Seung, H Sebastian},
  file = {Lee and Seung - Algorithms for Non-negative Matrix
          Factorization.pdf:/home/jmei/Zotero/storage/U52FXIPZ/Lee and Seung -
          Algorithms for Non-negative Matrix Factorization.pdf:application/pdf},
}

@misc{gillis_why_2014,
  title = {The {Why} and {How} of {Nonnegative} {Matrix} {Factorization}},
  url = {http://arxiv.org/abs/1401.5226},
  abstract = {Nonnegative matrix factorization (NMF) has become a widely used
              tool for the analysis of high-dimensional data as it automatically
              extracts sparse and meaningful features from a set of nonnegative
              data vectors. We ﬁrst illustrate this property of NMF on three
              applications, in image processing, text mining and hyperspectral
              imaging –this is the why. Then we address the problem of solving
              NMF, which is NP-hard in general. We review some standard NMF
              algorithms, and also present a recent subclass of NMF problems,
              referred to as near-separable NMF, that can be solved eﬃciently
              (that is, in polynomial time), even in the presence of noise –this
              is the how. Finally, we brieﬂy describe some problems in
              mathematics and computer science closely related to NMF via the
              nonnegative rank.},
  language = {en},
  urldate = {2023-06-22},
  publisher = {arXiv},
  author = {Gillis, Nicolas},
  month = mar,
  year = {2014},
  note = {arXiv:1401.5226 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Computer Science - Information Retrieval, Mathematics -
              Optimization and Control},
  file = {Gillis - 2014 - The Why and How of Nonnegative Matrix
          Factorizatio.pdf:/home/jmei/Zotero/storage/QH84G8JA/Gillis - 2014 - The
          Why and How of Nonnegative Matrix Factorizatio.pdf:application/pdf},
}

@article{wang_high_2018,
  title = {High {Dimensional} {Change} {Point} {Estimation} via {Sparse} {
           Projection}},
  volume = {80},
  issn = {1369-7412, 1467-9868},
  url = {https://academic.oup.com/jrsssb/article/80/1/57/7048412},
  doi = {10.1111/rssb.12243},
  abstract = {Change points are a very common feature of ‘big data’ that arrive
              in the form of a data stream. We study high dimensional time series
              in which, at certain time points, the mean structure changes in a
              sparse subset of the co-ordinates. The challenge is to borrow
              strength across the co-ordinates to detect smaller changes than
              could be observed in any individual component series. We propose a
              two-stage procedure called inspect for estimation of the change
              points: ﬁrst, we argue that a good projection direction can be
              obtained as the leading left singular vector of the matrix that
              solves a convex optimization problem derived from the cumulative
              sum transformation of the time series. We then apply an existing
              univariate change point estimation algorithm to the projected
              series. Our theory provides strong guarantees on both the number of
              estimated change points and the rates of convergence of their
              locations, and our numerical studies validate its highly
              competitive empirical performance for a wide range of
              data-generating mechanisms. Software implementing the methodology
              is available in the R package InspectChangepoint.},
  language = {en},
  number = {1},
  urldate = {2023-06-04},
  journal = {Journal of the Royal Statistical Society Series B: Statistical
             Methodology},
  author = {Wang, Tengyao and Samworth, Richard J.},
  month = jan,
  year = {2018},
  pages = {57--83},
  file = {Wang and Samworth - 2018 - High Dimensional Change Point Estimation
          via Spars.pdf:/home/jmei/Zotero/storage/C65IJ85E/Wang and Samworth -
          2018 - High Dimensional Change Point Estimation via
          Spars.pdf:application/pdf},
}

@article{chen_high-dimensional_2022,
  title = {High-{Dimensional}, {Multiscale} {Online} {Changepoint} {Detection}},
  volume = {84},
  issn = {1369-7412, 1467-9868},
  url = {https://academic.oup.com/jrsssb/article/84/1/234/7056123},
  doi = {10.1111/rssb.12447},
  abstract = {We introduce a new method for high-­dimensional, online
              changepoint detection in settings where a p-­variate Gaussian data
              stream may undergo a change in mean. The procedure works by
              performing likelihood ratio tests against simple alternatives of
              different scales in each coordinate, and then aggregating test
              statistics across scales and coordinates. The algorithm is online
              in the sense that both its storage requirements and worst-­ case
              computational complexity per new observation are independent of the
              number of previous observations; in practice, it may even be
              significantly faster than this. We prove that the patience, or
              average run length under the null, of our procedure is at least at
              the desired nominal level, and provide guarantees on its response
              delay under the alternative that depend on the sparsity of the
              vector of mean change. Simulations confirm the practical
              effectiveness of our proposal, which is implemented in the R
              package ocd, and we also demonstrate its utility on a seismology
              data set.},
  language = {en},
  number = {1},
  urldate = {2023-06-04},
  journal = {Journal of the Royal Statistical Society Series B: Statistical
             Methodology},
  author = {Chen, Yudong and Wang, Tengyao and Samworth, Richard J.},
  month = feb,
  year = {2022},
  pages = {234--266},
  file = {Chen et al. - 2022 - High-Dimensional, Multiscale Online Changepoint
          De.pdf:/home/jmei/Zotero/storage/EYWBH3AR/Chen et al. - 2022 -
          High-Dimensional, Multiscale Online Changepoint De.pdf:application/pdf},
}

@misc{wang_high-dimensional_2017,
  title = {High-dimensional changepoint estimation via sparse projection},
  url = {http://arxiv.org/abs/1606.06246},
  abstract = {Changepoints are a very common feature of Big Data that arrive in
              the form of a data stream. In this paper, we study high-dimensional
              time series in which, at certain time points, the mean structure
              changes in a sparse subset of the coordinates. The challenge is to
              borrow strength across the coordinates in order to detect smaller
              changes than could be observed in any individual component series.
              We propose a two-stage procedure called inspect for estimation of
              the changepoints: ﬁrst, we argue that a good projection direction
              can be obtained as the leading left singular vector of the matrix
              that solves a convex optimisation problem derived from the CUSUM
              transformation of the time series. We then apply an existing
              univariate changepoint estimation algorithm to the projected
              series. Our theory provides strong guarantees on both the number of
              estimated changepoints and the rates of convergence of their
              locations, and our numerical studies validate its highly
              competitive empirical performance for a wide range of data
              generating mechanisms. Software implementing the methodology is
              available in the R package InspectChangepoint.},
  language = {en},
  urldate = {2023-06-04},
  publisher = {arXiv},
  author = {Wang, Tengyao and Samworth, Richard J.},
  month = mar,
  year = {2017},
  note = {arXiv:1606.06246 [math, stat]},
  keywords = {Statistics - Methodology, 62H99, Mathematics - Statistics Theory},
  file = {Wang and Samworth - 2017 - High-dimensional changepoint estimation via
          sparse.pdf:/home/jmei/Zotero/storage/KXEDFTUM/Wang and Samworth - 2017
          - High-dimensional changepoint estimation via
          sparse.pdf:application/pdf},
}

@article{kwon_controlling_2017,
  title = {Controlling {Depth} of {Cellular} {Quiescence} by an {Rb}-{E2F} {
           Network} {Switch}},
  volume = {20},
  issn = {22111247},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2211124717312688},
  doi = {10.1016/j.celrep.2017.09.007},
  abstract = {Quiescence is a non-proliferative cellular state that is critical
              to tissue repair and regeneration. Although often described as the
              G0 phase, quiescence is not a single homogeneous state. As cells
              remain quiescent for longer durations, they move progressively
              deeper and display a reduced sensitivity to growth signals. Deep
              quiescent cells, unlike senescent cells, can still re-enter the
              cell cycle under physiological conditions. Mechanisms controlling
              quiescence depth are poorly understood, representing a currently
              underappreciated layer of complexity in growth control. Here, we
              show that the activation threshold of a Retinoblastoma (Rb)-E2F
              network switch controls quiescence depth. Particularly, deeper
              quiescent cells feature a higher E2F-switching threshold and
              exhibit a delayed traverse through the restriction point (R-point).
              We further show that different components of the Rb-E2F network can
              be experimentally perturbed, following computer model predictions,
              to coarse- or ﬁne-tune the E2F-switching threshold and drive cells
              into varying quiescence depths.},
  language = {en},
  number = {13},
  urldate = {2023-05-24},
  journal = {Cell Reports},
  author = {Kwon, Jungeun Sarah and Everetts, Nicholas J. and Wang, Xia and Wang
            , Weikang and Della Croce, Kimiko and Xing, Jianhua and Yao, Guang},
  month = sep,
  year = {2017},
  pages = {3223--3235},
  file = {Kwon et al. - 2017 - Controlling Depth of Cellular Quiescence by an
          Rb-.pdf:/home/jmei/Zotero/storage/FF4ZCSQ7/Kwon et al. - 2017 -
          Controlling Depth of Cellular Quiescence by an Rb-.pdf:application/pdf},
}

@techreport{lu_identifying_2022,
  type = {preprint},
  title = {Identifying {Strong} {Modulators} of {Cellular} {Quiescence} {Depth}
           {Across} {Different} {Quiescent} {Cells} and {Conditions}},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.11.19.517178},
  abstract = {The proper balance and transition between cellular quiescence and
              proliferation are critical to tissue homeostasis, repair, and
              regeneration. The likelihood of quiescence-to-proliferation
              transition is inversely correlated with quiescence depth, and deep
              quiescent cells are less likely to exit quiescence and reenter the
              cell cycle than shallow quiescent cells. The regulatory mechanisms
              of quiescence depth are poorly understood but essential for
              developing strategies against hypo- or hyper-proliferation diseases
              such as aging and cancer. Our earlier studies have demonstrated
              that the activation threshold of the bistable RbE2F gene network
              switch (ThE2F) controls quiescence depth. We have also identified
              coarse- and finetuning ThE2F modulators in rat embryonic
              fibroblasts. To examine whether other quiescent cells (including
              most adult stem and progenitor cells) under different environmental
              conditions use the same or different modulators of quiescence depth
              , here we studied the behaviors of 30,000 theoretical quiescent
              cell models that each support a functional Rb-E2F bistable switch
              with a unique parameter set. We found that although the vastly
              heterogeneous quiescent cell models exhibited no apparent parameter
              patterns, they converged at two alternative groups of strong
              quiescence-depth modulators (G1 cyclin/cdk-related and Rb/E2F
              complex-related). Our further machine learning (decision tree)
              analysis suggested that the Rb protein level and dephosphorylation
              rate in quiescent cells determine which modulator group to use to
              regulate quiescence depth.},
  language = {en},
  urldate = {2023-05-24},
  institution = {Systems Biology},
  author = {Lu, Eric and Yao, Guang},
  month = nov,
  year = {2022},
  doi = {10.1101/2022.11.19.517178},
  file = {Lu and Yao - 2022 - Identifying Strong Modulators of Cellular
          Quiescen.pdf:/home/jmei/Zotero/storage/BBJL2N4M/Lu and Yao - 2022 -
          Identifying Strong Modulators of Cellular Quiescen.pdf:application/pdf},
}

@misc{hara_making_2016,
  title = {Making {Tree} {Ensembles} {Interpretable}},
  url = {http://arxiv.org/abs/1606.05390},
  abstract = {Tree ensembles, such as random forest and boosted trees, are
              renowned for their high prediction performance, whereas their
              interpretability is critically limited. In this paper, we propose a
              post processing method that improves the model interpretability of
              tree ensembles. After learning a complex tree ensembles in a
              standard way, we approximate it by a simpler model that is
              interpretable for human. To obtain the simpler model, we derive the
              EM algorithm minimizing the KL divergence from the complex
              ensemble. A synthetic experiment showed that a complicated tree
              ensemble was approximated reasonably as interpretable.},
  language = {en},
  urldate = {2023-05-21},
  publisher = {arXiv},
  author = {Hara, Satoshi and Hayashi, Kohei},
  month = jun,
  year = {2016},
  note = {arXiv:1606.05390 [stat]},
  keywords = {Statistics - Machine Learning},
  file = {Hara and Hayashi - 2016 - Making Tree Ensembles
          Interpretable.pdf:/home/jmei/Zotero/storage/VSTMGXXZ/Hara and Hayashi -
          2016 - Making Tree Ensembles Interpretable.pdf:application/pdf},
}

@misc{domingos_every_2020,
  title = {Every {Model} {Learned} by {Gradient} {Descent} {Is} {Approximately}
           a {Kernel} {Machine}},
  url = {http://arxiv.org/abs/2012.00152},
  abstract = {Deep learning’s successes are often attributed to its ability to
              automatically discover new representations of the data, rather than
              relying on handcrafted features like other learning methods. We
              show, however, that deep networks learned by the standard gradient
              descent algorithm are in fact mathematically approximately
              equivalent to kernel machines, a learning method that simply
              memorizes the data and uses it directly for prediction via a
              similarity function (the kernel). This greatly enhances the
              interpretability of deep network weights, by elucidating that they
              are eﬀectively a superposition of the training examples. The
              network architecture incorporates knowledge of the target function
              into the kernel. This improved understanding should lead to better
              learning algorithms.},
  language = {en},
  urldate = {2023-05-21},
  publisher = {arXiv},
  author = {Domingos, Pedro},
  month = nov,
  year = {2020},
  note = {arXiv:2012.00152 [cs, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Computer Science - Neural and Evolutionary Computing, I.2.6,
              I.5.1},
  file = {Domingos - 2020 - Every Model Learned by Gradient Descent Is
          Approxi.pdf:/home/jmei/Zotero/storage/KN8JVXUX/Domingos - 2020 - Every
          Model Learned by Gradient Descent Is Approxi.pdf:application/pdf},
}

@article{haibe-kains_transparency_2020,
  title = {Transparency and reproducibility in artificial intelligence},
  volume = {586},
  issn = {0028-0836, 1476-4687},
  url = {https://www.nature.com/articles/s41586-020-2766-y},
  doi = {10.1038/s41586-020-2766-y},
  abstract = {Breakthroughs in artificial intelligence (AI) hold enormous
              potential as it can automate complex tasks and go even beyond human
              performance. In their study, McKinney et al. showed the high
              potential of AI for breast cancer screening. However, the lack of
              methods’ details and algorithm code undermines its scientific
              value. Here, we identify obstacles hindering transparent and
              reproducible AI research as faced by McKinney et al., and provide
              solutions to these obstacles with implications for the broader
              field.},
  language = {en},
  number = {7829},
  urldate = {2023-05-19},
  journal = {Nature},
  author = {Haibe-Kains, Benjamin and Adam, George Alexandru and Hosny, Ahmed
            and Khodakarami, Farnoosh and {Massive Analysis Quality Control
            (MAQC) Society Board of Directors} and Shraddha, Thakkar and Kusko,
            Rebecca and Sansone, Susanna-Assunta and Tong, Weida and Wolfinger,
            Russ D. and Mason, Christopher E. and Jones, Wendell and Dopazo,
            Joaquin and Furlanello, Cesare and Waldron, Levi and Wang, Bo and
            McIntosh, Chris and Goldenberg, Anna and Kundaje, Anshul and Greene,
            Casey S. and Broderick, Tamara and Hoffman, Michael M. and Leek,
            Jeffrey T. and Korthauer, Keegan and Huber, Wolfgang and Brazma,
            Alvis and Pineau, Joelle and Tibshirani, Robert and Hastie, Trevor
            and Ioannidis, John P. A. and Quackenbush, John and Aerts, Hugo J. W.
            L.},
  month = oct,
  year = {2020},
  pages = {E14--E16},
  file = {Haibe-Kains et al. - 2020 - Transparency and reproducibility in
          artificial int.pdf:/home/jmei/Zotero/storage/2G6W38YJ/Haibe-Kains et
          al. - 2020 - Transparency and reproducibility in artificial
          int.pdf:application/pdf},
}

@article{liang_just_2020,
  title = {Just {Interpolate}: {Kernel} "{Ridgeless}" {Regression} {Can} {
           Generalize}},
  volume = {48},
  issn = {0090-5364},
  shorttitle = {Just {Interpolate}},
  url = {http://arxiv.org/abs/1808.00387},
  doi = {10.1214/19-AOS1849},
  abstract = {In the absence of explicit regularization, Kernel “Ridgeless”
              Regression with nonlinear kernels has the potential to ﬁt the
              training data perfectly. It has been observed empirically, however,
              that such interpolated solutions can still generalize well on test
              data. We isolate a phenomenon of implicit regularization for
              minimum-norm interpolated solutions which is due to a combination
              of high dimensionality of the input data, curvature of the kernel
              function, and favorable geometric properties of the data such as an
              eigenvalue decay of the empirical covariance and kernel matrices.
              In addition to deriving a data-dependent upper bound on the
              out-of-sample error, we present experimental evidence suggesting
              that the phenomenon occurs in the MNIST dataset.},
  language = {en},
  number = {3},
  urldate = {2023-05-19},
  journal = {The Annals of Statistics},
  author = {Liang, Tengyuan and Rakhlin, Alexander},
  month = jun,
  year = {2020},
  note = {arXiv:1808.00387 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Mathematics - Statistics Theory},
  file = {Liang and Rakhlin - 2020 - Just Interpolate Kernel Ridgeless
          Regression Ca.pdf:/home/jmei/Zotero/storage/MDEVU8HY/Liang and Rakhlin
          - 2020 - Just Interpolate Kernel Ridgeless Regression
          Ca.pdf:application/pdf},
}

@misc{noauthor_reconciling_nodate,
  title = {Reconciling modern machine-learning practice and the classical
           bias–variance trade-off},
  url = {https://www.pnas.org/doi/10.1073/pnas.1903070116},
  language = {en},
  urldate = {2023-05-19},
  doi = {10.1073/pnas.1903070116},
  file = {Full Text:/home/jmei/Zotero/storage/XD689659/Reconciling modern
          machine-learning practice and
          t.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/GVKXHLGT/pnas.html:text/html
          },
}

@misc{hastie_surprises_2020,
  title = {Surprises in {High}-{Dimensional} {Ridgeless} {Least} {Squares} {
           Interpolation}},
  url = {http://arxiv.org/abs/1903.08560},
  abstract = {Interpolators—estimators that achieve zero training error—have
              attracted growing attention in machine learning, mainly because
              state-of-the art neural networks appear to be models of this type.
              In this paper, we study minimum 2 norm (“ridgeless”) interpolation
              in high-dimensional least squares regression. We consider two
              diﬀerent models for the feature distribution: a linear model, where
              the feature vectors xi ∈ Rp are obtained by applying a linear
              transform to a vector of i.i.d. entries, xi = Σ1/2zi (with zi ∈
              Rp); and a nonlinear model, where the feature vectors are obtained
              by passing the input through a random one-layer neural network, xi
              = ϕ(W zi) (with zi ∈ Rd, W ∈ Rp×d a matrix of i.i.d. entries, and ϕ
              an activation function acting componentwise on W zi). We recover—in
              a precise quantitative way—several phenomena that have been
              observed in large-scale neural networks and kernel machines,
              including the “double descent” behavior of the prediction risk, and
              the potential beneﬁts of overparametrization.},
  language = {en},
  urldate = {2023-05-19},
  publisher = {arXiv},
  author = {Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and
            Tibshirani, Ryan J.},
  month = dec,
  year = {2020},
  note = {arXiv:1903.08560 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Mathematics - Statistics Theory},
  file = {Hastie et al. - 2020 - Surprises in High-Dimensional Ridgeless Least
          Squa.pdf:/home/jmei/Zotero/storage/AWGGMF6K/Hastie et al. - 2020 -
          Surprises in High-Dimensional Ridgeless Least Squa.pdf:application/pdf},
}

@article{efron_prediction_nodate,
  title = {Prediction, {Estimation}, and {Attribution}},
  abstract = {The scientiﬁc needs and computational limitations of the Twentieth
              Century fashioned classical statistical methodology. Both the needs
              and limitations have changed in the Twenty-First, and so has the
              methodology. Large-scale prediction algorithms — neural nets, deep
              learning, boosting, support vector machines, random forests —have
              achieved star status in the popular press. They are recognizable as
              heirs to the regression tradition, but ones carried out at enormous
              scale and on titanic data sets. How do these algorithms compare
              with standard regression techniques such as ordinary least squares
              or logistic regression? Several key discrepancies will be examined,
              centering on the diﬀerences between prediction and estimation or
              prediction and attribution (signiﬁcance testing.) Most of the
              discussion is carried out through small numerical examples.},
  language = {en},
  author = {Efron, Bradley},
  file = {Efron - Prediction, Estimation, and
          Attribution.pdf:/home/jmei/Zotero/storage/BS62W25K/Efron - Prediction,
          Estimation, and Attribution.pdf:application/pdf},
}

@article{gelman_regression_nodate,
  title = {Regression and {Other} {Stories} (corrections up to 26 {Feb} 2022) {
           Please} do not reproduce in any form without permission},
  language = {en},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  file = {Gelman et al. - Regression and Other Stories (corrections up to
          26.pdf:/home/jmei/Zotero/storage/XUD5FTHT/Gelman et al. - Regression
          and Other Stories (corrections up to 26.pdf:application/pdf},
}

@book{boyd_convex_2004,
  address = {Cambridge, UK ; New York},
  title = {Convex optimization},
  isbn = {978-0-521-83378-3},
  language = {en},
  publisher = {Cambridge University Press},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  keywords = {Convex functions, Mathematical optimization},
  file = {Boyd and Vandenberghe - 2004 - Convex
          optimization.pdf:/home/jmei/Zotero/storage/ZACQHVAA/Boyd and
          Vandenberghe - 2004 - Convex optimization.pdf:application/pdf},
}

@article{garrido_methods_2014,
  title = {Methods for {Constructing} and {Assessing} {Propensity} {Scores}},
  volume = {49},
  issn = {0017-9124, 1475-6773},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/1475-6773.12182},
  doi = {10.1111/1475-6773.12182},
  language = {en},
  number = {5},
  urldate = {2023-05-12},
  journal = {Health Services Research},
  author = {Garrido, Melissa M. and Kelley, Amy S. and Paris, Julia and Roza,
            Katherine and Meier, Diane E. and Morrison, R. Sean and Aldridge,
            Melissa D.},
  month = oct,
  year = {2014},
  pages = {1701--1720},
  file = {Garrido et al. - 2014 - Methods for Constructing and Assessing
          Propensity .pdf:/home/jmei/Zotero/storage/CVYT3J2C/Garrido et al. -
          2014 - Methods for Constructing and Assessing Propensity
          .pdf:application/pdf},
}

@article{caliendo_practical_nodate,
  title = {Some {Practical} {Guidance} for the {Implementation} of {Propensity}
           {Score} {Matching}},
  language = {en},
  author = {Caliendo, Marco and Kopeinig, Sabine},
  file = {Caliendo and Kopeinig - Some Practical Guidance for the Implementation
          of .pdf:/home/jmei/Zotero/storage/HX99ULBP/Caliendo and Kopeinig - Some
          Practical Guidance for the Implementation of .pdf:application/pdf},
}

@misc{bates_cross-validation_2022,
  title = {Cross-validation: what does it estimate and how well does it do it?},
  shorttitle = {Cross-validation},
  url = {http://arxiv.org/abs/2104.00673},
  abstract = {Cross-validation is a widely-used technique to estimate prediction
              error, but its behavior is complex and not fully understood.
              Ideally, one would like to think that cross-validation estimates
              the prediction error for the model at hand, ﬁt to the training
              data. We prove that this is not the case for the linear model ﬁt by
              ordinary least squares; rather it estimates the average prediction
              error of models ﬁt on other unseen training sets drawn from the
              same population. We further show that this phenomenon occurs for
              most popular estimates of prediction error, including data
              splitting, bootstrapping, and Mallow’s Cp. Next, the standard
              conﬁdence intervals for prediction error derived from
              cross-validation may have coverage far below the desired level.
              Because each data point is used for both training and testing,
              there are correlations among the measured accuracies for each fold,
              and so the usual estimate of variance is too small. We introduce a
              nested cross-validation scheme to estimate this variance more
              accurately, and we show empirically that this modiﬁcation leads to
              intervals with approximately correct coverage in many examples
              where traditional cross-validation intervals fail.},
  language = {en},
  urldate = {2023-05-12},
  publisher = {arXiv},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  month = jul,
  year = {2022},
  note = {arXiv:2104.00673 [math, stat]},
  keywords = {Statistics - Machine Learning, Statistics - Computation,
              Statistics - Methodology, Mathematics - Statistics Theory},
  file = {Bates et al. - 2022 - Cross-validation what does it estimate and how
          we.pdf:/home/jmei/Zotero/storage/P7ZCNB4F/Bates et al. - 2022 -
          Cross-validation what does it estimate and how we.pdf:application/pdf},
}

@article{feyyad_data_1996,
  title = {Data mining and knowledge discovery: making sense out of data},
  volume = {11},
  issn = {2374-9407},
  shorttitle = {Data mining and knowledge discovery},
  doi = {10.1109/64.539013},
  abstract = {Current computing and storage technology is rapidly outstripping
              society's ability to make meaningful use of the torrent of
              available data. Without a concerted effort to develop knowledge
              discovery techniques, organizations stand to forfeit much of the
              value from the data they currently collect and store.},
  number = {5},
  journal = {IEEE Expert},
  author = {Feyyad, U.M.},
  month = oct,
  year = {1996},
  note = {Conference Name: IEEE Expert},
  keywords = {Credit cards, Data mining, Data visualization, Humans, Image
              databases, Information retrieval, Marketing and sales, Space
              vehicles, Transaction databases, TV},
  pages = {20--25},
  file = {IEEE Xplore Abstract
          Record:/home/jmei/Zotero/storage/MEPC2QGU/stamp.html:text/html;IEEE
          Xplore Full Text PDF:/home/jmei/Zotero/storage/9X7UIDTS/Feyyad - 1996 -
          Data mining and knowledge discovery making sense .pdf:application/pdf},
}

@article{simonsohn_p-curve_nodate,
  title = {P-{Curve}: {A} {Key} to the {File}-{Drawer}},
  abstract = {Because scientists tend to report only studies (publication bias)
              or analyses (p-hacking) that “work,” readers must ask, “Are these
              effects true, or do they merely reflect selective reporting?” We
              introduce p-curve as a way to answer this question. P-curve is the
              distribution of statistically significant p values for a set of
              studies (ps Ͻ .05). Because only true effects are expected to
              generate right-skewed p-curves— containing more low (.01s) than
              high (.04s) significant p values— only right-skewed p-curves are
              diagnostic of evidential value. By telling us whether we can rule
              out selective reporting as the sole explanation for a set of
              findings, p-curve offers a solution to the age-old inferential
              problems caused by file-drawers of failed studies and analyses.},
  language = {en},
  author = {Simonsohn, Uri and Nelson, Leif D and Simmons, Joseph P},
  file = {Simonsohn et al. - P-Curve A Key to the
          File-Drawer.pdf:/home/jmei/Zotero/storage/MPXGK4Y3/Simonsohn et al. -
          P-Curve A Key to the File-Drawer.pdf:application/pdf},
}

@article{stefan_big_2023,
  title = {Big little lies: a compendium and simulation of p-hacking strategies},
  volume = {10},
  shorttitle = {Big little lies},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.220346},
  doi = {10.1098/rsos.220346},
  abstract = {In many research fields, the widespread use of questionable
              research practices has jeopardized the credibility of scientific
              results. One of the most prominent questionable research practices
              is p-hacking. Typically, p-hacking is defined as a compound of
              strategies targeted at rendering non-significant hypothesis testing
              results significant. However, a comprehensive overview of these
              p-hacking strategies is missing, and current meta-scientific
              research often ignores the heterogeneity of strategies. Here, we
              compile a list of 12 p-hacking strategies based on an extensive
              literature review, identify factors that control their level of
              severity, and demonstrate their impact on false-positive rates
              using simulation studies. We also use our simulation results to
              evaluate several approaches that have been proposed to mitigate the
              influence of questionable research practices. Our results show that
              investigating p-hacking at the level of strategies can provide a
              better understanding of the process of p-hacking, as well as a
              broader basis for developing effective countermeasures. By making
              our analyses available through a Shiny app and R package, we
              facilitate future meta-scientific research aimed at investigating
              the ramifications of p-hacking across multiple strategies, and we
              hope to start a broader discussion about different manifestations
              of p-hacking in practice.},
  number = {2},
  urldate = {2023-05-02},
  journal = {Royal Society Open Science},
  author = {Stefan, Angelika M. and Schönbrodt, Felix D.},
  month = feb,
  year = {2023},
  note = {Publisher: Royal Society},
  keywords = {error rates, false-positive rate, p-curve, questionable research
              practices, Shiny app, significance, simulation},
  pages = {220346},
  file = {Full Text PDF:/home/jmei/Zotero/storage/EWWGXCD6/Stefan and Schönbrodt
          - 2023 - Big little lies a compendium and simulation of
          p-.pdf:application/pdf},
}

@article{button_power_2013,
  title = {Power failure: why small sample size undermines the reliability of
           neuroscience},
  volume = {14},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers
               Limited. All Rights Reserved.},
  issn = {1471-0048},
  shorttitle = {Power failure},
  url = {https://www.nature.com/articles/nrn3475},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific
              research; it reduces the chance of detecting a true effect.Perhaps
              less intuitively, low power also reduces the likelihood that a
              statistically significant result reflects a true effect.Empirically
              , we estimate the median statistical power of studies in the
              neurosciences is between ∼8\% and ∼31\%.We discuss the consequences
              of such low statistical power, which include overestimates of
              effect size and low reproducibility of results.There are ethical
              dimensions to the problem of low power; unreliable research is
              inefficient and wasteful.Improving reproducibility in neuroscience
              is a key priority and requires attention to well-established, but
              often ignored, methodological principles.We discuss how problems
              associated with low power can be addressed by adopting current
              best-practice and make clear recommendations for how to achieve
              this.},
  language = {en},
  number = {5},
  urldate = {2023-04-28},
  journal = {Nature Reviews Neuroscience},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire
            and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and
            Munafò, Marcus R.},
  month = may,
  year = {2013},
  note = {Number: 5 Publisher: Nature Publishing Group},
  keywords = {Molecular neuroscience},
  pages = {365--376},
  file = {Full Text PDF:/home/jmei/Zotero/storage/EEKRI26Z/Button et al. - 2013
          - Power failure why small sample size undermines th.pdf:application/pdf
          },
}

@article{gelman_garden_nodate,
  title = {The garden of forking paths: {Why} multiple comparisons can be a
           problem, even when there is no “ﬁshing expedition” or “p-hacking” and
           the research hypothesis was posited ahead of time},
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons
              problem, even in settings where researchers perform only a single
              analysis on their data. The problem is there can be a large number
              of potential comparisons when the details of data analysis are
              highly contingent on data, without the researcher having to perform
              any conscious procedure of ﬁshing or examining multiple p-values.
              We discuss in the context of several examples of published papers
              where data-analysis decisions were theoretically-motivated based on
              previous literature, but where the details of data selection and
              analysis were not pre-speciﬁed and, as a result, were contingent on
              data.},
  language = {en},
  author = {Gelman, Andrew and Loken, Eric},
  file = {Gelman and Loken - The garden of forking paths Why multiple
          comparis.pdf:/home/jmei/Zotero/storage/ZI85AM83/Gelman and Loken - The
          garden of forking paths Why multiple comparis.pdf:application/pdf},
}

@article{noauthor_2023_nodate,
  title = {2023 {Water} {Resource} {Plan}},
  language = {en},
  file = {2023 Water Resource Plan.pdf:/home/jmei/Zotero/storage/N6VUZKE8/2023
          Water Resource Plan.pdf:application/pdf},
}

@article{hoekstra_robust_2014,
  title = {Robust misinterpretation of confidence intervals},
  volume = {21},
  issn = {1069-9384, 1531-5320},
  url = {https://link.springer.com/10.3758/s13423-013-0572-3},
  doi = {10.3758/s13423-013-0572-3},
  abstract = {Null hypothesis significance testing (NHST) is undoubtedly the
              most common inferential technique used to justify claims in the
              social sciences. However, even staunch defenders of NHST agree that
              its outcomes are often misinterpreted. Confidence intervals (CIs)
              have frequently been proposed as a more useful alternative to NHST,
              and their use is strongly encouraged in the APA Manual.
              Nevertheless, little is known about how researchers interpret CIs.
              In this study, 120 researchers and 442 students—all in the field of
              psychology—were asked to assess the truth value of six particular
              statements involving different interpretations of a CI. Although
              all six statements were false, both researchers and students
              endorsed, on average, more than three statements, indicating a
              gross misunderstanding of CIs. Self-declared experience with
              statistics was not related to researchers’ performance, and, even
              more surprisingly, researchers hardly outperformed the students,
              even though the students had not received any education on
              statistical inference whatsoever. Our findings suggest that many
              researchers do not know the correct interpretation of a CI. The
              misunderstandings surrounding pvalues and CIs are particularly
              unfortunate because they constitute the main tools by which
              psychologists draw conclusions from data.},
  language = {en},
  number = {5},
  urldate = {2023-04-18},
  journal = {Psychonomic Bulletin \& Review},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and
            Wagenmakers, Eric-Jan},
  month = oct,
  year = {2014},
  pages = {1157--1164},
  file = {Hoekstra et al. - 2014 - Robust misinterpretation of confidence
          intervals.pdf:/home/jmei/Zotero/storage/42CWCFXY/Hoekstra et al. - 2014
          - Robust misinterpretation of confidence intervals.pdf:application/pdf},
}

@article{head_extent_2015,
  title = {The {Extent} and {Consequences} of {P}-{Hacking} in {Science}},
  volume = {13},
  issn = {1545-7885},
  url = {
         https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106
         },
  doi = {10.1371/journal.pbio.1002106},
  abstract = {A focus on novel, confirmatory, and statistically significant
              results leads to substantial bias in the scientific literature. One
              type of bias, known as “p-hacking,” occurs when researchers collect
              or select data or statistical analyses until nonsignificant results
              become significant. Here, we use text-mining to demonstrate that
              p-hacking is widespread throughout science. We then illustrate how
              one can test for p-hacking when performing a meta-analysis and show
              that, while p-hacking is probably common, its effect seems to be
              weak relative to the real effect sizes being measured. This result
              suggests that p-hacking probably does not drastically alter
              scientific consensuses drawn from meta-analyses.},
  language = {en},
  number = {3},
  urldate = {2023-04-18},
  journal = {PLOS Biology},
  author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T.
            and Jennions, Michael D.},
  month = mar,
  year = {2015},
  note = {Publisher: Public Library of Science},
  keywords = {Bibliometrics, Binomials, Medicine and health sciences,
              Metaanalysis, Publication ethics, Reproducibility, Statistical data
              , Test statistics},
  pages = {e1002106},
  file = {Full Text PDF:/home/jmei/Zotero/storage/YDA6TU5B/Head et al. - 2015 -
          The Extent and Consequences of P-Hacking in Scienc.pdf:application/pdf},
}

@misc{noauthor_copy_nodate,
  title = {Copy {Number} {Variation} and {Genetic} {Disease} {\textbar} {Learn}
           {Science} at {Scitable}},
  url = {
         http://www.nature.com/scitable/topicpage/copy-number-variation-and-genetic-disease-911
         },
  language = {en},
  urldate = {2023-04-01},
  note = {Cg\_cat: Copy Number Variation and Genetic Disease Cg\_level: DIF Cg\_
          topic: Copy Number Variation and Genetic Disease},
  file = {
          Snapshot:/home/jmei/Zotero/storage/YSE79KTU/copy-number-variation-and-genetic-disease-911.html:text/html
          },
}

@article{hao_super_2021,
  title = {A super scalable algorithm for short segment detection},
  volume = {13},
  issn = {1867-1764},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7963345/},
  doi = {10.1007/s12561-020-09278-z},
  abstract = {In many applications such as copy number variant (CNV) detection,
              the goal is to identify short segments on which the observations
              have different means or medians from the background. Those segments
              are usually short and hidden in a long sequence, and hence are very
              challenging to find. We study a super scalable short segment (4S)
              detection algorithm in this paper. This nonparametric method
              clusters the locations where the observations exceed a threshold
              for segment detection. It is computationally efficient and does not
              rely on Gaussian noise assumption. Moreover, we develop a framework
              to assign significance levels for detected segments. We demonstrate
              the advantages of our proposed method by theoretical, simulation,
              and real data studies.},
  number = {1},
  urldate = {2023-03-27},
  journal = {Statistics in biosciences},
  author = {Hao, Ning and Niu, Yue Selena and Xiao, Feifei and Zhang, Heping},
  month = apr,
  year = {2021},
  pmid = {33737983},
  pmcid = {PMC7963345},
  pages = {18--33},
  file = {PubMed Central Full Text PDF:/home/jmei/Zotero/storage/8LVN7L9A/Hao et
          al. - 2021 - A super scalable algorithm for short segment
          detec.pdf:application/pdf},
}

@article{baldi_autoencoders_nodate,
  title = {Autoencoders, {Unsupervised} {Learning}, and {Deep} {Architectures}},
  abstract = {Autoencoders play a fundamental role in unsupervised learning and
              in deep architectures for transfer learning and other tasks. In
              spite of their fundamental role, only linear autoencoders over the
              real numbers have been solved analytically. Here we present a
              general mathematical framework for the study of both linear and
              non-linear autoencoders. The framework allows one to derive an
              analytical treatment for the most non-linear autoencoder, the
              Boolean autoencoder. Learning in the Boolean autoencoder is
              equivalent to a clustering problem that can be solved in polynomial
              time when the number of clusters is small and becomes NP complete
              when the number of clusters is large. The framework sheds light on
              the diﬀerent kinds of autoencoders, their learning complexity,
              their horizontal and vertical composability in deep architectures,
              their critical points, and their fundamental connections to
              clustering, Hebbian learning, and information theory.},
  language = {en},
  author = {Baldi, Pierre},
  file = {Baldi - Autoencoders, Unsupervised Learning, and Deep
          Arch.pdf:/home/jmei/Zotero/storage/LCDGP6MN/Baldi - Autoencoders,
          Unsupervised Learning, and Deep Arch.pdf:application/pdf},
}

@article{zhang_detecting_2010,
  title = {Detecting simultaneous changepoints in multiple sequences},
  volume = {97},
  issn = {0006-3444},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3372242/},
  doi = {10.1093/biomet/asq025},
  abstract = {We discuss the detection of local signals that occur at the same
              location in multiple one-dimensional noisy sequences, with
              particular attention to relatively weak signals that may occur in
              only a fraction of the sequences. We propose simple scan and
              segmentation algorithms based on the sum of the chi-squared
              statistics for each individual sample, which is equivalent to the
              generalized likelihood ratio for a model where the errors in each
              sample are independent. The simple geometry of the statistic allows
              us to derive accurate analytic approximations to the significance
              level of such scans. The formulation of the model is motivated by
              the biological problem of detecting recurrent DNA copy number
              variants in multiple samples. We show using replicates and
              parent-child comparisons that pooling data across samples results
              in more accurate detection of copy number variants. We also apply
              the multisample segmentation algorithm to the analysis of a cohort
              of tumour samples containing complex nested and overlapping copy
              number aberrations, for which our method gives a sparse and
              intuitive cross-sample summary.},
  number = {3},
  urldate = {2023-02-28},
  journal = {Biometrika},
  author = {Zhang, Nancy R. and Siegmund, David O. and Ji, Hanlee and Li, Jun Z.
            },
  month = sep,
  year = {2010},
  pmid = {22822250},
  pmcid = {PMC3372242},
  pages = {631--645},
  file = {PubMed Central Full Text PDF:/home/jmei/Zotero/storage/XYA9ZPE8/Zhang
          et al. - 2010 - Detecting simultaneous changepoints in multiple
          se.pdf:application/pdf},
}

@book{shalev-shwartz_understanding_2014,
  edition = {1},
  title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
  isbn = {978-1-107-05713-5 978-1-107-29801-9},
  shorttitle = {Understanding {Machine} {Learning}},
  url = {
         https://www.cambridge.org/core/product/identifier/9781107298019/type/book
         },
  abstract = {Machine learning is one of the fastest growing areas of computer
              science, with far-reaching applications. The aim of this textbook
              is to introduce machine learning, and the algorithmic paradigms it
              offers, in a principled way. The book provides a theoretical
              account of the fundamentals underlying machine learning and the
              mathematical derivations that transform these principles into
              practical algorithms. Following a presentation of the basics, the
              book covers a wide array of central topics unaddressed by previous
              textbooks. These include a discussion of the computational
              complexity of learning and the concepts of convexity and stability;
              important algorithmic paradigms including stochastic gradient
              descent, neural networks, and structured output learning; and
              emerging theoretical concepts such as the PAC-Bayes approach and
              compression-based bounds. Designed for advanced undergraduates or
              beginning graduates, the text makes the fundamentals and algorithms
              of machine learning accessible to students and non-expert readers
              in statistics, computer science, mathematics and engineering.},
  language = {en},
  urldate = {2023-02-17},
  publisher = {Cambridge University Press},
  author = {Shalev-Shwartz, Shai and Ben-David, Shai},
  month = may,
  year = {2014},
  doi = {10.1017/CBO9781107298019},
  file = {Shalev-Shwartz and Ben-David - 2014 - Understanding Machine Learning
          From Theory to
          Alg.pdf:/home/jmei/Zotero/storage/IIM7J7VY/Shalev-Shwartz and Ben-David
          - 2014 - Understanding Machine Learning From Theory to
          Alg.pdf:application/pdf},
}

@article{fryzlewicz_wild_2014,
  title = {Wild binary segmentation for multiple change-point detection},
  volume = {42},
  issn = {0090-5364, 2168-8966},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-42/issue-6/Wild-binary-segmentation-for-multiple-change-point-detection/10.1214/14-AOS1245.full
         },
  doi = {10.1214/14-AOS1245},
  abstract = {We propose a new technique, called wild binary segmentation (WBS),
              for consistent estimation of the number and locations of multiple
              change-points in data. We assume that the number of change-points
              can increase to infinity with the sample size. Due to a certain
              random localisation mechanism, WBS works even for very short
              spacings between the change-points and/or very small jump
              magnitudes, unlike standard binary segmentation. On the other hand,
              despite its use of localisation, WBS does not require the choice of
              a window or span parameter, and does not lead to a significant
              increase in computational complexity. WBS is also easy to code. We
              propose two stopping criteria for WBS: one based on thresholding
              and the other based on what we term the ‘strengthened Schwarz
              information criterion’. We provide default recommended values of
              the parameters of the procedure and show that it offers very good
              practical performance in comparison with the state of the art. The
              WBS methodology is implemented in the R package wbs, available on
              CRAN. In addition, we provide a new proof of consistency of binary
              segmentation with improved rates of convergence, as well as a
              corresponding result for WBS.},
  number = {6},
  urldate = {2023-02-14},
  journal = {The Annals of Statistics},
  author = {Fryzlewicz, Piotr},
  month = dec,
  year = {2014},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {62G05, change-point detection, Bayesian Information Criterion,
              binary segmentation, Multiple change-points, randomised algorithms,
              thresholding},
  pages = {2243--2281},
  file = {Full Text PDF:/home/jmei/Zotero/storage/LCIYQU8N/Fryzlewicz - 2014 -
          Wild binary segmentation for multiple change-point.pdf:application/pdf},
}

@article{mnih_playing_nodate,
  title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
  abstract = {We present the ﬁrst deep learning model to successfully learn
              control policies directly from high-dimensional sensory input using
              reinforcement learning. The model is a convolutional neural network
              , trained with a variant of Q-learning, whose input is raw pixels
              and whose output is a value function estimating future rewards. We
              apply our method to seven Atari 2600 games from the Arcade Learning
              Environment, with no adjustment of the architecture or learning
              algorithm. We ﬁnd that it outperforms all previous approaches on
              six of the games and surpasses a human expert on three of them.},
  language = {en},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves,
            Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller,
            Martin},
  file = {Mnih et al. - Playing Atari with Deep Reinforcement
          Learning.pdf:/home/jmei/Zotero/storage/ANKWATPL/Mnih et al. - Playing
          Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@techreport{rosenblatt_perceptron_1957,
  address = {Ithaca, New York},
  title = {The {Perceptron} - {A} {Perceiving} and {Recognizing} {Automaton}},
  url = {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
  number = {85-460-1},
  urldate = {2023-02-03},
  author = {Rosenblatt, Frank},
  year = {1957},
  pages = {33},
  file = {The Perceptron - A Perceiving and Recognizing
          Automaton:/home/jmei/Zotero/storage/WCMHRJ7E/The Perceptron - A
          Perceiving and Recognizing Automaton.pdf:application/pdf},
}

@article{huber_projection_1985,
  title = {Projection {Pursuit}},
  volume = {13},
  issn = {0090-5364, 2168-8966},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-13/issue-2/Projection-Pursuit/10.1214/aos/1176349519.full
         },
  doi = {10.1214/aos/1176349519},
  abstract = {Projection pursuit is concerned with "interesting" projections of
              high dimensional data sets, with finding such projections by
              machine, and with using them for nonparametric fitting and other
              data-analytic purposes. This survey attempts to put the fascinating
              problems and ramifications of projection pursuit--which range from
              principal components, multidimensional scaling, factor analysis,
              nonparametric regression, density estimation and deconvolution of
              time series to computer tomography and problems in pure
              mathematics--into a coherent perspective},
  number = {2},
  urldate = {2023-02-03},
  journal = {The Annals of Statistics},
  author = {Huber, Peter J.},
  month = jun,
  year = {1985},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {62H99, Computer tomography, minimum entropy, multivariate data
              analysis, principal components, Projection pursuit, robust
              multivariate methods},
  pages = {435--475},
  file = {Full Text PDF:/home/jmei/Zotero/storage/ZICWZCHM/Huber - 1985 -
          Projection Pursuit.pdf:application/pdf},
}

@article{zhang_multiple_nodate,
  title = {Multiple {Change}-{Point} {Detection} and {Analysis} of {Chromosome}
           {Copy} {Number} {Variations}},
  language = {en},
  author = {Zhang, Heping},
  file = {Zhang - Multiple Change-Point Detection and Analysis of
          Ch.pdf:/home/jmei/Zotero/storage/CQEDVSIN/Zhang - Multiple Change-Point
          Detection and Analysis of Ch.pdf:application/pdf},
}

@article{xiao_accurate_2019,
  title = {An accurate and powerful method for copy number variation detection},
  volume = {35},
  issn = {1367-4803, 1460-2059},
  url = {https://academic.oup.com/bioinformatics/article/35/17/2891/5288773},
  doi = {10.1093/bioinformatics/bty1041},
  abstract = {Motivation: Integration of multiple genetic sources for copy
              number variation detection (CNV) is a powerful approach to improve
              the identiﬁcation of variants associated with complex traits.
              Although it has been shown that the widely used change point based
              methods can increase statistical power to identify variants, it
              remains challenging to effectively detect CNVs with weak signals
              due to the noisy nature of genotyping intensity data. We previously
              developed modSaRa, a normal mean-based model on a screening and
              ranking algorithm for copy number variation identiﬁcation which
              presented desirable sensitivity with high computational efﬁciency.
              To boost statistical power for the identiﬁcation of variants, here
              we present a novel improvement that integrates the relative allelic
              intensity with external information from empirical statistics with
              modeling, which we called modSaRa2.},
  language = {en},
  number = {17},
  urldate = {2023-02-02},
  journal = {Bioinformatics},
  author = {Xiao, Feifei and Luo, Xizhi and Hao, Ning and Niu, Yue S and Xiao,
            Xiangjun and Cai, Guoshuai and Amos, Christopher I and Zhang, Heping},
  editor = {Hancock, John},
  month = sep,
  year = {2019},
  pages = {2891--2898},
  file = {Xiao et al. - 2019 - An accurate and powerful method for copy number
          va.pdf:/home/jmei/Zotero/storage/VHNNXW62/Xiao et al. - 2019 - An
          accurate and powerful method for copy number va.pdf:application/pdf},
}

@article{zhang_survey_2021,
  title = {A {Survey} on {Neural} {Network} {Interpretability}},
  volume = {5},
  issn = {2471-285X},
  url = {http://arxiv.org/abs/2012.14261},
  doi = {10.1109/TETCI.2021.3100641},
  abstract = {Along with the great success of deep neural networks, there is
              also growing concern about their black-box nature. The
              interpretability issue affects people’s trust on deep learning
              systems. It is also related to many ethical problems, e.g.,
              algorithmic discrimination. Moreover, interpretability is a desired
              property for deep networks to become powerful tools in other
              research ﬁelds, e.g., drug discovery and genomics. In this survey,
              we conduct a comprehensive review of the neural network
              interpretability research. We ﬁrst clarify the deﬁnition of
              interpretability as it has been used in many different contexts.
              Then we elaborate on the importance of interpretability and propose
              a novel taxonomy organized along three dimensions: type of
              engagement (passive vs. active interpretation approaches), the type
              of explanation, and the focus (from local to global
              interpretability). This taxonomy provides a meaningful 3D view of
              distribution of papers from the relevant literature as two of the
              dimensions are not simply categorical but allow ordinal
              subcategories. Finally, we summarize the existing interpretability
              evaluation methods and suggest possible research directions
              inspired by our new taxonomy.},
  language = {en},
  number = {5},
  urldate = {2023-01-23},
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  author = {Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
  month = oct,
  year = {2021},
  note = {arXiv:2012.14261 [cs]},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial
              Intelligence},
  pages = {726--742},
  file = {Zhang et al. - 2021 - A Survey on Neural Network
          Interpretability.pdf:/home/jmei/Zotero/storage/8BAYMAB7/Zhang et al. -
          2021 - A Survey on Neural Network Interpretability.pdf:application/pdf},
}

@misc{bengio_representation_2014,
  title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
  shorttitle = {Representation {Learning}},
  url = {http://arxiv.org/abs/1206.5538},
  abstract = {The success of machine learning algorithms generally depends on
              data representation, and we hypothesize that this is because
              different representations can entangle and hide more or less the
              different explanatory factors of variation behind the data.
              Although specific domain knowledge can be used to help design
              representations, learning with generic priors can also be used, and
              the quest for AI is motivating the design of more powerful
              representation-learning algorithms implementing such priors. This
              paper reviews recent work in the area of unsupervised feature
              learning and deep learning, covering advances in probabilistic
              models, auto-encoders, manifold learning, and deep networks. This
              motivates longer-term unanswered questions about the appropriate
              objectives for learning good representations, for computing
              representations (i.e., inference), and the geometrical connections
              between representation learning, density estimation and manifold
              learning.},
  language = {en},
  urldate = {2023-01-23},
  publisher = {arXiv},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  month = apr,
  year = {2014},
  note = {arXiv:1206.5538 [cs]},
  keywords = {Computer Science - Machine Learning},
  file = {Bengio et al. - 2014 - Representation Learning A Review and New
          Perspect.pdf:/home/jmei/Zotero/storage/78M3P9YD/Bengio et al. - 2014 -
          Representation Learning A Review and New Perspect.pdf:application/pdf},
}

@book{montgomery_design_2013,
  address = {Hoboken, NJ},
  edition = {Eighth edition},
  title = {Design and analysis of experiments},
  isbn = {978-1-118-14692-7},
  abstract = {"The eighth edition of Design and Analysis of Experiments
              continues to provide extensive and in-depth information on
              engineering, business, and statistics-as well as informative ways
              to help readers design and analyze experiments for improving the
              quality, efficiency and performance of working systems. Furthermore
              , the text maintains its comprehensive coverage by including: new
              examples, exercises, and problems (including in the areas of
              biochemistry and biotechnology); new topics and problems in the
              area of response surface; new topics in nested and split-plot
              design; and the residual maximum likelihood method is now
              emphasized throughout the book"--},
  language = {en},
  publisher = {John Wiley \& Sons, Inc},
  author = {Montgomery, Douglas C.},
  year = {2013},
  keywords = {Experimental design, TECHNOLOGY \& ENGINEERING / Industrial
              Engineering},
  file = {Montgomery - 2013 - Design and analysis of
          experiments.pdf:/home/jmei/Zotero/storage/4IBM988C/Montgomery - 2013 -
          Design and analysis of experiments.pdf:application/pdf},
}

@misc{brown_language_2020,
  title = {Language {Models} are {Few}-{Shot} {Learners}},
  url = {http://arxiv.org/abs/2005.14165},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks
              and benchmarks by pre-training on a large corpus of text followed
              by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in
              architecture, this method still requires task-speciﬁc ﬁne-tuning
              datasets of thousands or tens of thousands of examples. By contrast
              , humans can generally perform a new language task from only a few
              examples or from simple instructions – something which current NLP
              systems still largely struggle to do. Here we show that scaling up
              language models greatly improves task-agnostic, few-shot
              performance, sometimes even reaching competitiveness with prior
              state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3,
              an autoregressive language model with 175 billion parameters, 10x
              more than any previous non-sparse language model, and test its
              performance in the few-shot setting. For all tasks, GPT-3 is
              applied without any gradient updates or ﬁne-tuning, with tasks and
              few-shot demonstrations speciﬁed purely via text interaction with
              the model. GPT-3 achieves strong performance on many NLP datasets,
              including translation, question-answering, and cloze tasks, as well
              as several tasks that require on-the-ﬂy reasoning or domain
              adaptation, such as unscrambling words, using a novel word in a
              sentence, or performing 3-digit arithmetic. At the same time, we
              also identify some datasets where GPT-3’s few-shot learning still
              struggles, as well as some datasets where GPT-3 faces
              methodological issues related to training on large web corpora.
              Finally, we ﬁnd that GPT-3 can generate samples of news articles
              which human evaluators have difﬁculty distinguishing from articles
              written by humans. We discuss broader societal impacts of this
              ﬁnding and of GPT-3 in general.},
  language = {en},
  urldate = {2023-01-08},
  publisher = {arXiv},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah,
            Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan,
            Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and
            Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and
            Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel
            M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and
            Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and
            Chess, Benjamin and Clark, Jack and Berner, Christopher and
            McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei,
            Dario},
  month = jul,
  year = {2020},
  note = {arXiv:2005.14165 [cs]},
  keywords = {Computer Science - Computation and Language},
  file = {Brown et al. - 2020 - Language Models are Few-Shot
          Learners.pdf:/home/jmei/Zotero/storage/R2LX4UIG/Brown et al. - 2020 -
          Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{march_16_moores_nodate,
  title = {Moore's {Law} for {Everything}},
  url = {https://moores.samaltman.com/},
  abstract = {We need to design a system that embraces this technological future
              and taxes the assets that will make up most of the value in that
              world–companies and land–in order to fairly distribute some of the
              coming wealth.},
  urldate = {2023-01-08},
  author = {March 16, Sam Altman · and {2021}},
  file = {
          Snapshot:/home/jmei/Zotero/storage/27WMGXEA/moores.samaltman.com.html:text/html
          },
}

@article{sculley_machine_nodate,
  title = {Machine {Learning}: {The} {High}-{Interest} {Credit} {Card} of {
           Technical} {Debt}},
  abstract = {Machine learning offers a fantastically powerful toolkit for
              building complex systems quickly. This paper argues that it is
              dangerous to think of these quick wins as coming for free. Using
              the framework of technical debt, we note that it is remarkably easy
              to incur massive ongoing maintenance costs at the system level when
              applying machine learning. The goal of this paper is highlight
              several machine learning speciﬁc risk factors and design patterns
              to be avoided or refactored where possible. These include boundary
              erosion, entanglement, hidden feedback loops, undeclared consumers,
              data dependencies, changes in the external world, and a variety of
              system-level anti-patterns.},
  language = {en},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene
            and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young,
            Michael},
  file = {Sculley et al. - Machine Learning The High-Interest Credit Card
          of.pdf:/home/jmei/Zotero/storage/SV6RH7CR/Sculley et al. - Machine
          Learning The High-Interest Credit Card of.pdf:application/pdf},
}

@misc{hao_equivariant_2022,
  title = {Equivariant {Variance} {Estimation} for {Multiple} {Change}-point {
           Model}},
  url = {http://arxiv.org/abs/2108.09431},
  abstract = {The variance of noise plays an important role in many change-point
              detection procedures and the associated inferences. Most commonly
              used variance estimators require strong assumptions on the true
              mean structure or normality of the error distribution, which may
              not hold in applications. More importantly, the qualities of these
              estimators have not been discussed systematically in the
              literature. In this paper, we introduce a framework of equivariant
              variance estimation for multiple change-point models. In particular
              , we characterize the set of all equivariant unbiased quadratic
              variance estimators for a family of change-point model classes, and
              develop a minimax theory for such estimators.},
  language = {en},
  urldate = {2022-12-19},
  publisher = {arXiv},
  author = {Hao, Ning and Niu, Yue Selena and Xiao, Han},
  month = apr,
  year = {2022},
  note = {arXiv:2108.09431 [math, stat]},
  keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
  file = {Hao et al. - 2022 - Equivariant Variance Estimation for Multiple
          Chang.pdf:/home/jmei/Zotero/storage/UECSTCD3/Hao et al. - 2022 -
          Equivariant Variance Estimation for Multiple Chang.pdf:application/pdf},
}

@article{lin_component_2006,
  title = {Component {Selection} and {Smoothing} in {Multivariate} {
           Nonparametric} {Regression}},
  volume = {34},
  doi = {10.1214/009053606000000722},
  abstract = {We propose a new method for model selection and model fitting in
              multivariate nonparametric regression models, in the framework of
              smoothing spline ANOVA. The "COSSO" is a method of regularization
              with the penalty functional being the sum of component norms,
              instead of the squared norm employed in the traditional smoothing
              spline method. The COSSO provides a unified framework for several
              recent proposals for model selection in linear models and smoothing
              spline ANOVA models. Theoretical properties, such as the existence
              and the rate of convergence of the COSSO estimator, are studied. In
              the special case of a tensor product design with periodic functions
              , a detailed analysis reveals that the COSSO does model selection
              by applying a novel soft thresholding type operation to the
              function components. We give an equivalent formulation of the COSSO
              estimator which leads naturally to an iterative algorithm. We
              compare the COSSO with MARS, a popular method that builds
              functional ANOVA models, in simulations and real examples. The
              COSSO method can be extended to classification problems and we
              compare its performance with those of a number of machine learning
              algorithms on real datasets. The COSSO gives very competitive
              performance in these studies.},
  number = {5},
  urldate = {2021-11-14},
  journal = {The Annals of Statistics},
  author = {Lin, Yi and Zhang, Hao Helen},
  year = {2006},
  pages = {2272--2297},
  file = {PDF:/home/jmei/Zotero/storage/SGM5EUWM/full-text.pdf:application/pdf},
}

@book{gu_smoothing_2002,
  title = {Smoothing {Spline} {ANOVA} {Models}},
  isbn = {978-1-4419-2966-2},
  publisher = {Springe},
  author = {Gu, Chong},
  year = {2002},
  file = {PDF:/home/jmei/Zotero/storage/FZWVBAN8/Bickel et al. - 2002 - Springer
          Series in Statistics Springer Science Business Media ,
          LLC.pdf:application/pdf},
}

@article{noauthor_sparse_nodate,
  title = {Sparse {Additive} {Machines}},
  url = {http://proceedings.mlr.press/v22/zhao12/zhao12.pdf},
  urldate = {2022-12-11},
  file = {
          zhao12.pdf:/home/jmei/Zotero/storage/H6BS3SF6/zhao12.pdf:application/pdf
          },
}

@article{tibshirani_regression_1996,
  title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
  volume = {58},
  issn = {00359246},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1996.tb02080.x},
  doi = {10.1111/j.2517-6161.1996.tb02080.x},
  abstract = {We propose a new method for estimation in linear models. The
              'lasso' minim residual sum of squares subject to the sum of the
              absolute value of the coefficients than a constant. Because of the
              nature of this constraint it tends to produce coefficients that are
              exactly 0 and hence gives interpretable models. Our simulatio
              suggest that the lasso enjoys some of the favourable properties of
              both subset sele ridge regression. It produces interpretable models
              like subset selection and exh stability of ridge regression. There
              is also an interesting relationship with recent adaptive function
              estimation by Donoho and Johnstone. The lasso idea is quite ge can
              be applied in a variety of statistical models: extensions to
              generalized regressio and tree-based models are briefly described.},
  language = {en},
  number = {1},
  urldate = {2022-12-08},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)
             },
  author = {Tibshirani, Robert},
  month = jan,
  year = {1996},
  pages = {267--288},
  file = {Tibshirani - 1996 - Regression Shrinkage and Selection Via the
          Lasso.pdf:/home/jmei/Zotero/storage/EUCMIGEF/Tibshirani - 1996 -
          Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@article{olshen_circular_2004,
  title = {Circular binary segmentation for the analysis of array-based {DNA}
           copy number data},
  volume = {5},
  issn = {1465-4644, 1468-4357},
  url = {
         https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/kxh008
         },
  doi = {10.1093/biostatistics/kxh008},
  abstract = {DNA sequence copy number is the number of copies of DNA at a
              region of a genome. Cancer progression often involves alterations
              in DNA copy number. Newly developed microarray technologies enable
              simultaneous measurement of copy number at thousands of sites in a
              genome. We have developed a modiﬁcation of binary segmentation,
              which we call circular binary segmentation, to translate noisy
              intensity measurements into regions of equal copy number. The
              method is evaluated by simulation and is demonstrated on cell line
              data with known copy number alterations and on a breast cancer cell
              line data set.},
  language = {en},
  number = {4},
  urldate = {2022-12-08},
  journal = {Biostatistics},
  author = {Olshen, A. B. and Venkatraman, E. S. and Lucito, R. and Wigler, M.},
  month = oct,
  year = {2004},
  pages = {557--572},
  file = {Olshen et al. - 2004 - Circular binary segmentation for the analysis
          of a.pdf:/home/jmei/Zotero/storage/6C6GAQ9C/Olshen et al. - 2004 -
          Circular binary segmentation for the analysis of a.pdf:application/pdf},
}

@article{zou_regularization_2005,
  title = {Regularization and variable selection via the elastic net},
  volume = {67},
  issn = {1369-7412, 1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  abstract = {We propose the elastic net, a new regularization and variable
              selection method. Real world data and a simulation study show that
              the elastic net often outperforms the lasso, while enjoying a
              similar sparsity of representation. In addition, the elastic net
              encourages a grouping effect, where strongly correlated predictors
              tend to be in or out of the model together. The elastic net is
              particularly useful when the number of predictors (p) is much
              bigger than the number of observations (n). By contrast, the lasso
              is not a very satisfactory variable selection method in the p n
              case. An algorithm called LARS-EN is proposed for computing elastic
              net regularization paths efﬁciently, much like algorithm LARS does
              for the lasso.},
  language = {en},
  number = {2},
  urldate = {2022-12-07},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical
             Methodology)},
  author = {Zou, Hui and Hastie, Trevor},
  month = apr,
  year = {2005},
  pages = {301--320},
  file = {Zou and Hastie - 2005 - Regularization and variable selection via the
          elas.pdf:/home/jmei/Zotero/storage/LA3S5U8I/Zou and Hastie - 2005 -
          Regularization and variable selection via the elas.pdf:application/pdf},
}

@article{ravikumar_sparse_2009,
  title = {Sparse additive models},
  volume = {71},
  issn = {13697412, 14679868},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2009.00718.x},
  doi = {10.1111/j.1467-9868.2009.00718.x},
  abstract = {We present a new class of methods for high dimensional
              non-parametric regression and classiﬁcation called sparse additive
              models. Our methods combine ideas from sparse linear modelling and
              additive non-parametric regression. We derive an algorithm for
              ﬁtting the models that is practical and effective even when the
              number of covariates is larger than the sample size. Sparse
              additive models are essentially a functional version of the grouped
              lasso of Yuan and Lin. They are also closely related to the COSSO
              model of Lin and Zhang but decouple smoothing and sparsity,
              enabling the use of arbitrary non-parametric smoothers. We give an
              analysis of the theoretical properties of sparse additive models
              and present empirical results on synthetic and real data, showing
              that they can be effective in ﬁtting sparse non-parametric models
              in high dimensional data.},
  language = {en},
  number = {5},
  urldate = {2022-12-06},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical
             Methodology)},
  author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman,
            Larry},
  month = nov,
  year = {2009},
  pages = {1009--1030},
  file = {Ravikumar et al. - 2009 - Sparse additive
          models.pdf:/home/jmei/Zotero/storage/WMPRNGNR/Ravikumar et al. - 2009 -
          Sparse additive models.pdf:application/pdf},
}

@article{hastie_generalized_1986,
  title = {Generalized {Additive} {Models}},
  volume = {1},
  abstract = {Likelihood-based regression models such as the normal linear
              regression model and the linear logistic model, assume a linear (or
              some other parametric) form for the covariates Xlt X2, ■•-, Xp. We
              introduce the class of generalized additive models which replaces
              the linear form 2 (IjXj by a sum of smooth functions \_£ \$j(Xj).
              The Sj(-)'s are unspecified functions that are estimated using a
              scatterplot smoother, in an iterative procedure we call the local
              scoring algorithm. The technique is applicable to any
              likelihood-based regression model: the class of generalized linear
              models contains many of these. In this class the linear predictor
              tj = £ fyXj is replaced by the additive predictor £ Sj(Xj); hence,
              the name generalized additive models. We illustrate the technique
              with binary response and survival data. In both cases, the method
              proves to be useful in uncovering nonlinear covariate effects. It
              has the advantage of being completely auto matic, i.e., no "
              detective work" is needed on the part of the statistician. As a
              theoretical underpinning, the technique is viewed as an empirical
              method of maximizing the expected log likelihood, or equivalently,
              of minimizing the Kullback-Leibler distance to the true model.},
  language = {en},
  number = {3},
  journal = {Statistical Science},
  author = {Hastie, Trevor and Tibshirani, Robert},
  year = {1986},
  pages = {297--318},
  file = {Hastie and Tibshirani - Generalized Additive
          Models.pdf:/home/jmei/Zotero/storage/GGJBSWYW/Hastie and Tibshirani -
          Generalized Additive Models.pdf:application/pdf},
}

@article{liu_spam_nodate,
  title = {{SpAM}: {Sparse} {Additive} {Models}},
  abstract = {We present a new class of models for high-dimensional
              nonparametric regression and classiﬁcation called sparse additive
              models (SpAM). Our methods combine ideas from sparse linear
              modeling and additive nonparametric regression. We derive a method
              for ﬁtting the models that is effective even when the number of
              covariates is larger than the sample size. A statistical analysis
              of the properties of SpAM is given together with empirical results
              on synthetic and real data, showing that SpAM can be effective in
              ﬁtting sparse nonparametric models in high dimensional data.},
  language = {en},
  author = {Liu, Han and Wasserman, Larry and Lafferty, John D and Ravikumar,
            Pradeep K},
  pages = {8},
  file = {Liu et al. - SpAM Sparse Additive
          Models.pdf:/home/jmei/Zotero/storage/D5CN6ZDN/Liu et al. - SpAM Sparse
          Additive Models.pdf:application/pdf},
}

@article{hao_multiple_2013,
  title = {Multiple change-point detection via a screening and ranking algorithm
           },
  issn = {10170405},
  url = {http://www3.stat.sinica.edu.tw/statistica/J23N4/J23N47/J23N47.html},
  doi = {10.5705/ss.2012.018s},
  abstract = {Let Y1, …, Yn be a sequence whose underlying mean is a step
              function with an unknown number of the steps and unknown change
              points. The detection of the change points, namely the positions
              where the mean changes, is an important problem in such fields as
              engineering, economics, climatology and bioscience. This problem
              has attracted a lot of attention in statistics, and a variety of
              solutions have been proposed and implemented. However, there is
              scant literature on the theoretical properties of those algorithms.
              Here, we investigate a recently developed algorithm called the
              Screening and Ranking Algorithm (SaRa). We characterize the
              theoretical properties of SaRa and show its superiority over other
              commonly used algorithms. In particular, we develop a false
              discovery rate approach to the multiple change-point problem and
              show a strong sure coverage property for the SaRa.},
  language = {en},
  urldate = {2022-12-03},
  journal = {Statistica Sinica},
  author = {Hao, Ning and Selena Niu, Yue and Zhang, Heping},
  year = {2013},
  file = {Hao et al. - 2013 - Multiple change-point detection via a screening
          an.pdf:/home/jmei/Zotero/storage/3T5ITANF/Hao et al. - 2013 - Multiple
          change-point detection via a screening an.pdf:application/pdf},
}

@article{nosedal-sanchez_reproducing_2012,
  title = {Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {Regression}:
           {A} {Tutorial}},
  volume = {66},
  issn = {0003-1305, 1537-2731},
  shorttitle = {Reproducing {Kernel} {Hilbert} {Spaces} for {Penalized} {
                Regression}},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.2012.678196},
  doi = {10.1080/00031305.2012.678196},
  language = {en},
  number = {1},
  urldate = {2022-12-03},
  journal = {The American Statistician},
  author = {Nosedal-Sanchez, Alvaro and Storlie, Curtis B. and Lee, Thomas C.M.
            and Christensen, Ronald},
  month = feb,
  year = {2012},
  pages = {50--60},
  file = {Nosedal-Sanchez et al. - 2012 - Reproducing Kernel Hilbert Spaces for
          Penalized Re.pdf:/home/jmei/Zotero/storage/DWYKGSDX/Nosedal-Sanchez et
          al. - 2012 - Reproducing Kernel Hilbert Spaces for Penalized
          Re.pdf:application/pdf},
}

@article{watson_reproducing_nodate,
  title = {Reproducing {Kernel} {Hilbert} {Spaces} and {Polynomial} {Smoothing}
           {Splines}},
  language = {en},
  author = {Watson, Sam},
  pages = {4},
  file = {Watson - Reproducing Kernel Hilbert Spaces and Polynomial
          S.pdf:/home/jmei/Zotero/storage/9XWJK8EN/Watson - Reproducing Kernel
          Hilbert Spaces and Polynomial S.pdf:application/pdf},
}

@article{lee_exact_2016,
  title = {Exact post-selection inference, with application to the lasso},
  volume = {44},
  issn = {0090-5364},
  url = {http://arxiv.org/abs/1311.6238},
  doi = {10.1214/15-AOS1371},
  abstract = {We develop a general approach to valid inference after model
              selection. At the core of our framework is a result that
              characterizes the distribution of a post-selection estimator
              conditioned on the selection event. We specialize the approach to
              model selection by the lasso to form valid confidence intervals for
              the selected coefficients and test whether all relevant variables
              have been included in the model.},
  language = {en},
  number = {3},
  urldate = {2022-12-01},
  journal = {The Annals of Statistics},
  author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor,
            Jonathan E.},
  month = jun,
  year = {2016},
  note = {arXiv:1311.6238 [math, stat]},
  keywords = {Statistics - Machine Learning, Statistics - Methodology,
              Mathematics - Statistics Theory},
  file = {Lee et al. - 2016 - Exact post-selection inference, with application
          t.pdf:/home/jmei/Zotero/storage/9JDR47X7/Lee et al. - 2016 - Exact
          post-selection inference, with application t.pdf:application/pdf},
}

@article{zou_regularization_2005-1,
  title = {Regularization and variable selection via the elastic net},
  volume = {67},
  issn = {1369-7412, 1467-9868},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2005.00503.x},
  doi = {10.1111/j.1467-9868.2005.00503.x},
  abstract = {We propose the elastic net, a new regularization and variable
              selection method. Real world data and a simulation study show that
              the elastic net often outperforms the lasso, while enjoying a
              similar sparsity of representation. In addition, the elastic net
              encourages a grouping effect, where strongly correlated predictors
              tend to be in or out of the model together. The elastic net is
              particularly useful when the number of predictors (p) is much
              bigger than the number of observations (n). By contrast, the lasso
              is not a very satisfactory variable selection method in the p n
              case. An algorithm called LARS-EN is proposed for computing elastic
              net regularization paths efﬁciently, much like algorithm LARS does
              for the lasso.},
  language = {en},
  number = {2},
  urldate = {2022-11-29},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical
             Methodology)},
  author = {Zou, Hui and Hastie, Trevor},
  month = apr,
  year = {2005},
  pages = {301--320},
  file = {Zou and Hastie - 2005 - Regularization and variable selection via the
          elas.pdf:/home/jmei/Zotero/storage/T5AHSDBH/Zou and Hastie - 2005 -
          Regularization and variable selection via the elas.pdf:application/pdf},
}

@inproceedings{taylor_selective_2019,
  address = {Rio de Janeiro, Brazil},
  title = {A {SELECTIVE} {SURVEY} {OF} {SELECTIVE} {INFERENCE}},
  isbn = {978-981-327-287-3 978-981-327-288-0},
  url = {https://www.worldscientific.com/doi/abs/10.1142/9789813272880_0170},
  doi = {10.1142/9789813272880_0170},
  abstract = {It is not difficult to find stories of a crisis in modern science,
              either in the popular press or in the scientific literature. There
              are likely multiple sources for this crisis. It is also well
              documented that one source of this crisis is the misuse of
              statistical methods in science, with the P -value receiving its
              fair share of criticism. It could be argued that this misuse of
              statistical methods is caused by a shift in how data is used in
              21st century science compared to its use in the mid-20th century
              which presumed scientists had formal statistical hypotheses before
              collecting data. With the advent of sophisticated statistical
              software available to anybody this paradigm has been shifted to one
              in which scientists collect data first and ask questions later.},
  language = {en},
  urldate = {2022-11-29},
  booktitle = {Proceedings of the {International} {Congress} of {Mathematicians}
               ({ICM} 2018)},
  publisher = {WORLD SCIENTIFIC},
  author = {Taylor, Jonathan E.},
  month = may,
  year = {2019},
  pages = {3019--3038},
  file = {Taylor - 2019 - A SELECTIVE SURVEY OF SELECTIVE
          INFERENCE.pdf:/home/jmei/Zotero/storage/VXCGPAEH/Taylor - 2019 - A
          SELECTIVE SURVEY OF SELECTIVE INFERENCE.pdf:application/pdf},
}

@article{noauthor_notitle_nodate,
}

@misc{liu_black-box_2022,
  title = {Black-box {Selective} {Inference} via {Bootstrapping}},
  url = {http://arxiv.org/abs/2203.14504},
  abstract = {We propose a method for selective inference after a model
              selection procedure that is potentially a black box. In the
              conditional post-selection inference framework, a crucial quantity
              in determining the post-selection distribution of a test statistic
              is the probability of selecting the model conditional on the
              statistic. By repeatedly running the model selection procedure on
              bootstrapped datasets, we can generate training data with binary
              responses indicating selection event as well as specially designed
              covariates, which are then used to learn the selection probability.
              We prove that the constructed conﬁdence intervals are
              asymptotically valid if we can learn the selection probability
              suﬃciently well around a neighborhood of the target parameter. The
              validity of the proposed algorithm is veriﬁed by several examples.},
  language = {en},
  urldate = {2022-11-29},
  publisher = {arXiv},
  author = {Liu, Sifan and Markovic, Jelena and Taylor, Jonathan},
  month = mar,
  year = {2022},
  note = {arXiv:2203.14504 [stat]},
  keywords = {Statistics - Machine Learning, Statistics - Methodology},
  file = {Liu et al. - 2022 - Black-box Selective Inference via
          Bootstrapping.pdf:/home/jmei/Zotero/storage/KTWGQRMD/Liu et al. - 2022
          - Black-box Selective Inference via Bootstrapping.pdf:application/pdf},
}

@article{hyun_post-selection_2018,
  title = {Post-{Selection} {Inference} for {Changepoint} {Detection} {
           Algorithms} with {Application} to {Copy} {Number} {Variation} {Data}},
  abstract = {Changepoint detection methods are used in many areas of science
              and engineering, e.g., in the analysis of copy number variation
              data, to detect abnormalities in copy numbers along the genome.
              Despite the broad array of available tools, methodology for
              quantifying our uncertainty in the strength (or presence) of given
              changepoints, post-detection, are lacking. Post-selection inference
              offers a framework to fill this gap, but the most straightforward
              application of these methods results in low-powered tests and
              leaves open several important questions about practical usability.
              In this work, we carefully tailor post-selection inference methods
              towards changepoint detection, focusing as our main scientific
              application on copy number variation data. As for changepoint
              algorithms, we study binary segmentation, and two of its most
              popular variants, wild and circular, and the fused lasso. We
              implement some of the latest developments in post-selection
              inference theory: we use auxiliary randomization to improve power,
              which requires implementations of MCMC algorithms (importance
              sampling and hit-and-run sampling) to carry out our tests. We also
              provide recommendations for improving practical useability,
              detailed simulations, and an example analysis on array comparative
              genomic hybridization (CGH) data.},
  urldate = {2022-10-01},
  author = {Hyun˚,, Sangwon and Lin, Kevin and G'sell, Max and Tibshirani, Ryan
            J},
  year = {2018},
  note = {arXiv: 1812.03644v1},
  keywords = {CGH analysis, changepoint detection, copy number variation,
              hypothesis tests, post-selection inference, segmentation algorithms
              },
  file = {PDF:/home/jmei/Zotero/storage/DHXXYRID/full-text.pdf:application/pdf},
}

@article{xu_neural_nodate,
  title = {{NEURAL} {NETWORK} {ALGORITHMS} {FOR} {ONTOLOGY} {INFORMED} {
           INFORMATION} {EXTRACTION}},
  language = {en},
  author = {Xu, Dongfang},
  pages = {164},
  file = {Xu - NEURAL NETWORK ALGORITHMS FOR ONTOLOGY INFORMED
          IN.pdf:/home/jmei/Zotero/storage/VWP3EYPU/Xu - NEURAL NETWORK
          ALGORITHMS FOR ONTOLOGY INFORMED IN.pdf:application/pdf},
}

@book{boyd_convex_2004-1,
  address = {Cambridge, UK ; New York},
  title = {Convex optimization},
  isbn = {978-0-521-83378-3},
  language = {en},
  publisher = {Cambridge University Press},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  keywords = {Convex functions, Mathematical optimization},
  file = {Boyd and Vandenberghe - 2004 - Convex
          optimization.pdf:/home/jmei/Zotero/storage/5DQKF39U/Boyd and
          Vandenberghe - 2004 - Convex optimization.pdf:application/pdf},
}

@article{mei_hw5_2022,
  title = {{HW5}},
  author = {Mei, Jeffrey},
  year = {2022},
  file = {PDF:/home/jmei/Zotero/storage/WDPQ7MIR/Mei - 2022 -
          HW5.pdf:application/pdf},
}

@article{buja_linear_1989,
  title = {Linear {Smoothers} and {Additive} {Models}},
  volume = {17},
  number = {2},
  urldate = {2022-11-10},
  journal = {Source: The Annals of Statistics},
  author = {Buja, Andreas and Hastie, Trevor and Tibshirani, Robert},
  year = {1989},
  pages = {453--510},
  file = {PDF:/home/jmei/Zotero/storage/8PW3D8WU/full-text.pdf:application/pdf},
}

@article{breiman_estimating_nodate,
  title = {Estimating {Optimal} {Transformations} for {Multiple} {Regression}
           and {Correlation}},
  urldate = {2022-11-10},
  author = {Breiman, Leo and Friedman, Jerome H},
  file = {PDF:/home/jmei/Zotero/storage/I3G5SP6X/full-text.pdf:application/pdf},
}

@article{huang_variable_2010,
  title = {Variable selection in nonparametric additive models},
  volume = {38},
  issn = {0090-5364},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-4/Variable-selection-in-nonparametric-additive-models/10.1214/09-AOS781.full
         },
  doi = {10.1214/09-AOS781},
  abstract = {We consider a nonparametric additive model of a conditional mean
              function in which the number of variables and additive components
              may be larger than the sample size but the number of nonzero
              additive components is “small” relative to the sample size. The
              statistical problem is to determine which additive components are
              nonzero. The additive components are approximated by truncated
              series expansions with B-spline bases. With this approximation, the
              problem of component selection becomes that of selecting the groups
              of coefficients in the expansion. We apply the adaptive group Lasso
              to select nonzero components, using the group Lasso to obtain an
              initial estimator and reduce the dimension of the problem. We give
              conditions under which the group Lasso selects a model whose number
              of components is comparable with the underlying model, and the
              adaptive group Lasso selects the nonzero components correctly with
              probability approaching one as the sample size increases and
              achieves the optimal rate of convergence. The results of Monte
              Carlo experiments show that the adaptive group Lasso procedure
              works well with samples of moderate size. A data example is used to
              illustrate the application of the proposed method.},
  number = {4},
  urldate = {2022-10-27},
  journal = {https://doi.org/10.1214/09-AOS781},
  author = {Huang, Jian and Horowitz, Joel L. and Wei, Fengrong},
  month = aug,
  year = {2010},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {62G08, 62G20, 62G99, adaptive group LASSO, component selection,
              High-dimensional data, Nonparametric regression, selection
              consistency},
  pages = {2282--2313},
  file = {PDF:/home/jmei/Zotero/storage/F7MWSM4X/full-text.pdf:application/pdf},
}

@article{ravikumar_sparse_2008,
  title = {{SPARSE} {ADDITIVE} {MODELS}},
  abstract = {We present a new class of methods for high-dimensional
              non-parametric regression and classification called sparse additive
              models (SpAM). Our methods combine ideas from sparse linear
              modeling and additive nonparametric regression. We derive an
              algorithm for fitting the models that is practical and effective
              even when the number of covariates is larger than the sample size.
              SpAM is essentially a functional version of the grouped lasso of
              Yuan and Lin (2006). SpAM is also closely related to the COSSO
              model of Lin and Zhang (2006), but decouples smoothing and sparsity
              , enabling the use of arbitrary nonparametric smoothers. We give an
              analysis of the theoretical properties of sparse additive models,
              and present empirical results on synthetic and real data, showing
              that SpAM can be effective in fitting sparse nonparametric models
              in high dimensional data.},
  urldate = {2022-10-28},
  author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman,
            Larry},
  year = {2008},
  note = {arXiv: 0711.4555v2},
  file = {PDF:/home/jmei/Zotero/storage/738BBGBX/full-text.pdf:application/pdf},
}

@article{mei_homework_2006,
  title = {Homework 6},
  doi = {10.1007/1-84628-477-5_57},
  abstract = {Two lightweight block cipher families, Simon and Speck, have been
              proposed by researchers from the NSA recently. In this paper, we
              introduce Simeck, a new family of lightweight block ciphers that
              combines the good design components from both Simon and Speck, in
              order to devise even more compact and efficient block ciphers. For
              Simeck32/64, we can achieve 505 GEs (before the Place and Route
              phase) and 549 GEs (after the Place and Route phase), with the
              power consumption of 0.417 μW{\textbackslash}mu W in CMOS 130 nm
              ASIC, and 454 GEs (before the Place and Route phase) and 488 GEs
              (after the Place and Route phase), with the power consumption of
              1.292 μW{\textbackslash}mu W in CMOS 65 nm ASIC. Furthermore, all
              of the instances of Simeck are smaller than the ones of
              hardware-optimized cipher Simon in terms of area and power
              consumption in both CMOS 130 nm and CMOS 65 nm techniques. In
              addition, we also give the security evaluation of Simeck with
              respect to many traditional cryptanalysis methods, including
              differential attacks, linear attacks, impossible differential
              attacks, meet-in-the-middle attacks, and slide attacks. Overall,
              all of the instances of Simeck can satisfy the area, power, and
              throughput requirements in passive RFID tags.},
  number = {X},
  journal = {Theory of Computation},
  author = {Mei, Jeffrey},
  year = {2006},
  pages = {281--281},
  file = {PDF:/home/jmei/Zotero/storage/FTEA7T98/Mei - 2006 - Homework
          6.pdf:application/pdf},
}

@article{olshen_circular_2004-1,
  title = {Circular binary segmentation for the analysis of array-based {DNA}
           copy number data},
  volume = {5},
  issn = {14654644},
  doi = {10.1093/biostatistics/kxh008},
  abstract = {DNA sequence copy number is the number of copies of DNA at a
              region of a genome. Cancer progression often involves alterations
              in DNA copy number. Newly developed microarray technologies enable
              simultaneous measurement of copy number at thousands of sites in a
              genome. We have developed a modification of binary segmentation,
              which we call circular binary segmentation, to translate noisy
              intensity measurements into regions of equal copy number. The
              method is evaluated by simulation and is demonstrated on cell line
              data with known copy number alterations and on a breast cancer cell
              line data set. © Oxford University Press 2004; all rights reserved.
              },
  number = {4},
  urldate = {2022-11-09},
  journal = {Biostatistics},
  author = {Olshen, Adam B. and Venkatraman, E. S. and Lucito, Robert and Wigler
            , Michael},
  year = {2004},
  pmid = {15475419},
  keywords = {Binary segmentation, Array CGH, Change-point, ROMA},
  pages = {557--572},
}

@article{olshen_circular_2004-2,
  title = {Circular binary segmentation for the analysis of array-based {DNA}
           copy number data},
  volume = {5},
  doi = {10.1093/biostatistics/kxh008},
  abstract = {DNA sequence copy number is the number of copies of DNA at a
              region of a genome. Cancer progression often involves alterations
              in DNA copy number. Newly developed microarray technologies enable
              simultaneous measurement of copy number at thousands of sites in a
              genome. We have developed a modification of binary segmentation,
              which we call circular binary segmentation, to translate noisy
              intensity measurements into regions of equal copy number. The
              method is evaluated by simulation and is demonstrated on cell line
              data with known copy number alterations and on a breast cancer cell
              line data set.},
  number = {4},
  urldate = {2022-11-09},
  journal = {Biostatistics},
  author = {Olshen, Adam B and Venkatraman, E S and Lucito, Robert and Wigler,
            Michael},
  year = {2004},
  keywords = {Binary segmentation, Array CGH, Change-point, ROMA},
  pages = {557--572},
  file = {PDF:/home/jmei/Zotero/storage/CNMGVMAQ/full-text.pdf:application/pdf},
}

@article{zhang_dna_2010,
  title = {{DNA} {Copy} {Number} {Profiling} in {Normal} and {Tumor} {Genomes}},
  url = {https://link.springer.com/chapter/10.1007/978-1-84996-196-7_14},
  doi = {10.1007/978-1-84996-196-7_14},
  abstract = {copy number profiling; copy number variation; cancer},
  urldate = {2022-11-05},
  author = {Zhang, Nancy R.},
  year = {2010},
  note = {Publisher: Springer, London},
  pages = {259--281},
  file = {PDF:/home/jmei/Zotero/storage/CYRYQW9I/full-text.pdf:application/pdf},
}

@article{fearnhead_detecting_2018,
  title = {Detecting {Changes} in {Slope} {With} an {L} 0 {Penalty} {Detecting}
           {Changes} in {Slope} {With} an {L} 0 {Penalty}},
  issn = {1537-2715},
  url = {
         https://www.tandfonline.com/action/journalInformation?journalCode=ucgs20https://doi.org/./..
         },
  doi = {10.1080/10618600.2018.1512868},
  abstract = {While there are many approaches to detecting changes in mean for a
              univariate time series, the problem of detecting multiple changes
              in slope has comparatively been ignored. Part of the reason for
              this is that detecting changes in slope is much more challenging:
              simple binary segmentation procedures do not work for this problem,
              while existing dynamic programming methods that work for the change
              in mean problem cannot be used for detecting changes in slope. We
              present a novel dynamic programming approach, CPOP, for finding the
              "best"continuous piecewise linear fit to data under a criterion
              that measures fit to data using the residual sum of squares, but
              penalizes complexity based on an L 0 penalty on changes in slope.
              We prove that detecting changes in this manner can lead to
              consistent estimation of the number of changepoints, and show
              empirically that using an L 0 penalty is more reliable at
              estimating changepoint locations than using an L 1 penalty.
              Empirically CPOP has good computational properties, and can analyze
              a time series with 10,000 observations and 100 changes in a few
              minutes. Our method is used to analyze data on the motion of
              bacteria, and provides better and more parsimonious fits than two
              competing approaches. Supplementary material for this article is
              available online.},
  urldate = {2022-11-05},
  author = {Fearnhead, Paul and Maidstone, Robert and Letchford, Adam},
  year = {2018},
  keywords = {Breakpoints, Functional pruning, Linear spline regression,
              Narrowest-over-threshold, Trend-filtering},
  file = {PDF:/home/jmei/Zotero/storage/ZFDGRSIB/full-text.pdf:application/pdf},
}

@article{sanmartin_problem_2020,
  title = {Problem 4},
  doi = {10.1201/9781351032506-5},
  journal = {Structure Determination By Spectroscopic Methods},
  author = {SanMartin, Raul and Herrero, María Teresa},
  year = {2020},
  pages = {11--13},
  file = {PDF:/home/jmei/Zotero/storage/TAV2NBLK/SanMartin, Herrero - 2020 -
          Problem 4.pdf:application/pdf},
}

@article{yamada_high-dimensional_2012,
  title = {High-{Dimensional} {Feature} {Selection} by {Feature}-{Wise} {
           Kernelized} {Lasso}},
  volume = {26},
  url = {http://arxiv.org/abs/1202.0515},
  doi = {10.1162/NECO_a_00537},
  abstract = {The goal of supervised feature selection is to find a subset of
              input features that are responsible for predicting output values.
              The least absolute shrinkage and selection operator (Lasso) allows
              computationally efficient feature selection based on linear
              dependency between input features and output values. In this paper,
              we consider a feature-wise kernelized Lasso for capturing
              non-linear input-output dependency. We first show that, with
              particular choices of kernel functions, non-redundant features with
              strong statistical dependence on output values can be found in
              terms of kernel-based independence measures. We then show that the
              globally optimal solution can be efficiently computed; this makes
              the approach scalable to high-dimensional problems. The
              effectiveness of the proposed method is demonstrated through
              feature selection experiments with thousands of features.},
  number = {1},
  urldate = {2022-10-27},
  journal = {Neural Computation},
  author = {Yamada, Makoto and Jitkrittum, Wittawat and Sigal, Leonid and Xing,
            Eric P. and Sugiyama, Masashi},
  month = feb,
  year = {2012},
  note = {arXiv: 1202.0515v4},
  pages = {185--207},
  file = {PDF:/home/jmei/Zotero/storage/RF8DJ3C5/full-text.pdf:application/pdf},
}

@article{bach_consistency_2008,
  title = {Consistency of the {Group} {Lasso} and {Multiple} {Kernel} {Learning}
           },
  volume = {9},
  abstract = {We consider the least-square regression problem with
              regularization by a block 1-norm, that is, a sum of Euclidean norms
              over spaces of dimensions larger than one. This problem, referred
              to as the group Lasso, extends the usual regularization by the
              1-norm where all spaces have dimension one, where it is commonly
              referred to as the Lasso. In this paper, we study the asymptotic
              group selection consistency of the group Lasso. We derive necessary
              and sufficient conditions for the consistency of group Lasso under
              practical assumptions, such as model misspecification. When the
              linear predictors and Euclidean norms are replaced by functions and
              reproducing kernel Hilbert norms, the problem is usually referred
              to as multiple kernel learning and is commonly used for learning
              from heterogeneous data sources and for non linear variable
              selection. Using tools from functional analysis, and in particular
              covariance operators, we extend the consistency results to this
              infinite dimensional case and also propose an adaptive scheme to
              obtain a consistent model estimate, even when the necessary
              condition required for the non adaptive scheme is not satisfied.},
  urldate = {2022-10-27},
  journal = {Journal of Machine Learning Research},
  author = {Bach, Francis R and Org, Francis Bach@mines},
  year = {2008},
  keywords = {convex optimization, consistency, covariance operators,
              regularization, sparsity},
  pages = {1179--1225},
  file = {PDF:/home/jmei/Zotero/storage/8L2JFDYP/full-text.pdf:application/pdf},
}

@article{lee_exact_2016-1,
  title = {{EXACT} {POST}-{SELECTION} {INFERENCE}, {WITH} {APPLICATION} {TO} {
           THE} {LASSO}},
  volume = {44},
  doi = {10.1214/15-AOS1371},
  abstract = {We develop a general approach to valid inference after model
              selection. At the core of our framework is a result that
              characterizes the distribution of a post-selection estimator
              conditioned on the selection event. We specialize the approach to
              model selection by the lasso to form valid confidence intervals for
              the selected coefficients and test whether all relevant variables
              have been included in the model.},
  number = {3},
  urldate = {2022-10-20},
  journal = {The Annals of Statistics},
  author = {Lee, Jason D and Sun, Dennis L and Sun, Yuekai and Taylor, Jonathan
            E},
  year = {2016},
  note = {arXiv: 1311.6238v8},
  keywords = {Lasso, 62E15, 62F03, 62J07, confidence interval, hypothesis test,
              model selection},
  pages = {907--927},
  file = {PDF:/home/jmei/Zotero/storage/VF7JM8IN/full-text.pdf:application/pdf},
}

@article{wang_random_2011,
  title = {Random lasso},
  volume = {5},
  issn = {1932-6157},
  url = {
         https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-1/Random-lasso/10.1214/10-AOAS377.full
         },
  doi = {10.1214/10-AOAS377},
  abstract = {We propose a computationally intensive method, the random lasso
              method, for variable selection in linear models. The method
              consists of two major steps. In step 1, the lasso method is applied
              to many bootstrap samples, each using a set of randomly selected
              covariates. A measure of importance is yielded from this step for
              each covariate. In step 2, a similar procedure to the first step is
              implemented with the exception that for each bootstrap sample, a
              subset of covariates is randomly selected with unequal selection
              probabilities determined by the covariates’ importance. Adaptive
              lasso may be used in the second step with weights determined by the
              importance measures. The final set of covariates and their
              coefficients are determined by averaging bootstrap results obtained
              from step 2. The proposed method alleviates some of the limitations
              of lasso, elastic-net and related methods noted especially in the
              context of microarray data analysis: it tends to remove highly
              correlated variables altogether or select them all, and maintains
              maximal flexibility in estimating their coefficients, particularly
              with different signs; the number of selected variables is no longer
              limited by the sample size; and the resulting prediction accuracy
              is competitive or superior compared to the alternatives. We
              illustrate the proposed method by extensive simulation studies. The
              proposed method is also applied to a Glioblastoma microarray data
              analysis.},
  number = {1},
  urldate = {2022-10-26},
  journal = {https://doi.org/10.1214/10-AOAS377},
  author = {Wang, Sijian and Nan, Bin and Rosset, Saharon and Zhu, Ji},
  month = mar,
  year = {2011},
  note = {Publisher: Institute of Mathematical Statistics},
  keywords = {Lasso, Variable selection, regularization, microarray},
  pages = {468--485},
  file = {PDF:/home/jmei/Zotero/storage/DWX3WMNU/full-text.pdf:application/pdf},
}

@article{zhou_group_2010,
  title = {Group variable selection via a hierarchical lasso and its oracle
           property},
  volume = {3},
  abstract = {In many engineering and scientific applications, prediction
              variables are grouped, for example, in biological applications
              where assayed genes or proteins can be grouped by biological roles
              or biological pathways. Common statistical analysis methods such as
              ANOVA, factor analysis, and functional modeling with basis sets
              also exhibit natural variable groupings. Existing successful group
              variable selection methods have the limitation of selecting
              variables in an "all-in-all-out" fashion, i.e., when one variable
              in a group is selected , all other variables in the same group are
              also selected [1, 23, 25]. In many real problems, however, we may
              want to keep the flexibility of selecting variables within a group,
              such as in gene-set selection. In this paper, we develop a new
              group variable selection method that not only removes unimportant
              groups effectively, but also keeps the flexibility of selecting
              variables within a group. We also show that the new method offers
              the potential for achieving the theoretical "oracle" property [6,
              7].},
  urldate = {2022-10-26},
  journal = {Statistics and Its Interface},
  author = {Zhou, Nengfeng and Zhu, Ji},
  year = {2010},
  keywords = {Lasso, Variable selection, Regularization, Oracle property, Group
              selection},
  pages = {557--574},
  file = {PDF:/home/jmei/Zotero/storage/ZGLXRLXP/full-text.pdf:application/pdf},
}

@article{mukherjee_reduced_2011,
  title = {Reduced {Rank} {Ridge} {Regression} and {Its} {Kernel} {Extensions}},
  doi = {10.1002/sam.10138},
  abstract = {In multivariate linear regression, it is often assumed that the
              response matrix is intrinsically of lower rank. This could be
              because of the correlation structure among the prediction variables
              or the coefficient matrix being lower rank. To accommodate both, we
              propose a reduced rank ridge regression for multivariate linear
              regression. Specifically, we combine the ridge penalty with the
              reduced rank constraint on the coefficient matrix to come up with a
              computationally straightforward algorithm. Numerical studies
              indicate that the proposed method consistently outperforms relevant
              competitors. A novel extension of the proposed method to the
              reproducing kernel Hilbert space (RKHS) setup is also developed.},
  urldate = {2022-10-26},
  author = {Mukherjee, Ashin and Zhu, Ji},
  year = {2011},
  keywords = {RKHS, reduced rank regression, ridge regression},
  file = {PDF:/home/jmei/Zotero/storage/TJX9EJPL/full-text.pdf:application/pdf},
}

@article{mei_hw4_2022,
  title = {{HW4}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--19},
  file = {PDF:/home/jmei/Zotero/storage/X2HFD5MI/Mei - 2022 -
          HW4.pdf:application/pdf},
}

@article{zhang_interpretable_2018,
  title = {Interpretable {Dynamic} {Treatment} {Regimes} {HHS} {Public} {Access}
           },
  volume = {113},
  doi = {10.1080/01621459.2017.1345743},
  abstract = {Precision medicine is currently a topic of great interest in
              clinical and intervention science. A key component of precision
              medicine is that it is evidence-based, i.e., data-driven, and
              consequently there has been tremendous interest in estimation of
              precision medicine strategies using observational or randomized
              study data. One way to formalize precision medicine is through a
              treatment regime, which is a sequence of decision rules, one per
              stage of clinical intervention, that map up-to-date patient
              information to a recommended treatment. An optimal treatment regime
              is defined as maximizing the mean of some cumulative clinical
              outcome if applied to a population of interest. It is well-known
              that even under simple generative models an optimal treatment
              regime can be a highly nonlinear function of patient information.
              Consequently, a focal point of recent methodological research has
              been the development of flexible models for estimating optimal
              treatment regimes. However, in many settings, estimation of an
              optimal treatment regime is an exploratory analysis intended to
              generate new hypotheses for subsequent research and not to directly
              dictate treatment to new patients. In such settings, an estimated
              treatment regime that is interpretable in a domain context may be
              of greater value than an unintelligible treatment regime built
              using 'black-box' estimation methods. We propose an estimator of an
              optimal treatment regime composed of a sequence of decision rules,
              each expressible as a list of "if-then" statements that can be
              presented as either a paragraph or as a simple flowchart that is
              immediately interpretable to domain experts. The discreteness of
              these lists precludes smooth, i.e., gradient-based, methods of
              estimation and leads to non-standard asymptotics. Nevertheless, we
              provide a computationally efficient estimation algorithm, prove
              consistency of the proposed estimator, and derive rates of
              convergence. We illustrate the proposed methods using a series of
              simulation examples and application to data from a sequential
              clinical trial on bipolar disorder.},
  number = {524},
  urldate = {2022-10-25},
  journal = {J Am Stat Assoc},
  author = {Zhang, Yichi and Laber, Eric B and Davidian, Marie and Tsiatis,
            Anastasios A},
  year = {2018},
  keywords = {Precision medicine, decision lists, interpretability,
              research-practice gap, treatment regimes, tree-based methods},
  pages = {1541--1549},
  file = {PDF:/home/jmei/Zotero/storage/Z9HGHC6I/full-text.pdf:application/pdf},
}

@article{aronszajn_theory_nodate,
  title = {{THEORY} {OF} {REPRODUCING} {KERNELS}(')},
  url = {https://www.ams.org/journal-terms-of-use},
  urldate = {2022-10-20},
  author = {Aronszajn, Nachman},
  file = {PDF:/home/jmei/Zotero/storage/585FUAVZ/full-text.pdf:application/pdf},
}

@article{competition_problem_2011,
  title = {Problem 1 {Problem} 1},
  volume = {1},
  abstract = {Mammals possess membrane-associated and cytosolic forms of the
              puromycin-sensitive aminopeptidase (PSA; EC 3.4.11.14). Increasing
              evidence suggests the membrane PSA is involved in neuromodulation
              within the central nervous system and in reproductive biology. The
              functional roles of the cytosolic PSA are less clear. The genome of
              the nematode Caenorhabditis elegans encodes an aminopeptidase,
              F49E8.3 (PAM-1), that is orthologous to PSA, and sequence analysis
              predicts it to be cytosolic. We have determined the spatio/temporal
              gene expression pattern of pam-1 by using the promoter region of
              F49E8.3 to control expression in the nematode of a second exon
              translational fusion of the aminopeptidase to green fluorescent
              protein. Cytosolic fluorescence was observed throughout development
              in the intestine and nerve cells of the head. Neuronal expression
              was also observed in the tail of adult males. Recombinant PAM-1,
              expressed and purified from Escherichia coli, hydrolyzed the
              N-terminal amino acid from peptide substrates. Favored substrates
              had positively charged or small neutral amino acids in the
              N-terminal position. Peptide hydrolysis was inhibited by the
              metal-chelating agent 1,10-phenanthroline and by the aminopeptidase
              inhibitors actinonin, amastatin, and leuhistin. However, the enzyme
              was approximately 100-fold less sensitive toward puromycin (IC50,
              135 mum) than other PSA homologues. Following inactivation of the
              enzyme, aminopeptidase activity was recovered with Zn2+, Co2+, and
              Ni2+. Silencing expression of pam-1 by RNA interference resulted in
              30\% embryonic lethality. Surviving adult hermaphrodites deposited
              large numbers of oocytes throughout the self-fertile period. The
              overall brood size was, however, unaffected. We conclude that pam-1
              encodes an aminopeptidase that clusters phylogenetically with the
              PSAs, despite attenuated sensitivity toward puromycin, and that it
              functions in embryo development and reproduction of the nematode},
  number = {July},
  author = {Competition, Experimental and Code, Student and Sheet, Answer and
            Page, Calibration and Ii, Pattern},
  year = {2011},
  pages = {2--3},
  file = {PDF:/home/jmei/Zotero/storage/H74VWMDL/Competition et al. - 2011 -
          Problem 1 Problem 1.pdf:application/pdf},
}

@book{wainwright_high-dimensional_2019,
  title = {High-dimensional statistics: {A} non-asymptotic viewpoint},
  isbn = {978-1-108-62777-1},
  abstract = {Recent years have witnessed an explosion in the volume and variety
              of data collected in all scientific disciplines and industrial
              settings. Such massive data sets present a number of challenges to
              researchers in statistics and machine learning. This book provides
              a self-contained introduction to the area of high-dimensional
              statistics, aimed at the first-year graduate level. It includes
              chapters that are focused on core methodology and theory -
              including tail bounds, concentration inequalities, uniform laws and
              empirical process, and random matrices - as well as chapters
              devoted to in-depth exploration of particular model classes -
              including sparse linear models, matrix models with rank constraints
              , graphical models, and various types of non-parametric models.
              With hundreds of worked examples and exercises, this text is
              intended both for courses and for self-study by graduate students
              and researchers in statistics, machine learning, and related fields
              who must understand, apply, and adapt modern statistical methods
              suited to large-scale data.},
  author = {Wainwright, Martin J.},
  year = {2019},
  doi = {10.1017/9781108627771},
  note = {Publication Title: High-Dimensional Statistics: A Non-Asymptotic
          Viewpoint},
  file = {PDF:/home/jmei/Zotero/storage/JUZYIFC5/Wainwright - 2019 -
          High-dimensional statistics A non-asymptotic
          viewpoint.pdf:application/pdf},
}

@article{li_quantile_2007,
  title = {Quantile {Regression} in {Reproducing} {Kernel} {Hilbert} {Spaces} {
           Quantit}? {Regression} in {Reproducing} {Kernel} {Hubert} {Spaces}},
  volume = {102},
  doi = {10.1198/016214506000000979},
  abstract = {REFERENCES Linked references are available on JSTOR for this
              article: https://www.jstor.org/stable/27639837?seq=1\&
              cid=pdf-reference\#references\_tab\_contents You may need to log in
              to JSTOR to access the linked references. JSTOR is a not-for-profit
              service that helps scholars, researchers, and students discover,
              use, and build upon a wide range of content in a trusted digital
              archive. We use information technology and tools to increase
              productivity and facilitate new forms of scholarship. For more
              information about JSTOR, please contact support@jstor.org. In this
              article we consider quantile regression in reproducing kernel
              Hilbert spaces, which we call kernel quantile regression (KQR). We
              make three contributions: (1) we propose an efficient algorithm
              that computes the entire solution path of the KQR, with essentially
              the same computational cost as fitting one KQR model; (2) we derive
              a simple formula for the effective dimension of the KQR model,
              which allows convenient selection of the regularization parameter;
              and (3) we develop an asymptotic theory for the KQR model.},
  number = {477},
  urldate = {2022-10-20},
  journal = {Source: Journal of the American Statistical Association},
  author = {Li, Youjuan and Liu, Yufeng and Zhu, Ji},
  year = {2007},
  keywords = {Quantile regression, Model selection, Degrees of freedom, Metric
              entropy, Quadratic programming, Reproducing kernel Hilbert space},
  pages = {255--268},
  file = {PDF:/home/jmei/Zotero/storage/L4XYBV4F/full-text.pdf:application/pdf},
}

@article{tibshirani_exact_2016,
  title = {Exact {Post}-{Selection} {Inference} for {Sequential} {Regression} {
           Procedures}},
  issn = {0162-1459},
  url = {
         http://www.tandfonline.com/action/journalInformation?journalCode=uasa20http://dx.doi.org/./..
         },
  doi = {10.1080/01621459.2015.1108848},
  abstract = {We propose new inference tools for forward stepwise regression,
              least angle regression, and the lasso. Assuming a Gaussian model
              for the observation vector y, we first describe a general scheme to
              perform valid inference after any selection event that can be
              characterized as y falling into a polyhedral set. This framework
              allows us to derive conditional (post-selection) hypothesis tests
              at any step of forward stepwise or least angle regression, or any
              step along the lasso regularization path, because, as it turns out,
              selection events for these procedures can be expressed as
              polyhedral constraints on y. The p-values associated with these
              tests are exactly uniform under the null distribution, in finite
              samples, yielding exact Type I error control. The tests can also be
              inverted to produce confidence intervals for appropriate underlying
              regression parameters. The R package selectiveInference, freely
              available on the CRAN repository, implements the new inference
              tools described in this article. Supplementary materials for this
              article are available online.},
  urldate = {2022-10-20},
  journal = {JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION },
  author = {Tibshirani, Ryan J and Taylor, Jonathan and Lockhart, Richard and
            Tibshirani, Robert},
  year = {2016},
  keywords = {Lasso, Confidence interval, Forward stepwise regression, Inference
              after selection, Least angle regression, p-Value},
  file = {PDF:/home/jmei/Zotero/storage/8LVMHUW9/full-text.pdf:application/pdf},
}

@article{tibshirani_sparsity_2005,
  title = {Sparsity and smoothness via the fused lasso},
  volume = {67},
  abstract = {The lasso penalizes a least squares regression by the sum of the
              absolute values (L 1-norm) of the coefficients. The form of this
              penalty encourages sparse solutions (with many coefficients equal
              to 0). We propose the 'fused lasso', a generalization that is
              designed for problems with features that can be ordered in some
              meaningful way. The fused lasso penalizes the L 1-norm of both the
              coefficients and their successive differences. Thus it encourages
              sparsity of the coefficients and also sparsity of their
              differences-i.e. local constancy of the coefficient profile. The
              fused lasso is especially useful when the number of features p is
              much greater than N, the sample size. The technique is also
              extended to the 'hinge' loss function that underlies the support
              vector classifier.We illustrate the methods on examples from
              protein mass spectroscopy and gene expression data.},
  urldate = {2022-10-15},
  journal = {J. R. Statist. Soc. B},
  author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu
            , Ji and Knight, Keith},
  year = {2005},
  keywords = {Lasso, Fused lasso, Gene expression, Least squares regression,
              Protein mass spectroscopy, Sparse solutions, Support vector
              classifier},
  pages = {91--108},
  file = {PDF:/home/jmei/Zotero/storage/ZWITLLIK/full-text.pdf:application/pdf},
}

@article{hao_multiple_2013-1,
  title = {Multiple {Change}-{Point} {Detection} via a {Screening} and {Ranking}
           {Algorithm}},
  volume = {23},
  issn = {10170405},
  url = {/pmc/articles/PMC3902887/},
  doi = {10.5705/SS.2012.018S},
  abstract = {Let Y1; Yn be a sequence whose underlying mean is a step function
              with an unknown number of the steps and unknown change points. The
              detection of the change points, namely the positions where the mean
              changes, is an important problem in such fields as engineering,
              economics, climatology and bioscience. This problem has attracted a
              lot of attention in statistics, and a variety of solutions have
              been proposed and implemented. However, there is scant literature
              on the theoretical properties of those algorithms. Here, we
              investigate a recently developed algorithm called the Screening and
              Ranking algorithm (SaRa). We characterize the theoretical
              properties of SaRa and show its superiority over other commonly
              used algorithms. In particular, we develop a false discovery rate
              approach to the multiple change-point problem and show a strong
              sure coverage property for the SaRa.},
  number = {4},
  urldate = {2022-10-15},
  journal = {Statistica Sinica},
  author = {Hao, Ning and Niu, Yue Selena and Zhang, Heping},
  month = jul,
  year = {2013},
  pmid = {24489450},
  note = {Publisher: NIH Public Access},
  keywords = {Screening and ranking algorithm, Change-point detection, Copy
              number variation, False discovery rate, High dimensional data},
  pages = {1553},
  file = {PDF:/home/jmei/Zotero/storage/H3EZQDWF/full-text.pdf:application/pdf},
}

@article{tibshirani_diagnosis_2002,
  title = {Diagnosis of multiple cancer types by shrunken centroids of gene
           expression},
  volume = {99},
  issn = {00278424},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.082099299},
  doi = {
         10.1073/PNAS.082099299/ASSET/6E8EA42E-DF8B-4D6C-9603-90BE800CC986/ASSETS/GRAPHIC/PQ0820992006.JPEG
         },
  abstract = {We have devised an approach to cancer class prediction from gene
              expression profiling, based on an enhancement of the simple nearest
              prototype (centroid) classifier. We shrink the prototypes and hence
              obtain a classifier that is often more accurate than competing
              methods. Our method of "nearest shrunken centroids" identifies
              subsets of genes that best characterize each class. The technique
              is general and can be used in many other classification problems.
              To demonstrate its effectiveness, we show that the method was
              highly efficient in finding genes for classifying small round blue
              cell tumors and leukemias.},
  number = {10},
  urldate = {2022-10-14},
  journal = {Proceedings of the National Academy of Sciences of the United
             States of America},
  author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan,
            Balasubramanian and Chu, Gilbert},
  month = may,
  year = {2002},
  pmid = {12011421},
  note = {Publisher: The National Academy of Sciences},
  pages = {6567--6572},
  file = {PDF:/home/jmei/Zotero/storage/83JN5ZVE/full-text.pdf:application/pdf},
}

@article{fan_variance_2012,
  title = {Variance estimation using refitted cross-validation in ultrahigh
           dimensional regression},
  volume = {74},
  abstract = {Variance estimation is a fundamental problem in statistical
              modelling. In ultrahigh dimensional linear regression where the
              dimensionality is much larger than the sample size, traditional
              variance estimation techniques are not applicable. Recent advances
              in variable selection in ultrahigh dimensional linear regression
              make this problem accessible. One of the major problems in
              ultrahigh dimensional regression is the high spurious correlation
              between the unob-served realized noise and some of the predictors.
              As a result, the realized noises are actually predicted when extra
              irrelevant variables are selected, leading to a serious
              underestimate of the level of noise. We propose a two-stage
              refitted procedure via a data splitting technique, called refitted
              cross-validation, to attenuate the influence of irrelevant
              variables with high spurious correlations. Our asymptotic results
              show that the resulting procedure performs as well as the oracle
              estimator, which knows in advance the mean regression function. The
              simulation studies lend further support to our theoretical claims.
              The naive two-stage estimator and the plug-in one-stage estimators
              using the lasso and smoothly clipped absolute deviation are also
              studied and compared. Their performances can be improved by the
              refitted cross-validation method proposed.},
  urldate = {2022-06-04},
  journal = {J. R. Statist. Soc. B},
  author = {Fan, Jianqing and Guo, Shaojun and Hao, Ning},
  year = {2012},
  keywords = {Variable selection, Dimension reduction, Data splitting, High
              dimensionality, Refitted cross-validation, Sure screening, Variance
              estimation},
  pages = {37--65},
  file = {PDF:/home/jmei/Zotero/storage/6BWCCN6V/full-text.pdf:application/pdf},
}

@article{yu_estimating_nodate,
  title = {Estimating the error variance in a high-dimensional linear model},
  abstract = {The lasso has been studied extensively as a tool for estimating
              the coefficient vector in the high-dimensional linear model;
              however, considerably less is known about estimating the error
              variance in this context. In this paper, we propose the natural
              lasso estimator for the error variance, which maximizes a penalized
              likelihood objective. A key aspect of the natural lasso is that the
              likelihood is expressed in terms of the natural parameterization of
              the multiparameter exponential family of a Gaussian with unknown
              mean and variance. The result is a remarkably simple estimator of
              the error variance with provably good performance in terms of mean
              squared error. These theoretical results do not require placing any
              assumptions on the design matrix or the true regression
              coefficients. We also propose a companion estimator, called the
              organic lasso, which theoretically does not require tuning of the
              regularization parameter. Both estimators do well empirically
              compared to preexisting methods, especially in settings where
              successful recovery of the true support of the coefficient vector
              is hard. Finally, we show that existing methods can do well under
              fewer assumptions than previously known, thus providing a fuller
              story about the problem of estimating the error variance in
              high-dimensional linear models.},
  urldate = {2022-06-04},
  author = {Yu, Guo and Bien, Jacob},
  note = {arXiv: 1712.02412v3},
  file = {PDF:/home/jmei/Zotero/storage/ISF43K5X/full-text.pdf:application/pdf},
}

@article{index_data_nodate,
  title = {data = read.table ( url ( https://www.math.arizona.edu/{
           \textasciitilde}piegorsch/{571A}/{Data}/{Chapter01}/{CH01PR27}.txt ))
           {X} = data \$ {V2} \# {Age} {Y} = data \$ {V1} \# {Muscle} {Mass} {
           Index} model = lm ({Y} {\textasciitilde} {X})},
  author = {Index, Muscle Mass and Whs, C I and Whs, Return and Yhat, Band},
  pages = {1--14},
  file = {PDF:/home/jmei/Zotero/storage/5QWWJ6TE/Index et al. - Unknown - data =
          read.table ( url ( httpswww.math.arizona.edu~
          piegorsch571ADataChapter01CH01PR27.txt )) X = data \$ V2 #
          A.pdf:application/pdf},
}

@article{mei_hw3_2022,
  title = {{HW3}},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--7},
  file = {PDF:/home/jmei/Zotero/storage/D6PJXKXU/Mei - 2022 -
          HW3.pdf:application/pdf},
}

@article{lin_sharp_nodate,
  title = {A {Sharp} {Error} {Analysis} for the {Fused} {Lasso}, with {
           Application} to {Approximate} {Changepoint} {Screening}},
  abstract = {In the 1-dimensional multiple changepoint detection problem, we
              derive a new fast error rate for the fused lasso estimator, under
              the assumption that the mean vector has a sparse number of
              changepoints. This rate is seen to be suboptimal (compared to the
              minimax rate) by only a factor of log log n. Our proof technique is
              centered around a novel construction that we call a lower
              interpolant. We extend our results to misspecified models and
              exponential family distributions. We also describe the implications
              of our error analysis for the approximate screening of
              changepoints.},
  urldate = {2022-10-01},
  author = {Lin, Kevin and Sharpnack, James and Rinaldo, Alessandro and
            Tibshirani, Ryan J},
  file = {PDF:/home/jmei/Zotero/storage/PGQ9R98X/full-text.pdf:application/pdf},
}

@article{wang_penncnv_2007,
  title = {{PennCNV}: {An} integrated hidden {Markov} model designed for
           high-resolution copy number variation detection in whole-genome {SNP}
           genotyping data},
  volume = {17},
  issn = {10889051},
  doi = {10.1101/GR.6861907},
  abstract = {Comprehensive identification and cataloging of copy number
              variations (CNVs) is required to provide a complete view of human
              genetic variation. The resolution of CNV detection in previous
              experimental designs has been limited to tens or hundreds of
              kilobases. Here we present PennCNV, a hidden Markov model (HMM)
              based approach, for kilobase-resolution detection of CNVs from
              Illumina high-density SNP genotyping data. This algorithm
              incorporates multiple sources of information, including total
              signal intensity and allelic intensity ratio at each SNP marker,
              the distance between neighboring SNPs, the allele frequency of SNPs
              , and the pedigree information where available. We applied PennCNV
              to genotyping data generated for 112 HapMap individuals; on average
              , we detected ∼27 CNVs for each individual with a median size of
              ∼12 kb. Excluding common rearrangements in lymphoblastoid cell
              lines, the fraction of CNVs in offspring not detected in parents
              (CNV-NDPs) was 3.3\%. Our results demonstrate the feasibility of
              whole-genome fine-mapping of CNVs via high-density SNP genotyping.
              ©2007 by Cold Spring Harbor Laboratory Press.},
  number = {11},
  urldate = {2022-09-30},
  journal = {Genome Research},
  author = {Wang, Kai and Li, Mingyao and Hadley, Dexter and Liu, Rui and
            Glessner, Joseph and Grant, Struan F.A. and Hakonarson, Hakon and
            Bucan, Maja},
  month = nov,
  year = {2007},
  pmid = {17921354},
  pages = {1665--1674},
  file = {PDF:/home/jmei/Zotero/storage/QLZKPVGQ/full-text.pdf:application/pdf},
}

@techreport{gelman_garden_2013,
  title = {Garden of {Forking} {Paths}},
  abstract = {"I thought of a labyrinth of labyrinths, of one sinuous spreading
              labyrinth that would encompass the past and the future. .. I felt
              myself to be, for an unknown period of time, an abstract per-ceiver
              of the world."-Borges (1941) Abstract Researcher degrees of freedom
              can lead to a multiple comparisons problem, even in settings where
              researchers perform only a single analysis on their data. The
              problem is there can be a large number of potential comparisons
              when the details of data analysis are highly contingent on data,
              without the researcher having to perform any conscious procedure of
              fishing or examining multiple p-values. We discuss in the context
              of several examples of published papers where data-analysis
              decisions were theoretically-motivated based on previous literature
              , but where the details of data selection and analysis were not
              pre-specified and, as a result, were contingent on data.},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2013},
  file = {PDF:/home/jmei/Zotero/storage/GH4273XQ/full-text.pdf:application/pdf},
}

@article{mei_homework_2022,
  title = {Homework \# 2},
  author = {Mei, Jeffrey},
  year = {2022},
  pages = {1--27},
  file = {PDF:/home/jmei/Zotero/storage/2GIUAR4H/Mei - 2022 - Homework #
          2.pdf:application/pdf},
}

@article{james_press_choosing_1978,
  title = {Choosing {Between} {Logistic} {Regression} and {Discriminant} {
           Analysis}},
  volume = {73},
  number = {364},
  urldate = {2022-09-26},
  journal = {Journal of the American Statistical Association},
  author = {James Press, S and Wilson, Sandra},
  year = {1978},
  pages = {699--705},
  file = {PDF:/home/jmei/Zotero/storage/B264PE4F/full-text.pdf:application/pdf},
}

@article{score_act_nodate,
  title = {{ACT} {Score} vs {GPA}},
  number = {ci},
  author = {Score, A C T},
  pages = {1--2},
  file = {PDF:/home/jmei/Zotero/storage/I86YMXE9/Score - Unknown - ACT Score vs
          GPA.pdf:application/pdf},
}

@article{freeman_copy_nodate,
  title = {Copy number variation: {New} insights in genome diversity},
  url = {www.genome.org/cgi/doi/10.1101/gr.3677206.},
  doi = {10.1101/gr.3677206},
  author = {Freeman, Jennifer L and Perry, George H and Feuk, Lars and Redon,
            Richard and Mccarroll, Steven A and Altshuler, David M and Aburatani,
            Hiroyuki and Jones, Keith W and Tyler-Smith, Chris and Hurles,
            Matthew E and Carter, Nigel P and Scherer, Stephen W and Lee, Charles
            },
  file = {PDF:/home/jmei/Zotero/storage/9MXAEWS5/full-text.pdf:application/pdf},
}

@article{wen_activation_2024,
  title = {Activation {Discovery} {With} {FDR} {Control}: {Application} to {FMRI
           } {Data}},
  issn = {10170405},
  shorttitle = {Activation {Discovery} {With} {FDR} {Control}},
  url = {http://www3.stat.sinica.edu.tw/statistica/J34N3/J34N317/J34N317.html},
  doi = {10.5705/ss.202022.0054},
  abstract = {Time series from a large number of sources are ubiquitous, and
              some of them usually incur structural changes throughout data
              acquisition. For example, in fMRI analysis, some brain regions
              associated with task-related stimuli or in resting states become
              active. An activated time series can be composed of readings from
              an activated region. Of interest is to control the uncertainty of
              discovering time series in activation (viz., activated regions in
              fMRI analysis) via the false discovery rate (FDR) tool. We propose
              a simple yet effective method, incorporating unknown asynchronous
              change patterns and spatial dependence. Its validity in controlling
              the FDR is justified by asymptotic analysis. Numerical experiments
              indicate that the proposed method is both accurate and powerful. An
              implementation is provided in the R package SLIP.},
  language = {en},
  urldate = {2024-08-24},
  journal = {Statistica Sinica},
  author = {Wen, Mengtao and Wang, Guanghui and Zou, Changliang and Wang,
            Zhaojun},
  year = {2024},
  file = {PDF:/home/jmei/Zotero/storage/MGCK6Q2X/Wen et al. - 2024 - Activation
          Discovery With FDR Control Application to FMRI Data.pdf:application/pdf
          },
}

@article{object_gaussian_nodate,
  title = {The {Gaussian} graphical model in cross-sectional and time-series
           data},
  url = {https://core.ac.uk/reader/157610110?utm_source=linkout},
  abstract = {We discuss the Gaussian graphical model (GGM; an undirected
              network of partial correlation coefficients) and detail its utility
              as an exploratory data analysis tool. The GGM shows which variables
              predict one-another, allows for sparse modeling of covariance
              structures, and may highlight potential causal relationships
              between observed variables. We describe the utility in 3 kinds of
              psychological datasets: datasets in which consecutive cases are
              assumed independent (e.g., cross-sectional data), temporally
              ordered datasets (e.g., n = 1 time series), and a mixture of the 2
              (e.g., n \&gt; 1 time series). In time-series analysis, the GGM can
              be used to model the residual structure of a vector-autoregression
              analysis (VAR), also termed graphical VAR. Two network models can
              then be obtained: a temporal network and a contemporaneous network.
              When analyzing data from multiple subjects, a GGM can also be
              formed on the covariance structure of stationary means—the
              between-subjects network. We discuss the interpretation of these
              models and propose estimation methods to obtain these networks,
              which we implement in the R packages graphicalVAR and mlVAR. The
              methods are showcased in two empirical examples, and simulation
              studies on these methods are included in the supplementary
              materials},
  urldate = {2024-08-24},
  author = {Object, object},
  file = {Full Text PDF:/home/jmei/Zotero/storage/EYESZNEX/Object - The Gaussian
          graphical model in cross-sectional and time-series
          data.pdf:application/pdf},
}

@article{sen_tests_1975,
  title = {On {Tests} for {Detecting} {Change} in {Mean}},
  volume = {3},
  issn = {0090-5364},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-3/issue-1/On-Tests-for-Detecting-Change-in-Mean/10.1214/aos/1176343001.full
         },
  doi = {10.1214/aos/1176343001},
  language = {en},
  number = {1},
  urldate = {2024-08-24},
  journal = {The Annals of Statistics},
  author = {Sen, Ashish and Srivastava, Muni S.},
  month = jan,
  year = {1975},
  file = {PDF:/home/jmei/Zotero/storage/TU2B6FBZ/Sen and Srivastava - 1975 - On
          Tests for Detecting Change in Mean.pdf:application/pdf},
}

@article{zou_consistent_2020,
  title = {Consistent selection of the number of change-points via
           sample-splitting},
  volume = {48},
  issn = {0090-5364},
  url = {
         https://projecteuclid.org/journals/annals-of-statistics/volume-48/issue-1/Consistent-selection-of-the-number-of-change-points-via-sample/10.1214/19-AOS1814.full
         },
  doi = {10.1214/19-AOS1814},
  abstract = {In multiple change-point analysis, one of the major challenges is
              to estimate the number of change-points. Most existing approaches
              attempt to minimize a Schwarz information criterion which balances
              a term quantifying model fit with a penalization term accounting
              for model complexity that increases with the number of
              change-points and limits overfitting. However, different
              penalization terms are required to adapt to different contexts of
              multiple change-point problems and the optimal penalization
              magnitude usually varies from the model and error distribution. We
              propose a data-driven selection criterion that is applicable to
              most kinds of popular change-point detection methods, including
              binary segmentation and optimal partitioning algorithms. The key
              idea is to select the number of change-points that minimizes the
              squared prediction error, which measures the fit of a specified
              model for a new sample. We develop a cross-validation estimation
              scheme based on an order-preserved sample-splitting strategy, and
              establish its asymptotic selection consistency under some mild
              conditions. Effectiveness of the proposed selection criterion is
              demonstrated on a variety of numerical experiments and real-data
              examples.},
  language = {en},
  number = {1},
  urldate = {2024-08-29},
  journal = {The Annals of Statistics},
  author = {Zou, Changliang and Wang, Guanghui and Li, Runze},
  month = feb,
  year = {2020},
  file = {PDF:/home/jmei/Zotero/storage/FRQ4Y37F/Zou et al. - 2020 - Consistent
          selection of the number of change-points via
          sample-splitting.pdf:application/pdf},
}

@article{cowie_electronic_2017,
  title = {Electronic health records to facilitate clinical research},
  volume = {106},
  issn = {1861-0692},
  url = {https://doi.org/10.1007/s00392-016-1025-6},
  doi = {10.1007/s00392-016-1025-6},
  abstract = {Electronic health records (EHRs) provide opportunities to enhance
              patient care, embed performance measures in clinical practice, and
              facilitate clinical research. Concerns have been raised about the
              increasing recruitment challenges in trials, burdensome and
              obtrusive data collection, and uncertain generalizability of the
              results. Leveraging electronic health records to counterbalance
              these trends is an area of intense interest. The initial
              applications of electronic health records, as the primary data
              source is envisioned for observational studies, embedded pragmatic
              or post-marketing registry-based randomized studies, or comparative
              effectiveness studies. Advancing this approach to randomized
              clinical trials, electronic health records may potentially be used
              to assess study feasibility, to facilitate patient recruitment, and
              streamline data collection at baseline and follow-up. Ensuring data
              security and privacy, overcoming the challenges associated with
              linking diverse systems and maintaining infrastructure for repeat
              use of high quality data, are some of the challenges associated
              with using electronic health records in clinical research.
              Collaboration between academia, industry, regulatory bodies, policy
              makers, patients, and electronic health record vendors is critical
              for the greater use of electronic health records in clinical
              research. This manuscript identifies the key steps required to
              advance the role of electronic health records in cardiovascular
              clinical research.},
  language = {en},
  number = {1},
  urldate = {2024-08-30},
  journal = {Clinical Research in Cardiology},
  author = {Cowie, Martin R. and Blomster, Juuso I. and Curtis, Lesley H. and
            Duclaux, Sylvie and Ford, Ian and Fritz, Fleur and Goldman, Samantha
            and Janmohamed, Salim and Kreuzer, Jörg and Leenay, Mark and Michel,
            Alexander and Ong, Seleen and Pell, Jill P. and Southworth, Mary Ross
            and Stough, Wendy Gattis and Thoenes, Martin and Zannad, Faiez and
            Zalewski, Andrew},
  month = jan,
  year = {2017},
  keywords = {Artificial Intelligence, Cardiovascular diseases, Clinical trials
              as topic, Electronic health records, Medical Ethics, Pragmatic
              clinical trials as topic},
  pages = {1--9},
  file = {Full Text PDF:/home/jmei/Zotero/storage/S5G385PC/Cowie et al. - 2017 -
          Electronic health records to facilitate clinical
          research.pdf:application/pdf},
}

@article{bennett_deep_2021,
  title = {Deep {Learned} {Process} {Parameterizations} {Provide} {Better} {
           Representations} of {Turbulent} {Heat} {Fluxes} in {Hydrologic} {
           Models}},
  volume = {57},
  issn = {1944-7973},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2020WR029328},
  doi = {10.1029/2020WR029328},
  abstract = {Deep learning (DL) methods have shown great promise for accurately
              predicting hydrologic processes but have not yet reached the
              complexity of traditional process-based hydrologic models (PBHM) in
              terms of representing the entire hydrologic cycle. The ability of
              PBHMs to simulate the hydrologic cycle makes them useful for a wide
              range of modeling and simulation tasks, for which DL methods have
              not yet been adapted. We argue that we can take advantage of each
              of these approaches by embedding DL methods into PBHMs to represent
              individual processes. We demonstrate that this is viable by
              developing DL-based representations of turbulent heat fluxes and
              coupling them into the Structure for Unifying Multiple Modeling
              Alternatives (SUMMA), a modular PBHM modeling framework. We
              developed two DL parameterizations and integrated them into SUMMA,
              resulting in a one-way coupled implementation which relies only on
              model inputs and a two-way coupled implementation, which also
              incorporates SUMMA-derived model states. Our results demonstrate
              that the DL parameterizations are able to outperform calibrated
              standalone SUMMA benchmark simulations. Further we demonstrate that
              the two-way coupling can simulate the long-term latent heat flux
              better than the standalone benchmark and one-way coupled
              configuration. This shows that DL methods can benefit from PBHM
              information, and the synergy between these modeling approaches is
              superior to either approach individually.},
  language = {en},
  number = {5},
  urldate = {2024-09-07},
  journal = {Water Resources Research},
  author = {Bennett, Andrew and Nijssen, Bart},
  year = {2021},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2020WR029328
          },
  keywords = {deep learning, hydrologic modeling, evapotranspiration, FluxNet,
              neural networks, turbulent heat},
  pages = {e2020WR029328},
  file = {Full Text PDF:/home/jmei/Zotero/storage/UHT72TSQ/Bennett and Nijssen -
          2021 - Deep Learned Process Parameterizations Provide Better
          Representations of Turbulent Heat Fluxes in
          Hy.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/467EANHV/2020WR029328.html:text/html
          },
}

@article{bennett_spatio-temporal_2024,
  title = {Spatio-{Temporal} {Machine} {Learning} for {Regional} to {Continental
           } {Scale} {Terrestrial} {Hydrology}},
  volume = {16},
  copyright = {© 2024 The Authors. Journal of Advances in Modeling Earth Systems
               published by Wiley Periodicals LLC on behalf of American
               Geophysical Union.},
  issn = {1942-2466},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2023MS004095},
  doi = {10.1029/2023MS004095},
  abstract = {Integrated hydrologic models can simulate coupled surface and
              subsurface processes but are computationally expensive to run at
              high resolutions over large domains. Here we develop a novel deep
              learning model to emulate subsurface flows simulated by the
              integrated ParFlow-CLM model across the contiguous US. We compare
              convolutional neural networks like ResNet and UNet run
              autoregressively against our novel architecture called the Forced
              SpatioTemporal RNN (FSTR). The FSTR model incorporates separate
              encoding of initial conditions, static parameters, and
              meteorological forcings, which are fused in a recurrent loop to
              produce spatiotemporal predictions of groundwater. We evaluate the
              model architectures on their ability to reproduce 4D pressure heads
              , water table depths, and surface soil moisture over the contiguous
              US at 1 km resolution and daily time steps over the course of a
              full water year. The FSTR model shows superior performance to the
              baseline models, producing stable simulations that capture both
              seasonal and event-scale dynamics across a wide array of
              hydroclimatic regimes. The emulators provide over 1,000× speedup
              compared to the original physical model, which will enable new
              capabilities like uncertainty quantification and data assimilation
              for integrated hydrologic modeling that were not previously
              possible. Our results demonstrate the promise of using specialized
              deep learning architectures like FSTR for emulating complex
              process-based models without sacrificing fidelity.},
  language = {en},
  number = {6},
  urldate = {2024-09-07},
  journal = {Journal of Advances in Modeling Earth Systems},
  author = {Bennett, Andrew and Tran, Hoang and De la Fuente, Luis and Triplett,
            Amanda and Ma, Yueling and Melchior, Peter and Maxwell, Reed M. and
            Condon, Laura E.},
  year = {2024},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2023MS004095
          },
  keywords = {deep learning, groundwater, hydrologic modeling, surrogate models},
  pages = {e2023MS004095},
  file = {Full Text PDF:/home/jmei/Zotero/storage/7KBNYE4E/Bennett et al. - 2024
          - Spatio-Temporal Machine Learning for Regional to Continental Scale
          Terrestrial
          Hydrology.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/SCACFIJ8/2023MS004095.html:text/html
          },
}

@article{parsaeian_structural_2024,
  title = {Structural breaks in seemingly unrelated regression models},
  volume = {28},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  issn = {1365-1005, 1469-8056},
  url = {
         https://www.cambridge.org/core/product/identifier/S1365100523000329/type/journal_article
         },
  doi = {10.1017/S1365100523000329},
  abstract = {This paper develops an efﬁcient Stein-like shrinkage estimator for
              estimating slope parameters under structural breaks in seemingly
              unrelated regression models, which is then used for forecasting.
              The proposed method is a weighted average of two estimators: a
              restricted estimator that estimates the parameters under the
              restriction of no break in the coefﬁcients, and an unrestricted
              estimator that considers break points and estimates the parameters
              using the observations within each regime. It is established that
              the asymptotic risk of the Stein-like shrinkage estimator is
              smaller than that of the unrestricted estimator, which is the
              method typically used to estimate the slope coefﬁcients under
              structural breaks. Furthermore, this paper proposes an averaging
              minimal mean squared error estimator in which the averaging weight
              is derived by minimizing its asymptotic risk. Insights from the
              theoretical analysis are demonstrated in Monte Carlo simulations
              and through an empirical example of forecasting output growth of G7
              countries.},
  language = {en},
  number = {4},
  urldate = {2024-09-18},
  journal = {Macroeconomic Dynamics},
  author = {Parsaeian, Shahnaz},
  month = jun,
  year = {2024},
  pages = {946--969},
  file = {PDF:/home/jmei/Zotero/storage/VPDTBPA5/Parsaeian - 2024 - Structural
          breaks in seemingly unrelated regression models.pdf:application/pdf},
}

@article{bailey_implementing_2011,
  title = {Implementing {Panel}-{Corrected} {Standard} {Errors} in \textit{{R}}
           : {The} \textbf{pcse} {Package}},
  volume = {42},
  issn = {1548-7660},
  shorttitle = {Implementing {Panel}-{Corrected} {Standard} {Errors} in \textit{
                {R}}},
  url = {http://www.jstatsoft.org/v42/c01/},
  doi = {10.18637/jss.v042.c01},
  abstract = {This introduction to the R package pcse is a (slightly) modiﬁed
              version of Bailey and Katz (2011), published in the Journal of
              Statistical Software. Time-series–cross-section (TSCS) data are
              characterized by having repeated observations over time on some set
              of units, such as states or nations. TSCS data typically display
              both contemporaneous correlation across units and unit level
              heteroskedasity making inference from standard errors produced by
              ordinary least squares incorrect. Panel-corrected standard errors
              (PCSE) account for these these deviations from spherical errors and
              allow for better inference from linear models estimated from TSCS
              data. In this paper, we discuss an implementation of them in the R
              system for statistical computing. The key computational issue is
              how to handle unbalanced data.},
  language = {en},
  number = {Code Snippet 1},
  urldate = {2024-09-19},
  journal = {Journal of Statistical Software},
  author = {Bailey, Delia and Katz, Jonathan N.},
  year = {2011},
  file = {PDF:/home/jmei/Zotero/storage/TDT7P6VJ/Bailey and Katz - 2011 -
          Implementing Panel-Corrected Standard Errors in R The pcse
          Package.pdf:application/pdf},
}

@misc{noauthor_advancing_nodate,
  title = {Advancing biomedical imaging},
  url = {https://www.pnas.org/doi/10.1073/pnas.1508524112},
  language = {en},
  urldate = {2024-09-19},
  doi = {10.1073/pnas.1508524112},
  file = {Full Text:/home/jmei/Zotero/storage/NIBMIIYY/Advancing biomedical
          imaging.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/DCC7UIJG/pnas.html:text/html
          },
}

@article{hasin_multi-omics_2017,
  title = {Multi-omics approaches to disease},
  volume = {18},
  issn = {1474-760X},
  url = {https://doi.org/10.1186/s13059-017-1215-1},
  doi = {10.1186/s13059-017-1215-1},
  abstract = {High-throughput technologies have revolutionized medical research.
              The advent of genotyping arrays enabled large-scale genome-wide
              association studies and methods for examining global transcript
              levels, which gave rise to the field of “integrative genetics”.
              Other omics technologies, such as proteomics and metabolomics, are
              now often incorporated into the everyday methodology of biological
              researchers. In this review, we provide an overview of such omics
              technologies and focus on methods for their integration across
              multiple omics layers. As compared to studies of a single omics
              type, multi-omics offers the opportunity to understand the flow of
              information that underlies disease.},
  number = {1},
  urldate = {2024-09-19},
  journal = {Genome Biology},
  author = {Hasin, Yehudit and Seldin, Marcus and Lusis, Aldons},
  month = may,
  year = {2017},
  keywords = {Causal Variant, Cell Type Composition, GWAS Locus, Omics Approach,
              Omics Technology},
  pages = {83},
  file = {Full Text PDF:/home/jmei/Zotero/storage/C2P4XC4J/Hasin et al. - 2017 -
          Multi-omics approaches to
          disease.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/H3L8T5A8/s13059-017-1215-1.html:text/html
          },
}

@misc{angelopoulos_prediction-powered_2023,
  title = {Prediction-{Powered} {Inference}},
  url = {http://arxiv.org/abs/2301.09633},
  abstract = {Prediction-powered inference is a framework for performing valid
              statistical inference when an experimental dataset is supplemented
              with predictions from a machine-learning system. The framework
              yields simple algorithms for computing provably valid confidence
              intervals for quantities such as means, quantiles, and linear and
              logistic regression coefficients, without making any assumptions on
              the machinelearning algorithm that supplies the predictions.
              Furthermore, more accurate predictions translate to smaller
              confidence intervals. Prediction-powered inference could enable
              researchers to draw valid and more data-efficient conclusions using
              machine learning. The benefits of prediction-powered inference are
              demonstrated with datasets from proteomics, astronomy, genomics,
              remote sensing, census analysis, and ecology.},
  language = {en},
  urldate = {2024-09-29},
  publisher = {arXiv},
  author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fannjiang, Clara
            and Jordan, Michael I. and Zrnic, Tijana},
  month = nov,
  year = {2023},
  note = {arXiv:2301.09633 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Statistics - Methodology, Computer Science - Artificial
              Intelligence, Quantitative Biology - Quantitative Methods},
  file = {PDF:/home/jmei/Zotero/storage/IUQI2JNA/Angelopoulos et al. - 2023 -
          Prediction-Powered Inference.pdf:application/pdf},
}

@article{heise_causal_1970,
  title = {Causal {Inference} from {Panel} {Data}},
  volume = {2},
  issn = {0081-1750},
  url = {https://www.jstor.org/stable/270780},
  doi = {10.2307/270780},
  urldate = {2024-09-30},
  journal = {Sociological Methodology},
  author = {Heise, David R.},
  year = {1970},
  note = {Publisher: [American Sociological Association, Wiley, Sage
          Publications, Inc.]},
  pages = {3--27},
  file = {JSTOR Full Text PDF:/home/jmei/Zotero/storage/J6GQQMWN/Heise - 1970 -
          Causal Inference from Panel Data.pdf:application/pdf},
}

@misc{frick_multiscale_2013,
  title = {Multiscale {Change}-{Point} {Inference}},
  url = {http://arxiv.org/abs/1301.7212},
  abstract = {We introduce a new estimator SMUCE (simultaneous multiscale
              changepoint estimator) for the change-point problem in exponential
              family regression. An unknown step function is estimated by
              minimizing the number of change-points over the acceptance region
              of a multiscale test at a level α.},
  language = {en},
  urldate = {2024-10-04},
  publisher = {arXiv},
  author = {Frick, Klaus and Munk, Axel and Sieling, Hannes},
  month = aug,
  year = {2013},
  note = {arXiv:1301.7212 [math, stat]},
  keywords = {Statistics - Methodology, Mathematics - Statistics Theory},
  file = {PDF:/home/jmei/Zotero/storage/8UC5ZTG5/Frick et al. - 2013 -
          Multiscale Change-Point Inference.pdf:application/pdf},
}

@article{pallmann_adaptive_2018,
  title = {Adaptive designs in clinical trials: why use them, and how to run and
           report them},
  volume = {16},
  issn = {1741-7015},
  shorttitle = {Adaptive designs in clinical trials},
  url = {https://doi.org/10.1186/s12916-018-1017-7},
  doi = {10.1186/s12916-018-1017-7},
  abstract = {Adaptive designs can make clinical trials more flexible by
              utilising results accumulating in the trial to modify the trial’s
              course in accordance with pre-specified rules. Trials with an
              adaptive design are often more efficient, informative and ethical
              than trials with a traditional fixed design since they often make
              better use of resources such as time and money, and might require
              fewer participants. Adaptive designs can be applied across all
              phases of clinical research, from early-phase dose escalation to
              confirmatory trials. The pace of the uptake of adaptive designs in
              clinical research, however, has remained well behind that of the
              statistical literature introducing new methods and highlighting
              their potential advantages. We speculate that one factor
              contributing to this is that the full range of adaptations
              available to trial designs, as well as their goals, advantages and
              limitations, remains unfamiliar to many parts of the clinical
              community. Additionally, the term adaptive design has been
              misleadingly used as an all-encompassing label to refer to certain
              methods that could be deemed controversial or that have been
              inadequately implemented.},
  number = {1},
  urldate = {2024-10-06},
  journal = {BMC Medicine},
  author = {Pallmann, Philip and Bedding, Alun W. and Choodari-Oskooei, Babak
            and Dimairo, Munyaradzi and Flight, Laura and Hampson, Lisa V. and
            Holmes, Jane and Mander, Adrian P. and Odondi, Lang’o and Sydes,
            Matthew R. and Villar, Sofía S. and Wason, James M. S. and Weir,
            Christopher J. and Wheeler, Graham M. and Yap, Christina and Jaki,
            Thomas},
  month = feb,
  year = {2018},
  keywords = {Adaptive design, Design modification, Flexible design, Interim
              analysis, Seamless design, Statistical methods},
  pages = {29},
  file = {Full Text PDF:/home/jmei/Zotero/storage/P48VHLTJ/Pallmann et al. -
          2018 - Adaptive designs in clinical trials why use them, and how to run
          and report
          them.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/2CHRB6CL/s12916-018-1017-7.html:text/html
          },
}

@article{abdar_review_2021,
  title = {A review of uncertainty quantification in deep learning: {Techniques}
           , applications and challenges},
  volume = {76},
  issn = {1566-2535},
  shorttitle = {A review of uncertainty quantification in deep learning},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
  doi = {10.1016/j.inffus.2021.05.008},
  abstract = {Uncertainty quantification (UQ) methods play a pivotal role in
              reducing the impact of uncertainties during both optimization and
              decision making processes. They have been applied to solve a
              variety of real-world problems in science and engineering. Bayesian
              approximation and ensemble learning techniques are two widely-used
              types of uncertainty quantification (UQ) methods. In this regard,
              researchers have proposed different UQ methods and examined their
              performance in a variety of applications such as computer vision
              (e.g., self-driving cars and object detection), image processing
              (e.g., image restoration), medical image analysis (e.g., medical
              image classification and segmentation), natural language processing
              (e.g., text classification, social media texts and recidivism
              risk-scoring), bioinformatics, etc. This study reviews recent
              advances in UQ methods used in deep learning, investigates the
              application of these methods in reinforcement learning, and
              highlights fundamental research challenges and directions
              associated with UQ.},
  urldate = {2024-10-06},
  journal = {Information Fusion},
  author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and
            Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth,
            Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra
            and Makarenkov, Vladimir and Nahavandi, Saeid},
  month = dec,
  year = {2021},
  keywords = {Artificial intelligence, Bayesian statistics, Deep learning,
              Ensemble learning, Machine learning, Uncertainty quantification},
  pages = {243--297},
  file = {ScienceDirect Full Text PDF:/home/jmei/Zotero/storage/JJHX925D/Abdar
          et al. - 2021 - A review of uncertainty quantification in deep learning
          Techniques, applications and
          challenges.pdf:application/pdf;ScienceDirect
          Snapshot:/home/jmei/Zotero/storage/EMX46934/S1566253521001081.html:text/html
          },
}

@article{cheng_general_2024,
  title = {A {General} {Primer} for {Data} {Harmonization}},
  volume = {11},
  issn = {2052-4463},
  url = {https://www.nature.com/articles/s41597-024-02956-3},
  doi = {10.1038/s41597-024-02956-3},
  language = {en},
  number = {1},
  urldate = {2024-10-06},
  journal = {Scientific Data},
  author = {Cheng, Cindy and Messerschmidt, Luca and Bravo, Isaac and Waldbauer,
            Marco and Bhavikatti, Rohan and Schenk, Caress and Grujic, Vanja and
            Model, Tim and Kubinec, Robert and Barceló, Joan},
  month = jan,
  year = {2024},
  pages = {152},
  file = {PDF:/home/jmei/Zotero/storage/2IW5CUMW/Cheng et al. - 2024 - A General
          Primer for Data Harmonization.pdf:application/pdf},
}

@article{aminikhanghahi_survey_2017,
  title = {A survey of methods for time series change point detection},
  volume = {51},
  issn = {0219-1377, 0219-3116},
  url = {http://link.springer.com/10.1007/s10115-016-0987-z},
  doi = {10.1007/s10115-016-0987-z},
  abstract = {Change points are abrupt variations in time series data. Such
              abrupt changes may represent transitions that occur between states.
              Detection of change points is useful in modelling and prediction of
              time series and is found in application areas such as medical
              condition monitoring, climate change detection, speech and image
              analysis, and human activity analysis. This survey article
              enumerates, categorizes, and compares many of the methods that have
              been proposed to detect change points in time series. The methods
              examined include both supervised and unsupervised algorithms that
              have been introduced and evaluated. We introduce several criteria
              to compare the algorithms. Finally, we present some grand
              challenges for the community to consider.},
  language = {en},
  number = {2},
  urldate = {2024-10-09},
  journal = {Knowledge and Information Systems},
  author = {Aminikhanghahi, Samaneh and Cook, Diane J.},
  month = may,
  year = {2017},
  pages = {339--367},
  file = {PDF:/home/jmei/Zotero/storage/ZM4MJNYB/Aminikhanghahi and Cook - 2017
          - A survey of methods for time series change point
          detection.pdf:application/pdf},
}

@article{bernal_deep_2019,
  title = {Deep convolutional neural networks for brain image analysis on
           magnetic resonance imaging: a review},
  volume = {95},
  issn = {0933-3657},
  shorttitle = {Deep convolutional neural networks for brain image analysis on
                magnetic resonance imaging},
  url = {https://www.sciencedirect.com/science/article/pii/S0933365716305206},
  doi = {10.1016/j.artmed.2018.08.008},
  abstract = {In recent years, deep convolutional neural networks (CNNs) have
              shown record-shattering performance in a variety of computer vision
              problems, such as visual object recognition, detection and
              segmentation. These methods have also been utilised in medical
              image analysis domain for lesion segmentation, anatomical
              segmentation and classification. We present an extensive literature
              review of CNN techniques applied in brain magnetic resonance
              imaging (MRI) analysis, focusing on the architectures,
              pre-processing, data-preparation and post-processing strategies
              available in these works. The aim of this study is three-fold. Our
              primary goal is to report how different CNN architectures have
              evolved, discuss state-of-the-art strategies, condense their
              results obtained using public datasets and examine their pros and
              cons. Second, this paper is intended to be a detailed reference of
              the research activity in deep CNN for brain MRI analysis. Finally,
              we present a perspective on the future of CNNs in which we hint
              some of the research directions in subsequent years.},
  urldate = {2024-10-10},
  journal = {Artificial Intelligence in Medicine},
  author = {Bernal, Jose and Kushibar, Kaisar and Asfaw, Daniel S. and Valverde,
            Sergi and Oliver, Arnau and Martí, Robert and Lladó, Xavier},
  month = apr,
  year = {2019},
  keywords = {Segmentation, Brain MRI, Deep convolutional neural network, Review
              },
  pages = {64--81},
  file = {ScienceDirect Full Text PDF:/home/jmei/Zotero/storage/IX6T7GHG/Bernal
          et al. - 2019 - Deep convolutional neural networks for brain image
          analysis on magnetic resonance imaging a
          review.pdf:application/pdf;ScienceDirect
          Snapshot:/home/jmei/Zotero/storage/RFNYCJDW/S0933365716305206.html:text/html
          },
}

@article{cerullo_association_2022,
  title = {Association {Between} {Hospital} {Private} {Equity} {Acquisition} and
           {Outcomes} of {Acute} {Medical} {Conditions} {Among} {Medicare} {
           Beneficiaries}},
  volume = {5},
  issn = {2574-3805},
  url = {https://doi.org/10.1001/jamanetworkopen.2022.9581},
  doi = {10.1001/jamanetworkopen.2022.9581},
  abstract = {As private equity (PE) acquisitions of short-term acute care
              hospitals (ACHs) continue, their impact on the care of medically
              vulnerable older adults remains largely unexplored.To investigate
              the association between PE acquisition of ACHs and access to care,
              patient outcomes, and spending among Medicare beneficiaries
              hospitalized with acute medical conditions.This cross-sectional
              study used a generalized difference-in-differences approach to
              compare 21 091 222 patients admitted to PE-acquired vs
              non–PE-acquired short-term ACHs between January 1, 2001, and
              December 31, 2018, at least 3 years before to 3 years after PE
              acquisition. The analysis was conducted between December 28, 2020,
              and February 1, 2022. Differences were estimated using both
              facility and hospital service area fixed effects. To assess the
              robustness of findings, regressions were reestimated after
              including fixed effects of patient county of origin to account for
              geographic differences in underlying health risks. Two subset
              analyses were also conducted: (1) an analysis including only
              hospitals in hospital referral regions with at least 1 PE
              acquisition and (2) an analysis stratified by participation in the
              Hospital Corporation of America 2006 acquisition. The study
              included Medicare beneficiaries 66 years and older who were
              hospitalized with 1 of 5 acute medical conditions: acute myocardial
              infarction (AMI), acute stroke, chronic obstructive pulmonary
              disease exacerbation, congestive heart failure exacerbation, and
              pneumonia.Acquisition of hospitals by PE firms.Comorbidity burden
              (measured by Elixhauser comorbidity score), hospital length of stay
              , in-hospital mortality, 30-day mortality, 30-day readmission, and
              30-day episode payments.Among 21 091 222 total Medicare
              beneficiaries admitted to ACHs between 2001 and 2018, 20 431 486
              patients received care at non–PE-acquired hospitals, and 659 736
              received care at PE-acquired hospitals. Across all admissions, the
              mean (SD) age was 79.45 (7.95) years; 11 727 439 patients (55.6\%)
              were male, and 4 550 012 patients (21.6\%) had dual insurance; 2
              996 560 (14.2\%) patients were members of racial or ethnic minority
              groups, including 2 085 128 [9.9\%] Black and 371 648 [1.8\%]
              Hispanic; 18 094 662 patients (85.8\%) were White. Overall, 3 083
              760 patients (14.6\%) were hospitalized with AMI, 2 835 777 (13.4\%
              ) with acute stroke, 3 674 477 (17.4\%) with chronic obstructive
              pulmonary disease exacerbation, 5 868 034 (27.8\%) with congestive
              heart failure exacerbation, and 5 629 174 (26.7\%) with pneumonia.
              Comorbidity burden decreased slightly among patients admitted with
              acute stroke (difference, −0.04 SDs; 95\% CI, −0.004 to −0.07 SDs)
              at acquired hospitals compared with nonacquired hospitals but was
              unchanged across the other 4 conditions. Among patients with AMI, a
              greater decrease in in-hospital mortality was observed in
              PE-acquired hospitals compared with non–PE-acquired hospitals
              (difference, −1.14 percentage points, 95\% CI, −1.86 to −0.42
              percentage points). In addition, a greater decrease in 30-day
              mortality (difference, −1.41 percentage points; 95\% CI, −2.26 to
              −0.56 percentage points) was found at acquired vs nonacquired
              hospitals. However, 30-day spending and readmission rates remained
              unchanged across all conditions. The extent and directionality of
              estimates were preserved across all robustness assessments and
              subset analyses.In this cross-sectional study using a
              difference-in-differences approach, PE acquisition had no
              substantial association with the patient-level outcomes examined,
              although it was associated with a moderate improvement in mortality
              among Medicare beneficiaries hospitalized with AMI.},
  number = {4},
  urldate = {2024-10-13},
  journal = {JAMA Network Open},
  author = {Cerullo, Marcelo and Yang, Kelly and Joynt Maddox, Karen E. and
            McDevitt, Ryan C. and Roberts, James W. and Offodile, 2nd, Anaeze C.},
  month = apr,
  year = {2022},
  pages = {e229581},
  file = {Full Text PDF:/home/jmei/Zotero/storage/NQ7V8SHA/Cerullo et al. - 2022
          - Association Between Hospital Private Equity Acquisition and Outcomes
          of Acute Medical Conditions
          Amo.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/V3HMSEWG/2791727.html:text/html
          },
}

@article{goodair_effect_2024,
  title = {The effect of health-care privatisation on the quality of care},
  volume = {9},
  issn = {24682667},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2468266724000033},
  doi = {10.1016/S2468-2667(24)00003-3},
  language = {en},
  number = {3},
  urldate = {2024-10-13},
  journal = {The Lancet Public Health},
  author = {Goodair, Benjamin and Reeves, Aaron},
  month = mar,
  year = {2024},
  pages = {e199--e206},
  file = {PDF:/home/jmei/Zotero/storage/SYFXD5YI/Goodair and Reeves - 2024 - The
          effect of health-care privatisation on the quality of
          care.pdf:application/pdf},
}

@article{fearnhead_relating_2020,
  title = {Relating and comparing methods for detecting changes in mean},
  volume = {9},
  issn = {2049-1573},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.291},
  doi = {10.1002/sta4.291},
  abstract = {In recent years, there have been a large number of proposed
              approaches to detecting changes in mean. A natural question for an
              analyst is which method is most appropriate for their applications.
              Answering this question is difficult because current empirical
              studies often give conflicting conclusions. This paper aims to show
              the similarities and differences between different changepoint
              methods. We highlight that there are two aspects to estimating
              changepoints: estimating the number of changes and estimating their
              locations, and that comparisons should separately evaluate these
              two aspects. We perform an extensive comparison of different
              methods across a range of simulation scenarios and provide code and
              full results for an interested practitioner to extend this
              comparison to more methods or different scenarios.},
  language = {en},
  number = {1},
  urldate = {2024-10-21},
  journal = {Stat},
  author = {Fearnhead, Paul and Rigaill, Guillem},
  year = {2020},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.291},
  keywords = {binary segmentation, breakpoints, dynamic programming, model error
              , penalized likelihood, structural breaks},
  pages = {e291},
  file = {Full Text PDF:/home/jmei/Zotero/storage/CTIWFPQZ/Fearnhead and Rigaill
          - 2020 - Relating and comparing methods for detecting changes in
          mean.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/6M3RGVEN/sta4.html:text/html
          },
}

@article{fearnhead_relating_2020-1,
  title = {Relating and comparing methods for detecting changes in mean},
  volume = {9},
  copyright = {© 2020 The Authors. Stat published by John Wiley \& Sons, Ltd.},
  issn = {2049-1573},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.291},
  doi = {10.1002/sta4.291},
  abstract = {In recent years, there have been a large number of proposed
              approaches to detecting changes in mean. A natural question for an
              analyst is which method is most appropriate for their applications.
              Answering this question is difficult because current empirical
              studies often give conflicting conclusions. This paper aims to show
              the similarities and differences between different changepoint
              methods. We highlight that there are two aspects to estimating
              changepoints: estimating the number of changes and estimating their
              locations, and that comparisons should separately evaluate these
              two aspects. We perform an extensive comparison of different
              methods across a range of simulation scenarios and provide code and
              full results for an interested practitioner to extend this
              comparison to more methods or different scenarios.},
  language = {en},
  number = {1},
  urldate = {2024-10-21},
  journal = {Stat},
  author = {Fearnhead, Paul and Rigaill, Guillem},
  year = {2020},
  note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.291},
  keywords = {binary segmentation, breakpoints, dynamic programming, model error
              , penalized likelihood, structural breaks},
  pages = {e291},
  file = {Full Text PDF:/home/jmei/Zotero/storage/BDTPM9SC/Fearnhead and Rigaill
          - 2020 - Relating and comparing methods for detecting changes in
          mean.pdf:application/pdf;Snapshot:/home/jmei/Zotero/storage/HG2B3BCB/sta4.html:text/html
          },
}

@article{cho_data_2024,
  title = {Data segmentation algorithms: {Univariate} mean change and beyond},
  volume = {30},
  issn = {2452-3062},
  shorttitle = {Data segmentation algorithms},
  url = {https://www.sciencedirect.com/science/article/pii/S2452306221001234},
  doi = {10.1016/j.ecosta.2021.10.008},
  abstract = {Data segmentation a.k.a. multiple change point analysis has
              received considerable attention due to its importance in time
              series analysis and signal processing, with applications in a
              variety of fields including natural and social sciences, medicine,
              engineering and finance. The first part reviews the existing
              literature on the canonical data segmentation problem which aims at
              detecting and localising multiple change points in the mean of
              univariate time series. An overview of popular methodologies is
              provided on their computational complexity and theoretical
              properties. In particular, the theoretical discussion focuses on
              the separation rate relating to which change points are detectable
              by a given procedure, and the localisation rate quantifying the
              precision of corresponding change point estimators, and a
              distinction is made whether a homogeneous or multiscale viewpoint
              has been adopted in their derivation. It is further highlighted
              that the latter viewpoint provides the most general setting for
              investigating the optimality of data segmentation algorithms.
              Arguably, the canonical segmentation problem has been the most
              popular framework to propose new data segmentation algorithms and
              study their efficiency in the last decades. The second part of this
              survey motivates the importance of attaining an in-depth
              understanding of strengths and weaknesses of methodologies for the
              change point problem in a simpler, univariate setting, as a
              stepping stone for the development of methodologies for more
              complex problems. This point is illustrated with a range of
              examples showcasing the connections between complex distributional
              changes and those in the mean. Extensions towards high-dimensional
              change point problems are also discussed where it is demonstrated
              that the challenges arising from high dimensionality are orthogonal
              to those in dealing with multiple change points.},
  urldate = {2024-10-21},
  journal = {Econometrics and Statistics},
  author = {Cho, Haeran and Kirch, Claudia},
  month = apr,
  year = {2024},
  keywords = {Change point analysis, Data segmentation, High-dimensional
              statistics, Time series analysis},
  pages = {76--95},
  file = {ScienceDirect Full Text PDF:/home/jmei/Zotero/storage/D5ESHRUI/Cho and
          Kirch - 2024 - Data segmentation algorithms Univariate mean change and
          beyond.pdf:application/pdf;ScienceDirect
          Snapshot:/home/jmei/Zotero/storage/ZEYBYPHJ/S2452306221001234.html:text/html
          },
}

@misc{wang_poisoning_2024,
  title = {Poisoning {Attacks} against {Recommender} {Systems}: {A} {Survey}},
  shorttitle = {Poisoning {Attacks} against {Recommender} {Systems}},
  url = {http://arxiv.org/abs/2401.01527},
  abstract = {Modern recommender systems have seen substantial success, yet they
              remain vulnerable to malicious activities, notably poisoning
              attacks. These attacks involve injecting malicious data into the
              training datasets of RS, thereby compromising their integrity and
              manipulating recommendation outcomes for unscrupulous gains. This
              survey paper provides a systematic and up-to-date review of the
              research landscape on Poisoning Attacks against Recommendation
              (PAR). A novel and comprehensive taxonomy is proposed, categorizing
              existing PAR methodologies into three distinct categories:
              Component-Specific, Goal-Driven, and Capability Probing. Each
              category is examined in detail, covering their mechanisms and
              associated methods. To spur future research, we also highlight
              present challenges and propose innovative directions in this field.
              Moreover, to facilitate and standardize the empirical comparison of
              PAR, we introduce an open-source library, ARLib, which encompasses
              a comprehensive collection of PAR models and datasets. The library
              is released at https://github.com/CoderWZW/ARLib.},
  language = {en},
  urldate = {2024-10-28},
  publisher = {arXiv},
  author = {Wang, Zongwei and Gao, Min and Yu, Junliang and Ma, Hao and Yin,
            Hongzhi and Sadiq, Shazia},
  month = jan,
  year = {2024},
  note = {arXiv:2401.01527 [cs]},
  keywords = {Computer Science - Information Retrieval},
  file = {PDF:/home/jmei/Zotero/storage/4WJ39LRS/Wang et al. - 2024 - Poisoning
          Attacks against Recommender Systems A Survey.pdf:application/pdf},
}

@inproceedings{chen_xgboost_2016,
  address = {San Francisco California USA},
  title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
  isbn = {978-1-4503-4232-2},
  shorttitle = {{XGBoost}},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly eﬀective and widely used machine
              learning method. In this paper, we describe a scalable endto-end
              tree boosting system called XGBoost, which is used widely by data
              scientists to achieve state-of-the-art results on many machine
              learning challenges. We propose a novel sparsity-aware algorithm
              for sparse data and weighted quantile sketch for approximate tree
              learning. More importantly, we provide insights on cache access
              patterns, data compression and sharding to build a scalable tree
              boosting system. By combining these insights, XGBoost scales beyond
              billions of examples using far fewer resources than existing
              systems.},
  language = {en},
  urldate = {2024-11-17},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {
               Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
  publisher = {ACM},
  author = {Chen, Tianqi and Guestrin, Carlos},
  month = aug,
  year = {2016},
  pages = {785--794},
  file = {PDF:/home/jmei/Zotero/storage/HHGL9XLK/Chen and Guestrin - 2016 -
          XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@article{le_gratiet_recursive_2014,
  title = {{RECURSIVE} {CO}-{KRIGING} {MODEL} {FOR} {DESIGN} {OF} {COMPUTER} {
           EXPERIMENTS} {WITH} {MULTIPLE} {LEVELS} {OF} {FIDELITY}},
  volume = {4},
  issn = {2152-5080},
  url = {http://www.dl.begellhouse.com/journals/52034eb04b657aea,
         2f7b99cc281f2702,4c83626c5e64207a.html},
  doi = {10.1615/Int.J.UncertaintyQuantification.2014006914},
  language = {en},
  number = {5},
  urldate = {2024-11-23},
  journal = {International Journal for Uncertainty Quantification},
  author = {Le Gratiet, Loic and Garnier, Josselin},
  year = {2014},
  pages = {365--386},
  file = {PDF:/home/jmei/Zotero/storage/NC9L46JX/Le Gratiet and Garnier - 2014 -
          RECURSIVE CO-KRIGING MODEL FOR DESIGN OF COMPUTER EXPERIMENTS WITH
          MULTIPLE LEVELS OF FIDELITY.pdf:application/pdf},
}

@article{wang_intuitive_2023,
  title = {An {Intuitive} {Tutorial} to {Gaussian} {Process} {Regression}},
  volume = {25},
  issn = {1521-9615, 1558-366X},
  url = {http://arxiv.org/abs/2009.10862},
  doi = {10.1109/MCSE.2023.3342149},
  abstract = {This tutorial aims to provide an intuitive introduction to
              Gaussian process regression (GPR). GPR models have been widely used
              in machine learning applications due to their representation
              flexibility and inherent capability to quantify uncertainty over
              predictions. The tutorial starts with explaining the basic concepts
              that a Gaussian process is built on, including multivariate normal
              distribution, kernels, non-parametric models, and joint and
              conditional probability. It then provides a concise description of
              GPR and an implementation of a standard GPR algorithm. In addition,
              the tutorial reviews packages for implementing state-of-the-art
              Gaussian process algorithms. This tutorial is accessible to a broad
              audience, including those new to machine learning, ensuring a clear
              understanding of GPR fundamentals.},
  language = {en},
  number = {4},
  urldate = {2024-11-23},
  journal = {Computing in Science \& Engineering},
  author = {Wang, Jie},
  month = jul,
  year = {2023},
  note = {arXiv:2009.10862 [stat]},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning
              , Computer Science - Robotics},
  pages = {4--11},
  file = {PDF:/home/jmei/Zotero/storage/YY6REYIJ/2009.pdf:application/pdf},
}

@article{zrnic_cross-prediction-powered_2024,
  title = {Cross-prediction-powered inference},
  volume = {121},
  issn = {0027-8424, 1091-6490},
  url = {https://pnas.org/doi/10.1073/pnas.2322083121},
  doi = {10.1073/pnas.2322083121},
  abstract = {While reliable data-driven decision-making hinges on high-quality
              labeled data, the acquisition of quality labels often involves
              laborious human annotations or slow and expensive scientific
              measurements. Machine learning is becoming an appealing alternative
              as sophisticated predictive techniques are being used to quickly
              and cheaply produce large amounts of predicted labels; e.g.,
              predicted protein structures are used to supplement experimentally
              derived structures, predictions of socioeconomic indicators from
              satellite imagery are used to supplement accurate survey data, and
              so on. Since predictions are imperfect and potentially biased, this
              practice brings into question the validity of downstream
              inferences. We introduce cross-prediction: a method for valid
              inference powered by machine learning. With a small labeled dataset
              and a large unlabeled dataset, cross-prediction imputes the missing
              labels via machine learning and applies a form of debiasing to
              remedy the prediction inaccuracies. The resulting inferences
              achieve the desired error probability and are more powerful than
              those that only leverage the labeled data. Closely related is the
              recent proposal of prediction-powered inference [A. N. Angelopoulos
              , S. Bates, C. Fannjiang, M. I. Jordan, T. Zrnic, Science 382 ,
              669–674 (2023)], which assumes that a good pretrained model is
              already available. We show that cross-prediction is consistently
              more powerful than an adaptation of prediction-powered inference in
              which a fraction of the labeled data is split off and used to train
              the model. Finally, we observe that cross-prediction gives more
              stable conclusions than its competitors; its CIs typically have
              significantly lower variability.},
  language = {en},
  number = {15},
  urldate = {2024-12-13},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Zrnic, Tijana and Candès, Emmanuel J.},
  month = apr,
  year = {2024},
  pages = {e2322083121},
  file = {PDF:/home/jmei/Zotero/storage/Z8WMQDHL/Zrnic and Candès - 2024 -
          Cross-prediction-powered inference.pdf:application/pdf},
}
